{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: BERT Vectors Correction Data\n",
    "output-file: bert_vectors_correction_data.html\n",
    "description: Functionality for error correction with the BERT vectors correction dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp bert_vectors_correction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import sys\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ocrpostcorrection.error_correction import (\n",
    "    PAD_IDX,\n",
    "    SimpleCorrectionSeq2seq,\n",
    "    generate_vocabs,\n",
    "    get_text_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import os\n",
    "\n",
    "from ocrpostcorrection.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BertVectorsCorrectionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        bert_vectors_file: Path,\n",
    "        split_name: str,\n",
    "        max_len: int = 11,\n",
    "    ):\n",
    "        ds = data.copy()\n",
    "        ds.reset_index(drop=True, inplace=True)\n",
    "        ds = ds.query(f\"len_ocr < {max_len}\").query(f\"len_gs < {max_len}\").copy()\n",
    "        ds.reset_index(drop=False, inplace=True)\n",
    "        self.ds = ds\n",
    "\n",
    "        f = h5py.File(bert_vectors_file, \"r\")\n",
    "        self.bert_vectors = f.get(split_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds.loc[idx]\n",
    "        original_idx = sample[\"index\"]\n",
    "        bert_vector = torch.as_tensor(np.array(self.bert_vectors[original_idx]))\n",
    "\n",
    "        return [char for char in sample.ocr], [char for char in sample.gs], bert_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample bert vectors have been generated using `python src/stages/create-bert-vectors.py --seed 1234 --dataset-in ../ocrpostcorrection/nbs/data/correction/dataset.csv --model-dir models/error-detection/ --model-name bert-base-multilingual-cased --batch-size 1 --out-file ../ocrpostcorrection/nbs/data/correction/bert-vectors.hdf5` (from ocrpostcorrection-notebooks, model from [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "split_name = \"test\"\n",
    "\n",
    "dataset = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def collate_fn_with_text_transform(text_transform, batch):\n",
    "    \"\"\"Function to collate data samples into batch tensors, to be used as partial with instatiated text_transform\"\"\"\n",
    "    src_batch, tgt_batch, bert_vectors = [], [], []\n",
    "    for src_sample, tgt_sample, bert_vector in batch:\n",
    "        src_batch.append(text_transform[\"ocr\"](src_sample))\n",
    "        tgt_batch.append(text_transform[\"gs\"](tgt_sample))\n",
    "        bert_vectors.append(bert_vector)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    # Size of encoder_hidden should be 1 x batch_size x hidden_size\n",
    "    encoder_hidden = torch.unsqueeze(torch.stack(bert_vectors, dim=0), dim=0)\n",
    "\n",
    "    return src_batch.to(torch.int64), tgt_batch.to(torch.int64), encoder_hidden\n",
    "\n",
    "\n",
    "def collate_fn(text_transform):\n",
    "    \"\"\"Function to collate data samples into batch tensors\"\"\"\n",
    "    return partial(collate_fn_with_text_transform, text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Can we loop over the entire dataset?\n",
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "data.fillna(\"\", inplace=True)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "split_name = \"test\"\n",
    "vocab_transform = generate_vocabs(data.query('dataset == \"test\"'))\n",
    "text_transform = get_text_transform(vocab_transform)\n",
    "\n",
    "dataset = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "num_samples = 0\n",
    "for batch in dataloader:\n",
    "    print(len(batch))\n",
    "    print(batch[2].shape)\n",
    "\n",
    "    num_samples += batch[0].shape[1]\n",
    "assert num_samples == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def validate_model(model, dataloader, device):\n",
    "    cum_loss = 0\n",
    "    cum_examples = 0\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, encoder_hidden in dataloader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            example_losses, _decoder_ouputs = model(src, encoder_hidden, tgt)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return cum_loss / cum_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 2\n",
    "hidden_size = 768\n",
    "dropout = 0.1\n",
    "max_token_len = 10\n",
    "\n",
    "model = SimpleCorrectionSeq2seq(\n",
    "    len(vocab_transform[\"ocr\"]),\n",
    "    hidden_size,\n",
    "    len(vocab_transform[\"gs\"]),\n",
    "    dropout,\n",
    "    max_token_len,\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.723804897732204"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_name = \"val\"\n",
    "\n",
    "val = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "loss = validate_model(model, val_dataloader, device)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def train_model(\n",
    "    train_dl: DataLoader[int],\n",
    "    val_dl: DataLoader[int],\n",
    "    model: SimpleCorrectionSeq2seq,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int = 5,\n",
    "    valid_niter: int = 5000,\n",
    "    model_save_path: Path = Path(\"model.rar\"),\n",
    "    max_num_patience: int = 5,\n",
    "    max_num_trial: int = 5,\n",
    "    lr_decay: float = 0.5,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> pd.DataFrame:\n",
    "    num_iter = 0\n",
    "    report_loss = 0\n",
    "    report_examples = 0\n",
    "    val_loss_hist: List[float] = []\n",
    "    train_loss_hist: List[float] = []\n",
    "    num_trial = 0\n",
    "    patience = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        cum_loss = 0\n",
    "        cum_examples = 0\n",
    "\n",
    "        for src, tgt, encoder_hidden in tqdm(train_dl):\n",
    "            num_iter += 1\n",
    "\n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            example_losses, _ = model(src, encoder_hidden, tgt)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            report_loss += bl\n",
    "            report_examples += batch_size\n",
    "\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_iter % valid_niter == 0:\n",
    "                val_loss = validate_model(model, val_dl, device)\n",
    "                train_loss = report_loss / report_examples\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch}, iter {num_iter}, avg. train loss \"\n",
    "                    + f\"{train_loss}, avg. val loss {val_loss}\"\n",
    "                )\n",
    "\n",
    "                report_loss = 0\n",
    "                report_examples = 0\n",
    "\n",
    "                better_model = len(val_loss_hist) == 0 or val_loss < min(val_loss_hist)\n",
    "                if better_model:\n",
    "                    logger.info(f\"Saving model and optimizer to {model_save_path}\")\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        },\n",
    "                        model_save_path,\n",
    "                    )\n",
    "                elif patience < max_num_patience:\n",
    "                    patience += 1\n",
    "                    logger.info(f\"Hit patience {patience}\")\n",
    "\n",
    "                    if patience == max_num_patience:\n",
    "                        num_trial += 1\n",
    "                        logger.info(f\"Hit #{num_trial} trial\")\n",
    "                        if num_trial == max_num_trial:\n",
    "                            logger.info(\"Early stop!\")\n",
    "                            sys.exit()\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0][\"lr\"] * lr_decay\n",
    "                        logger.info(\n",
    "                            f\"Load best model so far and decay learning rate to {lr}\"\n",
    "                        )\n",
    "\n",
    "                        # load model\n",
    "                        checkpoint = torch.load(model_save_path)\n",
    "                        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "\n",
    "                val_loss_hist.append(val_loss)\n",
    "                train_loss_hist.append(train_loss)\n",
    "\n",
    "    # Create train log\n",
    "    df = pd.DataFrame({\"train_loss\": train_loss_hist, \"val_loss\": val_loss_hist})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     32\u001b[0m msp \u001b[39m=\u001b[39m Path(os\u001b[39m.\u001b[39mgetcwd()) \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_bert_vectors.rar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m train_log \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     35\u001b[0m     train_dl\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     36\u001b[0m     val_dl\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     37\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     39\u001b[0m     model_save_path\u001b[39m=\u001b[39;49mmsp,\n\u001b[1;32m     40\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     41\u001b[0m     valid_niter\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     42\u001b[0m     max_num_patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     43\u001b[0m     max_num_trial\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     44\u001b[0m     lr_decay\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m train_log\n",
      "Cell \u001b[0;32mIn[22], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dl, val_dl, model, optimizer, num_epochs, valid_niter, model_save_path, max_num_patience, max_num_trial, lr_decay, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m tgt \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m encoder_hidden\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m example_losses, _ \u001b[39m=\u001b[39m model(src, encoder_hidden, tgt)\n\u001b[1;32m     39\u001b[0m example_losses \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mexample_losses\n\u001b[1;32m     40\u001b[0m batch_loss \u001b[39m=\u001b[39m example_losses\u001b[39m.\u001b[39msum()\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/ocrpostcorrection/error_correction.py:253\u001b[0m, in \u001b[0;36mSimpleCorrectionSeq2seq.forward\u001b[0;34m(self, input, encoder_hidden, target)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# print('encoder outputs size', encoder_outputs.size())\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m ei \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(input_length):\n\u001b[1;32m    252\u001b[0m     \u001b[39m# print(f'Index {ei}; input size: {input_tensor[ei].size()}; encoder hidden size: {encoder_hidden.size()}')\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     encoder_output, encoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    254\u001b[0m         input_tensor[ei], encoder_hidden\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[39m# print('Index', ei)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[39m# print('encoder output size', encoder_output.size())\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# print('encoder outputs size', encoder_outputs.size())\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# print('output selection size', encoder_output[:, 0].size())\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# print('ouput to save', encoder_outputs[:,ei].size())\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     encoder_outputs[:, ei] \u001b[39m=\u001b[39m encoder_output[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/ocrpostcorrection/error_correction.py:137\u001b[0m, in \u001b[0;36mEncoderRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden):\n\u001b[1;32m    134\u001b[0m     \u001b[39m# print('Encoder')\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39m# print('input size', input.size())\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39m# print('hidden size', hidden.size())\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m     \u001b[39m# print('embedded size', embedded.size())\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# print(embedded)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# print('embedded size met view', embedded.view(1, 1, -1).size())\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     output \u001b[39m=\u001b[39m embedded\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/.venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/code/ocrpostcorrection/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "split_name = \"train\"\n",
    "train = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train, batch_size=2, collate_fn=collate_fn(text_transform), shuffle=True\n",
    ")\n",
    "\n",
    "split_name = \"val\"\n",
    "val = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=3, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "hidden_size = 768\n",
    "model = SimpleCorrectionSeq2seq(\n",
    "    len(vocab_transform[\"ocr\"]),\n",
    "    hidden_size,\n",
    "    len(vocab_transform[\"gs\"]),\n",
    "    0.1,\n",
    "    10,\n",
    "    teacher_forcing_ratio=0.0,\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "msp = Path(os.getcwd()) / \"data\" / \"model_bert_vectors.rar\"\n",
    "\n",
    "train_log = train_model(\n",
    "    train_dl=train_dataloader,\n",
    "    val_dl=val_dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    model_save_path=msp,\n",
    "    num_epochs=2,\n",
    "    valid_niter=5,\n",
    "    max_num_patience=5,\n",
    "    max_num_trial=5,\n",
    "    lr_decay=0.5,\n",
    ")\n",
    "train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
