{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: BERT Vectors Correction Data\n",
    "output-file: bert_vectors_correction_data.html\n",
    "description: Functionality for error correction with the BERT vectors correction dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp bert_vectors_correction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from ocrpostcorrection.error_correction import (\n",
    "    PAD_IDX,\n",
    "    generate_vocabs,\n",
    "    get_text_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BertVectorsCorrectionDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, bert_vectors_file: Path, split_name: str, max_len: int=11):\n",
    "        ds = data.copy()\n",
    "        ds.reset_index(drop=True, inplace=True)\n",
    "        ds = ds.query(f'len_ocr < {max_len}').query(f'len_gs < {max_len}').copy()\n",
    "        ds.reset_index(drop=False, inplace=True)\n",
    "        self.ds = ds\n",
    "\n",
    "        f = h5py.File(bert_vectors_file, \"r\")\n",
    "        self.bert_vectors = f.get(split_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds.loc[idx]\n",
    "        original_idx = sample[\"index\"]\n",
    "        bert_vector = torch.as_tensor(np.array(self.bert_vectors[original_idx]))\n",
    "\n",
    "        return [char for char in sample.ocr], [char for char in sample.gs], bert_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample bert vectors have been generated using `python src/stages/create-bert-vectors.py --seed 1234 --dataset-in ../ocrpostcorrection/nbs/data/correction/dataset.csv --model-dir models/error-detection/ --model-name bert-base-multilingual-cased --batch-size 1 --out-file ../ocrpostcorrection/nbs/data/correction/bert-vectors.hdf5` (from ocrpostcorrection-notebooks, model from [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "split_name = \"test\"\n",
    "\n",
    "dataset = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"), \n",
    "    bert_vectors_file=bert_vectors_file, \n",
    "    split_name=split_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def collate_fn_with_text_transform(text_transform, batch):\n",
    "    \"\"\"Function to collate data samples into batch tensors, to be used as partial with instatiated text_transform\"\"\"\n",
    "    src_batch, tgt_batch, bert_vectors = [], [], []\n",
    "    for src_sample, tgt_sample, bert_vector in batch:\n",
    "        src_batch.append(text_transform[\"ocr\"](src_sample))\n",
    "        tgt_batch.append(text_transform[\"gs\"](tgt_sample))\n",
    "        bert_vectors.append(bert_vector)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return src_batch.to(torch.int64), tgt_batch.to(torch.int64), torch.stack(bert_vectors, dim=1)\n",
    "\n",
    "\n",
    "def collate_fn(text_transform):\n",
    "    \"\"\"Function to collate data samples into batch tensors\"\"\"\n",
    "    return partial(collate_fn_with_text_transform, text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Can we loop over the entire dataset?\n",
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "data.fillna(\"\", inplace=True)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "split_name = \"test\"\n",
    "vocab_transform = generate_vocabs(data.query('dataset == \"test\"'))\n",
    "text_transform = get_text_transform(vocab_transform)\n",
    "\n",
    "dataset = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"), \n",
    "    bert_vectors_file=bert_vectors_file, \n",
    "    split_name=split_name\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=5, collate_fn=collate_fn(text_transform)\n",
    ")\n",
    "\n",
    "num_samples = 0\n",
    "for batch in dataloader:\n",
    "    \n",
    "    num_samples += batch[0].shape[1]\n",
    "assert num_samples == len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
