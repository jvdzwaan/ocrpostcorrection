{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: BERT Vectors Correction Data\n",
    "output-file: bert_vectors_correction_data.html\n",
    "description: Functionality for error correction with the BERT vectors correction dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp bert_vectors_correction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ocrpostcorrection.error_correction import (\n",
    "    BOS_IDX,\n",
    "    PAD_IDX,\n",
    "    SimpleCorrectionSeq2seq,\n",
    "    generate_vocabs,\n",
    "    get_text_transform,\n",
    "    indices2string,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import os\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import AutoTokenizer, BertModel, DataCollatorWithPadding\n",
    "\n",
    "from ocrpostcorrection.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BertVectorsCorrectionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        split_name: str,\n",
    "        bert_vectors_file: Optional[Path] = None,\n",
    "        max_len: int = 11,\n",
    "        hidden_size: int = 768,\n",
    "        look_up_bert_vectors: bool = True,\n",
    "    ):\n",
    "        ds = data.copy()\n",
    "        ds.reset_index(drop=True, inplace=True)\n",
    "        ds = ds.query(f\"len_ocr < {max_len}\").query(f\"len_gs < {max_len}\").copy()\n",
    "        ds.reset_index(drop=False, inplace=True)\n",
    "        self.ds = ds\n",
    "\n",
    "        if bert_vectors_file:\n",
    "            f = h5py.File(bert_vectors_file, \"r\")\n",
    "            self.bert_vectors = f.get(split_name)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.look_up_bert_vectors = look_up_bert_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds.loc[idx]\n",
    "        if self.look_up_bert_vectors:\n",
    "            original_idx = sample[\"index\"]\n",
    "            bert_vector = torch.as_tensor(np.array(self.bert_vectors[original_idx]))\n",
    "        else:\n",
    "            # Bert vectors should be calculated on the fly\n",
    "            bert_vector = torch.zeros(self.hidden_size)\n",
    "\n",
    "        return [char for char in sample.ocr], [char for char in sample.gs], bert_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample bert vectors have been generated using `python src/stages/create-bert-vectors.py --seed 1234 --dataset-in ../ocrpostcorrection/nbs/data/correction/dataset.csv --model-dir models/error-detection/ --model-name bert-base-multilingual-cased --batch-size 1 --out-file ../ocrpostcorrection/nbs/data/correction/bert-vectors.hdf5` (from ocrpostcorrection-notebooks, model from [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "data.fillna(\"\", inplace=True)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "split_name = \"test\"\n",
    "\n",
    "dataset = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    "    max_len=11,\n",
    "    hidden_size=768,\n",
    "    look_up_bert_vectors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"test\"\n",
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "data.fillna(\"\", inplace=True)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "\n",
    "dataset_no_look_up = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=None,\n",
    "    split_name=split_name,\n",
    "    max_len=11,\n",
    "    hidden_size=768,\n",
    "    look_up_bert_vectors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def collate_fn_with_text_transform(text_transform, batch):\n",
    "    \"\"\"Function to collate data samples into batch tensors, to be used as partial with instatiated text_transform\"\"\"\n",
    "    src_batch, tgt_batch, bert_vectors = [], [], []\n",
    "    for src_sample, tgt_sample, bert_vector in batch:\n",
    "        src_batch.append(text_transform[\"ocr\"](src_sample))\n",
    "        tgt_batch.append(text_transform[\"gs\"](tgt_sample))\n",
    "        bert_vectors.append(bert_vector)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    # Size of encoder_hidden should be 1 x batch_size x hidden_size\n",
    "    encoder_hidden = torch.unsqueeze(torch.stack(bert_vectors, dim=0), dim=0)\n",
    "\n",
    "    return src_batch.to(torch.int64), tgt_batch.to(torch.int64), encoder_hidden\n",
    "\n",
    "\n",
    "def collate_fn(text_transform):\n",
    "    \"\"\"Function to collate data samples into batch tensors\"\"\"\n",
    "    return partial(collate_fn_with_text_transform, text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Can we loop over the entire dataset?\n",
    "vocab_transform = generate_vocabs(data.query('dataset == \"train\"'))\n",
    "text_transform = get_text_transform(vocab_transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "num_samples = 0\n",
    "for batch in dataloader:\n",
    "    print(len(batch))\n",
    "    print(batch[2].shape)\n",
    "\n",
    "    num_samples += batch[0].shape[1]\n",
    "assert num_samples == len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n",
      "3\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Can we loop over the entire dataset?\n",
    "dataloader_no_look_up = DataLoader(dataset_no_look_up, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "num_samples = 0\n",
    "for batch in dataloader:\n",
    "    print(len(batch))\n",
    "    print(batch[2].shape)\n",
    "\n",
    "    num_samples += batch[0].shape[1]\n",
    "assert num_samples == len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def validate_model(model, dataloader, device):\n",
    "    cum_loss = 0\n",
    "    cum_examples = 0\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, encoder_hidden in dataloader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            example_losses, _decoder_ouputs = model(src, encoder_hidden, tgt)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return cum_loss / cum_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCorrectionSeq2seq(\n",
       "  (encoder): EncoderRNN(\n",
       "    (embedding): Embedding(46, 768)\n",
       "    (gru): GRU(768, 768, batch_first=True)\n",
       "  )\n",
       "  (decoder): AttnDecoderRNN(\n",
       "    (embedding): Embedding(44, 768)\n",
       "    (attn): Linear(in_features=1536, out_features=11, bias=True)\n",
       "    (attn_combine): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (gru): GRU(768, 768)\n",
       "    (out): Linear(in_features=768, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 2\n",
    "hidden_size = 768\n",
    "dropout = 0.1\n",
    "max_token_len = 10\n",
    "\n",
    "model = SimpleCorrectionSeq2seq(\n",
    "    len(vocab_transform[\"ocr\"]),\n",
    "    hidden_size,\n",
    "    len(vocab_transform[\"gs\"]),\n",
    "    dropout,\n",
    "    max_token_len,\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    device=device,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.875640021430122"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_name = \"val\"\n",
    "data_csv = Path(os.getcwd()) / \"data\" / \"correction\" / \"dataset.csv\"\n",
    "data = pd.read_csv(data_csv, index_col=0)\n",
    "data.fillna(\"\", inplace=True)\n",
    "bert_vectors_file = Path(os.getcwd()) / \"data\" / \"correction\" / \"bert-vectors.hdf5\"\n",
    "\n",
    "val = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    "    max_len=11,\n",
    "    hidden_size=768,\n",
    "    look_up_bert_vectors=True\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "loss = validate_model(model, val_dataloader, device)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>gs</th>\n",
       "      <th>ocr_aligned</th>\n",
       "      <th>gs_aligned</th>\n",
       "      <th>start</th>\n",
       "      <th>len_ocr</th>\n",
       "      <th>language</th>\n",
       "      <th>subset</th>\n",
       "      <th>dataset</th>\n",
       "      <th>len_gs</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test- AAA</td>\n",
       "      <td>test-.AAA</td>\n",
       "      <td>test- AAA</td>\n",
       "      <td>test-.AAA</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-BBB</td>\n",
       "      <td>test- BBB</td>\n",
       "      <td>test@-BBB</td>\n",
       "      <td>test- BBB</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-CCC</td>\n",
       "      <td>test- CCC</td>\n",
       "      <td>test-@CCC</td>\n",
       "      <td>test- CCC</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-DDD</td>\n",
       "      <td>DDD</td>\n",
       "      <td>-DDD</td>\n",
       "      <td>DDD</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test- EEE</td>\n",
       "      <td>test-EEE</td>\n",
       "      <td>test- EEE</td>\n",
       "      <td>test-@EEE</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>species!</td>\n",
       "      <td>species.</td>\n",
       "      <td>species!</td>\n",
       "      <td>species.</td>\n",
       "      <td>111</td>\n",
       "      <td>8</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Test -hyhen</td>\n",
       "      <td>Testhyhen</td>\n",
       "      <td>Test -hyhen</td>\n",
       "      <td>Test@@hyhen</td>\n",
       "      <td>120</td>\n",
       "      <td>11</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>error</td>\n",
       "      <td>errors</td>\n",
       "      <td>error@</td>\n",
       "      <td>errors</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>C</td>\n",
       "      <td>CCC</td>\n",
       "      <td>C@@</td>\n",
       "      <td>CCC</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>34</td>\n",
       "      <td>3 4</td>\n",
       "      <td>3@4</td>\n",
       "      <td>3 4</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ocr         gs  ocr_aligned   gs_aligned  start  len_ocr language  \\\n",
       "0     test- AAA  test-.AAA    test- AAA    test-.AAA      0        9       fr   \n",
       "1      test-BBB  test- BBB    test@-BBB    test- BBB     10        8       fr   \n",
       "2      test-CCC  test- CCC    test-@CCC    test- CCC     19        8       fr   \n",
       "3          -DDD        DDD         -DDD          DDD     33        4       fr   \n",
       "4     test- EEE   test-EEE    test- EEE    test-@EEE     38        9       fr   \n",
       "..          ...        ...          ...          ...    ...      ...      ...   \n",
       "75     species!   species.     species!     species.    111        8       en   \n",
       "76  Test -hyhen  Testhyhen  Test -hyhen  Test@@hyhen    120       11       en   \n",
       "77        error     errors       error@       errors    137        5       en   \n",
       "78            C        CCC          C@@          CCC    151        1       en   \n",
       "79           34        3 4          3@4          3 4    153        2       en   \n",
       "\n",
       "        subset dataset  len_gs  diff  \n",
       "0    fr_sample   train       9     0  \n",
       "1    fr_sample   train       9    -1  \n",
       "2    fr_sample   train       9    -1  \n",
       "3    fr_sample   train       3     1  \n",
       "4    fr_sample   train       8     1  \n",
       "..         ...     ...     ...   ...  \n",
       "75  eng_sample    test       8     0  \n",
       "76  eng_sample    test       9     2  \n",
       "77  eng_sample    test       6    -1  \n",
       "78  eng_sample    test       3    -2  \n",
       "79  eng_sample    test       3    -1  \n",
       "\n",
       "[80 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def train_model(\n",
    "    train_dl: DataLoader[int],\n",
    "    val_dl: DataLoader[int],\n",
    "    model: SimpleCorrectionSeq2seq,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int = 5,\n",
    "    valid_niter: int = 5000,\n",
    "    model_save_path: Path = Path(\"model.rar\"),\n",
    "    max_num_patience: int = 5,\n",
    "    max_num_trial: int = 5,\n",
    "    lr_decay: float = 0.5,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> pd.DataFrame:\n",
    "    num_iter = 0\n",
    "    report_loss = 0\n",
    "    report_examples = 0\n",
    "    val_loss_hist: List[float] = []\n",
    "    train_loss_hist: List[float] = []\n",
    "    num_trial = 0\n",
    "    patience = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        cum_loss = 0\n",
    "        cum_examples = 0\n",
    "\n",
    "        for src, tgt, encoder_hidden in tqdm(train_dl):\n",
    "            num_iter += 1\n",
    "\n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            encoder_hidden = encoder_hidden.to(device)\n",
    "\n",
    "            example_losses, _ = model(src, encoder_hidden, tgt)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            report_loss += bl\n",
    "            report_examples += batch_size\n",
    "\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_iter % valid_niter == 0:\n",
    "                val_loss = validate_model(model, val_dl, device)\n",
    "                train_loss = report_loss / report_examples\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch}, iter {num_iter}, avg. train loss \"\n",
    "                    + f\"{train_loss}, avg. val loss {val_loss}\"\n",
    "                )\n",
    "\n",
    "                report_loss = 0\n",
    "                report_examples = 0\n",
    "\n",
    "                better_model = len(val_loss_hist) == 0 or val_loss < min(val_loss_hist)\n",
    "                if better_model:\n",
    "                    logger.info(f\"Saving model and optimizer to {model_save_path}\")\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        },\n",
    "                        model_save_path,\n",
    "                    )\n",
    "                elif patience < max_num_patience:\n",
    "                    patience += 1\n",
    "                    logger.info(f\"Hit patience {patience}\")\n",
    "\n",
    "                    if patience == max_num_patience:\n",
    "                        num_trial += 1\n",
    "                        logger.info(f\"Hit #{num_trial} trial\")\n",
    "                        if num_trial == max_num_trial:\n",
    "                            logger.info(\"Early stop!\")\n",
    "                            # Create train log\n",
    "                            df = pd.DataFrame({\"train_loss\": train_loss_hist, \"val_loss\": val_loss_hist})\n",
    "                            return df\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0][\"lr\"] * lr_decay\n",
    "                        logger.info(\n",
    "                            f\"Load best model so far and decay learning rate to {lr}\"\n",
    "                        )\n",
    "\n",
    "                        # load model\n",
    "                        checkpoint = torch.load(model_save_path)\n",
    "                        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group[\"lr\"] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "\n",
    "                val_loss_hist.append(val_loss)\n",
    "                train_loss_hist.append(train_loss)\n",
    "\n",
    "    # Create train log\n",
    "    df = pd.DataFrame({\"train_loss\": train_loss_hist, \"val_loss\": val_loss_hist})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [00:00<00:01,  8.61it/s]2023-09-03 19:00:06.994 | INFO     | __main__:train_model:58 - Epoch 1, iter 5, avg. train loss 27.373350143432617, avg. val loss 24.627723693847656\n",
      "2023-09-03 19:00:06.995 | INFO     | __main__:train_model:68 - Saving model and optimizer to /Users/janneke/code/ocrpostcorrection/nbs/data/model_bert_vectors.rar\n",
      " 69%|██████▉   | 9/13 [00:01<00:00,  8.23it/s]2023-09-03 19:00:07.644 | INFO     | __main__:train_model:58 - Epoch 1, iter 10, avg. train loss 24.103273010253908, avg. val loss 24.043284098307293\n",
      "2023-09-03 19:00:07.644 | INFO     | __main__:train_model:68 - Saving model and optimizer to /Users/janneke/code/ocrpostcorrection/nbs/data/model_bert_vectors.rar\n",
      "100%|██████████| 13/13 [00:01<00:00,  7.17it/s]\n",
      "  8%|▊         | 1/13 [00:00<00:01,  8.18it/s]2023-09-03 19:00:08.502 | INFO     | __main__:train_model:58 - Epoch 2, iter 15, avg. train loss 19.344550323486327, avg. val loss 19.952612982855904\n",
      "2023-09-03 19:00:08.502 | INFO     | __main__:train_model:68 - Saving model and optimizer to /Users/janneke/code/ocrpostcorrection/nbs/data/model_bert_vectors.rar\n",
      " 38%|███▊      | 5/13 [00:00<00:01,  6.30it/s]2023-09-03 19:00:09.292 | INFO     | __main__:train_model:58 - Epoch 2, iter 20, avg. train loss 22.214738655090333, avg. val loss 18.82603581746419\n",
      "2023-09-03 19:00:09.293 | INFO     | __main__:train_model:68 - Saving model and optimizer to /Users/janneke/code/ocrpostcorrection/nbs/data/model_bert_vectors.rar\n",
      " 77%|███████▋  | 10/13 [00:01<00:00,  6.35it/s]2023-09-03 19:00:10.207 | INFO     | __main__:train_model:58 - Epoch 2, iter 25, avg. train loss 21.808086776733397, avg. val loss 18.29765616522895\n",
      "2023-09-03 19:00:10.208 | INFO     | __main__:train_model:68 - Saving model and optimizer to /Users/janneke/code/ocrpostcorrection/nbs/data/model_bert_vectors.rar\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.373350</td>\n",
       "      <td>24.627724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.103273</td>\n",
       "      <td>24.043284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.344550</td>\n",
       "      <td>19.952613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.214739</td>\n",
       "      <td>18.826036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.808087</td>\n",
       "      <td>18.297656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss   val_loss\n",
       "0   27.373350  24.627724\n",
       "1   24.103273  24.043284\n",
       "2   19.344550  19.952613\n",
       "3   22.214739  18.826036\n",
       "4   21.808087  18.297656"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_name = \"train\"\n",
    "train = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train, batch_size=2, collate_fn=collate_fn(text_transform), shuffle=True\n",
    ")\n",
    "\n",
    "split_name = \"val\"\n",
    "val = BertVectorsCorrectionDataset(\n",
    "    data=data.query(f\"dataset == '{split_name}'\"),\n",
    "    bert_vectors_file=bert_vectors_file,\n",
    "    split_name=split_name,\n",
    ")\n",
    "val_dataloader = DataLoader(val, batch_size=3, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "hidden_size = 768\n",
    "model = SimpleCorrectionSeq2seq(\n",
    "    len(vocab_transform[\"ocr\"]),\n",
    "    hidden_size,\n",
    "    len(vocab_transform[\"gs\"]),\n",
    "    0.1,\n",
    "    10,\n",
    "    teacher_forcing_ratio=0.0,\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "msp = Path(os.getcwd()) / \"data\" / \"model_bert_vectors.rar\"\n",
    "\n",
    "train_log = train_model(\n",
    "    train_dl=train_dataloader,\n",
    "    val_dl=val_dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    model_save_path=msp,\n",
    "    num_epochs=2,\n",
    "    valid_niter=5,\n",
    "    max_num_patience=5,\n",
    "    max_num_trial=5,\n",
    "    lr_decay=0.5,\n",
    ")\n",
    "os.remove(msp)\n",
    "train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference / prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=greedy%20decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class GreedySearchDecoder(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.max_length = model.max_length\n",
    "        self.encoder = model.encoder\n",
    "        self.decoder = model.decoder\n",
    "\n",
    "        self.device = model.device\n",
    "\n",
    "    def forward(self, input, encoder_hidden, target):\n",
    "        # input is src seq len x batch size\n",
    "        # input voor de encoder (1 stap) moet zijn input seq len x batch size x 1\n",
    "        input_tensor = input.unsqueeze(2)\n",
    "        # print('input tensor size', input_tensor.size())\n",
    "\n",
    "        input_length = input.size(0)\n",
    "\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Encoder part\n",
    "        encoder_outputs = torch.zeros(\n",
    "            batch_size, self.max_length, self.encoder.hidden_size, device=self.device\n",
    "        )\n",
    "        # print('encoder outputs size', encoder_outputs.size())\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            # print(f'Index {ei}; input size: {input_tensor[ei].size()}; encoder hidden size: {encoder_hidden.size()}')\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                input_tensor[ei], encoder_hidden\n",
    "            )\n",
    "            # print('Index', ei)\n",
    "            # print('encoder output size', encoder_output.size())\n",
    "            # print('encoder outputs size', encoder_outputs.size())\n",
    "            # print('output selection size', encoder_output[:, 0].size())\n",
    "            # print('ouput to save', encoder_outputs[:,ei].size())\n",
    "            encoder_outputs[:, ei] = encoder_output[0, 0]\n",
    "\n",
    "        # print('encoder outputs', encoder_outputs)\n",
    "        # print('encoder hidden', encoder_hidden)\n",
    "\n",
    "        # Decoder part\n",
    "        # Target = seq len x batch size\n",
    "        # Decoder input moet zijn: batch_size x 1 (van het eerste token = BOS)\n",
    "        target_length = target.size(0)\n",
    "\n",
    "        decoder_input = torch.tensor(\n",
    "            [[BOS_IDX] for _ in range(batch_size)], device=self.device\n",
    "        )\n",
    "        # print('decoder input size', decoder_input.size())\n",
    "\n",
    "        all_tokens = torch.zeros(\n",
    "            batch_size, self.max_length, device=self.device, dtype=torch.long\n",
    "        )\n",
    "        # print('all_tokens size', all_tokens.size())\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "            # print('decoder input size:', decoder_input.size())\n",
    "            # print('decoder input squeezed', decoder_input.clone().squeeze())\n",
    "\n",
    "            # Record token\n",
    "            all_tokens[:, di] = decoder_input.clone().squeeze(1)\n",
    "            # print('all_tokens', all_tokens)\n",
    "\n",
    "        return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6,  4,  8,  3,  3,  3,  3,  3,  0],\n",
      "        [ 4,  5,  6,  4,  8,  4,  3,  3,  3,  3,  0],\n",
      "        [ 4,  4,  5,  6,  4,  3,  3,  3,  3,  3,  0],\n",
      "        [14, 13,  3,  3,  3,  3,  3,  3,  3,  3,  0],\n",
      "        [ 4,  5,  6,  4,  8,  3,  3,  3,  3,  3,  0]])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n"
     ]
    }
   ],
   "source": [
    "decoder = GreedySearchDecoder(model)\n",
    "\n",
    "test_dataloader = DataLoader(dataset, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (src, tgt, encoder_hidden) in enumerate(test_dataloader):\n",
    "        predicted_indices = decoder(src, encoder_hidden, tgt)\n",
    "        if i == 0:\n",
    "            print(predicted_indices)\n",
    "        else:\n",
    "            print(predicted_indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6,  4,  8,  6,  4,  3,  3,  3,  0],\n",
      "        [ 4,  5,  6,  4,  8,  4,  3,  3,  3,  3,  0],\n",
      "        [ 4,  4,  5,  6,  4,  3,  4,  3,  3,  3,  0],\n",
      "        [14, 13,  3,  3,  3,  3,  3,  3,  3,  3,  0],\n",
      "        [ 4,  5,  6,  4,  8,  3,  3,  3,  3,  3,  0]])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n",
      "torch.Size([5, 11])\n"
     ]
    }
   ],
   "source": [
    "decoder = GreedySearchDecoder(model)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_no_look_up, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (src, tgt, encoder_hidden) in enumerate(test_dataloader):\n",
    "        predicted_indices = decoder(src, encoder_hidden, tgt)\n",
    "        if i == 0:\n",
    "            print(predicted_indices)\n",
    "        else:\n",
    "            print(predicted_indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def predict_and_convert_to_str(model, dataloader, bert_model, dataloader_bert_vectors, tgt_vocab, device):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    decoder = GreedySearchDecoder(model)\n",
    "\n",
    "    itos = tgt_vocab.get_itos()\n",
    "    output_strings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (src, tgt, _bert_vector), bert_vector_input in tqdm(zip(dataloader, dataloader_bert_vectors)):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            bert_vector_input = bert_vector_input.to(device)\n",
    "\n",
    "            bert_vector_output = bert_model(**bert_vector_input)\n",
    "            encoder_hidden = bert_vector_output[\"pooler_output\"].detach().unsqueeze(0)\n",
    "\n",
    "            predicted_indices = decoder(src, encoder_hidden, tgt)\n",
    "\n",
    "            strings_batch = indices2string(predicted_indices, itos)\n",
    "            for s in strings_batch:\n",
    "                output_strings.append(s)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return output_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4191fc1b241548229424f2cfc1cb4d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "7it [00:00, 11.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test-', 'test-t', 'ttest']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_model.eval()\n",
    "\n",
    "dataset_bert_vectors = HFDataset.from_pandas(test_dataloader.dataset.ds.ocr.to_frame())\n",
    "tokenized_dataset = dataset_bert_vectors.map(\n",
    "    lambda sample: tokenizer(sample[\"ocr\"], truncation=True),\n",
    "    batched=True,\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [\"ocr\"]\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "test_dataloader_bert_vectors = DataLoader(\n",
    "    tokenized_dataset, batch_size=5, collate_fn=collator\n",
    ")\n",
    "\n",
    "predictions = predict_and_convert_to_str(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    bert_model=bert_model,\n",
    "    dataloader_bert_vectors=test_dataloader_bert_vectors,\n",
    "    tgt_vocab=vocab_transform[\"gs\"],\n",
    "    device=device,\n",
    ")\n",
    "predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
