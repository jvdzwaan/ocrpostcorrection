{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Results\n",
    "output-file: results.html\n",
    "description: Experimental results for the different tasks\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the 2019 ICDAR competition on Post-OCR Text correction can be found \n",
    "in [this paper](https://hal.archives-ouvertes.fr/hal-02304334/document). The best\n",
    "results are repeated in the tables below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Token Classification\n",
    "\n",
    "### Summarized results (F-measure)\n",
    "\n",
    "Best results in **bold**.\n",
    "\n",
    "| Method | BG | CZ | DE | EN | ES | FI | FR | NL | PL | SL |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| CCC (2019 competition winner) |  **0.77** | **0.70** | 0.95 | **0.67** | **0.69** | **0.84** | 0.67 | **0.71** | **0.82** | **0.69** |\n",
    "| [Experiment 1](#sec-experiment-detection-exp-1) | 0.74 | 0.64 | 0.93 | 0.62 | 0.59 | 0.82 | 0.59 | 0.66 | 0.77 | 0.63 | \n",
    "| [Experiment 2 (long sequences)](#sec-experiment-detection-exp-2-long-sequences) | 0.74 | 0.69 | **0.96** | **0.67** | 0.63 | 0.83 | 0.65 | 0.69 |  0.8 | **0.69** |\n",
    "| [Experiment 1 reproduced with DVC](#sec-experiment-detection-exp-1-dvc-stratified-language) | 0.75 | 0.68 | **0.96** | 0.66 | 0.64 | 0.83 | **0.69** | 0.69 | 0.81 | 0.67 |\n",
    "| Experiment 1 reproduced with DVC - evaluation with old eval script | 0.75 | 0.68 | **0.96** | 0.66 | 0.64 | 0.83 | 0.66 | 0.69 | 0.81 | 0.67 |\n",
    "| [Experiment 1 reproduced with DVC - stratified on subset](#sec-experiment-detection-exp-1-dvc-stratified-subset) | 0.75 | 0.69 | **0.96** | **0.67** | 0.63 | 0.83 | **0.69** | 0.69 | 0.81 | 0.68 |\n",
    "| [Experiment 1 (code refactor, training batch size 32) ](#sec-experiment-detection-exp1-training-batch-size-32)  | 0.74 | 0.68 | **0.96** | 0.66 | 0.63 | 0.83 | 0.67 | 0.69 |  0.80 | 0.68 |\n",
    "| [Experiment 1 (code refactor, training batch size 16) ](#sec-experiment-detection-exp1-train-batch-size-16) | 0.75 | 0.69 | **0.96** | **0.67** | 0.63 | 0.83 | **0.69** | 0.69 | 0.81 | 0.68 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: token classification with Huggingface BERT 07-02-2022 { #sec-experiment-detection-exp-1 }\n",
    "\n",
    "* Validation set: 10% of texts (stratified on language)\n",
    "* Dataset 07-02-2022\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) lenght: 35, step: 30 (overlap of 5 tokens)\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.253900\n",
    "    * Val: 0.290570\n",
    "    * Test: \n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|:---------------|:------------|:-------------|\n",
    "| BG         |       0.875714 |    0.665102 |     0.73898  |\n",
    "| CZ         |       0.81     |    0.548696 |     0.635217 |\n",
    "| DE         |       0.975809 |    0.88539  |     0.927579 |\n",
    "| EN         |       0.850833 |    0.535625 |     0.623125 |\n",
    "| ES         |       0.9068   |    0.464    |     0.591    |\n",
    "| FI         |       0.895    |    0.770125 |     0.8235   |\n",
    "| FR         |       0.806301 |    0.48875  |     0.592939 |\n",
    "| NL         |       0.870816 |    0.596939 |     0.662449 |\n",
    "| PL         |       0.8894   |    0.6982   |     0.7738   |\n",
    "| SL         |       0.805    |    0.575833 |     0.63875  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: token classification with Huggingface BERT long sequences { #sec-experiment-detection-exp-2-long-sequences }\n",
    "\n",
    "* Validation set: 10% of texts (stratified on language)\n",
    "* Dataset 07-02-2022\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) lenght: 150, step: 150\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.224500\n",
    "    * Val: 0.285791\n",
    "    * Test: 0.4178357720375061\n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|---------------:|------------:|-------------:|\n",
    "| BG         |       0.85     |    0.693673 |     0.744286 |\n",
    "| CZ         |       0.808043 |    0.623261 |     0.685652 |\n",
    "| DE         |       0.971874 |    0.954152 |     0.962806 |\n",
    "| EN         |       0.823333 |    0.618125 |     0.668333 |\n",
    "| ES         |       0.8722   |    0.52     |     0.6254   |\n",
    "| FI         |       0.896625 |    0.785625 |     0.833375 |\n",
    "| FR         |       0.797703 |    0.571588 |     0.651368 |\n",
    "| NL         |       0.875102 |    0.634286 |     0.690204 |\n",
    "| PL         |       0.8872   |    0.7466   |     0.8026   |\n",
    "| SL         |       0.806667 |    0.653333 |     0.692917 |\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "For some texts the sequences of length 150 have to be truncated to fit into the 512 input \n",
    "tokens for BERT. Consequently, we are missing predictions for these truncated tokens. Maybe \n",
    "it is a good idea to decrease the 'step' size, so we'll have predictions for every \n",
    "token. However, this would also mean that we'll have more repetition in the training set.\n",
    "This might impact the results in a negative way. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 reproduced with DVC { #sec-experiment-detection-exp-1-dvc-stratified-language }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [430d228](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/blob/430d22850a4cf00320e8dd62f0d78e4d5aa71c60/reports/detection/experiment_results.md)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) length: size: 35, step: 30\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.2398\n",
    "    * Val: 0.2871749699115753\n",
    "    * Test: 0.4474944472312927\n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|---------------:|------------:|-------------:|\n",
    "| BG         |           0.85 |        0.71 |         0.75 |\n",
    "| CZ         |           0.82 |        0.6  |         0.68 |\n",
    "| DE         |           0.97 |        0.96 |         0.96 |\n",
    "| EN         |           0.81 |        0.6  |         0.66 |\n",
    "| ES         |           0.87 |        0.54 |         0.64 |\n",
    "| FI         |           0.91 |        0.78 |         0.83 |\n",
    "| FR         |           0.8  |        0.63 |         0.69 |\n",
    "| NL         |           0.86 |        0.63 |         0.69 |\n",
    "| PL         |           0.89 |        0.75 |         0.81 |\n",
    "| SL         |           0.81 |        0.62 |         0.67 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 reproduced with DVC, stratified on subset { #sec-experiment-detection-exp-1-dvc-stratified-subset }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [6231bca](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/tree/6231bcae0e20a2182a90892e7184c86f67a2b74f)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) length: size: 35, step: 30\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.2439\n",
    "    * Val: 0.2839458584785461\n",
    "    * Test: 0.4422231018543243\n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|---------------:|------------:|-------------:|\n",
    "| BG         |           0.86 |        0.7  |         0.75 |\n",
    "| CZ         |           0.85 |        0.6  |         0.69 |\n",
    "| DE         |           0.97 |        0.95 |         0.96 |\n",
    "| EN         |           0.82 |        0.61 |         0.67 |\n",
    "| ES         |           0.89 |        0.53 |         0.63 |\n",
    "| FI         |           0.89 |        0.79 |         0.83 |\n",
    "| FR         |           0.81 |        0.62 |         0.69 |\n",
    "| NL         |           0.87 |        0.64 |         0.69 |\n",
    "| PL         |           0.89 |        0.75 |         0.81 |\n",
    "| SL         |           0.81 |        0.64 |         0.68 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 with training batch size 32 (2023-07-28) { #sec-experiment-detection-exp1-training-batch-size-32 }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [8f7329e](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/8f7329e95652a0e3df717844a8edb08054faf075)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) length: size: 35, step: 30\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.2625\n",
    "    * Val: 0.2949527204036712\n",
    "    * Test: 0.4553228318691253\n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|---------------:|------------:|-------------:|\n",
    "| BG         |           0.86 |        0.69 |         0.74 |\n",
    "| CZ         |           0.85 |        0.59 |         0.68 |\n",
    "| DE         |           0.97 |        0.95 |         0.96 |\n",
    "| EN         |           0.83 |        0.59 |         0.66 |\n",
    "| ES         |           0.88 |        0.53 |         0.63 |\n",
    "| FI         |           0.89 |        0.79 |         0.83 |\n",
    "| FR         |           0.8  |        0.61 |         0.67 |\n",
    "| NL         |           0.86 |        0.64 |         0.69 |\n",
    "| PL         |           0.89 |        0.75 |         0.8  |\n",
    "| SL         |           0.82 |        0.63 |         0.68 |\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "Training batch size was set to 32 (instead of 16).\n",
    "Trained on Google Colab T4 High RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 with training batch size 16 (2023-07-28) { #sec-experiment-detection-exp1-train-batch-size-16 }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Normalized editdistance threshold for 'sentences': 0.3 (only for train and val)\n",
    "    * Sequence (sentence) length: size: 35, step: 30\n",
    "* Pretrained model: [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)\n",
    "* Loss\n",
    "    * Train: 0.2439\n",
    "    * Val: 0.2839458584785461\n",
    "    * Test: 0.4422231018543243\n",
    "\n",
    "| language   |   T1_Precision |   T1_Recall |   T1_Fmesure |\n",
    "|:-----------|---------------:|------------:|-------------:|\n",
    "| BG         |           0.86 |        0.7  |         0.75 |\n",
    "| CZ         |           0.85 |        0.6  |         0.69 |\n",
    "| DE         |           0.97 |        0.95 |         0.96 |\n",
    "| EN         |           0.82 |        0.61 |         0.67 |\n",
    "| ES         |           0.89 |        0.53 |         0.63 |\n",
    "| FI         |           0.89 |        0.79 |         0.83 |\n",
    "| FR         |           0.81 |        0.62 |         0.69 |\n",
    "| NL         |           0.87 |        0.64 |         0.69 |\n",
    "| PL         |           0.89 |        0.75 |         0.81 |\n",
    "| SL         |           0.81 |        0.64 |         0.68 |\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "Training batch size was set to 16 again. Trained on Google Colab T4 High RAM.\n",
    "\n",
    "Results are back to what they were for\n",
    "[Experiment 1 reproduced with DVC - stratified on subset](#sec-experiment-detection-exp-1-dvc-stratified-subset)\n",
    "(as expected). It seems that batch size has a small impact on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Error Correction\n",
    "\n",
    "### Task 1 Perfect \n",
    "\n",
    "Summarized results (average % of improvement in edit distance between original and corrected). The input is the 'perfect' results for error detection.\n",
    "\n",
    "| Method | BG | CZ | DE | EN | ES | FI | FR | NL | PL | SL |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| CCC (2019 competition winner) | n/a | n/a | n/a | n/a | n/a | n/a | n/a | n/a | n/a | n/a |\n",
    "| Experiment 1 |  -38 |  -64 |   47 |  -13 |   -1 |   14 |   -8 |   -4 |  -14 |  -34 |\n",
    "| [Experiment 1 reproduced with DVC](#sec-experiment-correction-exp1-dvc) |   14 |  -64 |   21 |   -2 |   17 |   15 |  nan |    8 |   -5 |  -25 |\n",
    "| [Baseline 2 (hidden size 768)](#sec-experiment-correction-baseline-2) |   17 |  -67 |   25 |   -4 |   17 |   21 |  -3 |   10 |   -7 |  -32 |\n",
    "| [byt5-small experiment 1](#sec-experiment-correction-byt5-small-1) |   16 |  -18 |   56 |    2 |    7 |   38 |   11 |    6 |    9 |  -14 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Results\n",
    "\n",
    "Summarized results (average % of improvement in edit distance between original and corrected). The input is the errors detected by a model. The experiment notes specify which error detection model was used.\n",
    "\n",
    "| Method | BG | CZ | DE | EN | ES | FI | FR | NL | PL | SL |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| CCC (2019 competition winner) | 9 | **6** | 24 | **11** | **11** | 8 | **5** | **12** | **17** | **14** |\n",
    "| [Experiment 1 reproduced with DVC](#sec-experiment-correction-exp1-dvc) |   -5 |  -45 |   25 |  -16 |   -6 |   12 |  -13 |   -9 |  -15 |  -37 |\n",
    "| [Baseline 2 (hidden size 768)](#sec-experiment-correction-baseline-2) |   -4 |  -48 |   28 |  -20 |   -7 |   18 |  -13 |   -7 |  -15 |  -47 |\n",
    "| [byt5-small experiment 1](#sec-experiment-correction-byt5-small-1) |   **10** |  -21 |   **52** |   -9 |    1 |   **34** |    2 |   -0 |    5 |  -24 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Correction Experiment Notes\n",
    "\n",
    "#### Correction experiment 1 reproduced with DVC (2023-07-29) { #sec-experiment-correction-exp1-dvc }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [dc2af99](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/dc2af99c1b6f2b643af252b052bcea293280b76c)\n",
    "* Detection model from experiment [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Max token length: 22\n",
    "* Model: `SimpleCorrectionSeq2seq`\n",
    "* Decoder: `GreedySearchDecoder`\n",
    "* Loss\n",
    "    * Train: 8.595188051536567\n",
    "    * Val: 9.212168355464936\n",
    "    * Test: 9.366749288250466\n",
    "\n",
    "Trained on Google Colab T4 High RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction experiment baseline 2 (2023-08-05) { #sec-experiment-correction-baseline-2 }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [45fa416](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/45fa416ddf9257eab1bb07ad530e765d974fbd42)\n",
    "* Detection model from experiment [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Max token length: 22\n",
    "* Model: `SimpleCorrectionSeq2seq`\n",
    "* Decoder: `GreedySearchDecoder`\n",
    "* Loss (Updated run from commit [765a7df](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/765a7dfad223c71572553c9f7b597678aab990e0))\n",
    "    * Train: 7.310251626014709\n",
    "    * Val: 7.631718857658534\n",
    "    * Test: 8.613492756178495 (Updated run from commit [3955d38](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/3955d3847683713cabcc581db61c86bec35b77b2))\n",
    "\n",
    "Set hidden size to 768 to create baseline for an experiment with BERT hidden vectors as additional input.\n",
    "\n",
    "Trained on Google Colab T4 High RAM.\n",
    "\n",
    "Results in table have been recalculated after the problem with nan and -inf for two French texts had been fixed. The results are tracked in DVC in commit [765a7df](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/765a7dfad223c71572553c9f7b597678aab990e0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment byt5-small 1 (2024-03-01) { #sec-experiment-correction-byt5-small-1 }\n",
    "\n",
    "* ocrpostcorrection-notebooks commit: [b677b6b](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/b677b6b4f2097a1eae7f4f374948da3635f5ceba)\n",
    "* Detection model from experiment [9099e78](https://github.com/jvdzwaan/ocrpostcorrection-notebooks/commit/9099e785177a5c5207d01d80422e68d30f39636d)\n",
    "* Dataset\n",
    "    * Split seed: 8232\n",
    "    * Validation set: 10.0%\n",
    "    * Max token length: 22\n",
    "* Model: [byt5-small](https://huggingface.co/docs/transformers/model_doc/byt5)\n",
    "    * Number of epochs: 1\n",
    "* Loss\n",
    "    * Train: 0.6192\n",
    "    * Val: 0.4882390201091766\n",
    "    * Test: 0.5294567942619324\n",
    "\n",
    "Trained on Google Colab T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
