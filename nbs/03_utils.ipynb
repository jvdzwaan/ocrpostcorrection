{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Utils\n",
    "output-file: utils.html\n",
    "description: Util functionality\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import codecs\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ocrpostcorrection.icdar_data import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from datasets import Dataset\n",
    "\n",
    "from ocrpostcorrection.simple_correction_data import SimpleCorrectionDataset\n",
    "from ocrpostcorrection.icdar_data import (\n",
    "    InputToken,\n",
    "    Text,\n",
    "    generate_data,\n",
    "    generate_sentences,\n",
    "    process_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert predictions into ICDAR output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def predictions_to_labels(predictions):\n",
    "    return np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# b x max seq x # classes\n",
    "predictions = np.zeros((16, 10, 3))\n",
    "\n",
    "# Always predict 1\n",
    "predictions[:, :, 1] = 1\n",
    "\n",
    "output = predictions_to_labels(predictions)\n",
    "\n",
    "assert np.array_equal(np.ones((16, 10)), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# shape: b x max seq x # classes\n",
    "predictions = np.array([np.identity(5)])\n",
    "\n",
    "result = predictions_to_labels(predictions)\n",
    "\n",
    "assert np.array_equal(np.array([0, 1, 2, 3, 4]), result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def separate_subtoken_predictions(word_ids, preds):\n",
    "    # print(len(word_ids), word_ids)\n",
    "    result = defaultdict(list)\n",
    "    for word_idx, p_label in zip(word_ids, preds):\n",
    "        # print(word_idx, p_label)\n",
    "        if word_idx is not None:\n",
    "            result[word_idx].append(p_label)\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 0, 0], 1: [0, 0, 1], 2: [1, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "word_ids = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "preds = [0, 0, 0, 0, 0, 1, 1, 2, 2]\n",
    "\n",
    "token_preds = separate_subtoken_predictions(word_ids, preds)\n",
    "print(token_preds)\n",
    "\n",
    "assert token_preds == {0: [0, 0, 0], 1: [0, 0, 1], 2: [1, 2, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def merge_subtoken_predictions(subtoken_predictions):\n",
    "    token_level_predictions = []\n",
    "    for word_idx, preds in subtoken_predictions.items():\n",
    "        token_label = 0\n",
    "        c = Counter(preds)\n",
    "        # print(c)\n",
    "        if c[1] > 0 and c[1] >= c[2]:\n",
    "            token_label = 1\n",
    "        elif c[2] > 0 and c[2] >= c[1]:\n",
    "            token_label = 2\n",
    "\n",
    "        token_level_predictions.append(token_label)\n",
    "    return token_level_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "subtoken_predictions = {\n",
    "    0: [0, 0, 0],  # 0\n",
    "    1: [1, 1, 0],  # 1\n",
    "    2: [1, 2],  # 1\n",
    "    3: [2, 2, 1],  # 2\n",
    "    4: [0, 1, 2],  # 1\n",
    "    5: [0, 1, 0],\n",
    "}  # 1\n",
    "\n",
    "token_preds = merge_subtoken_predictions(subtoken_predictions)\n",
    "print(token_preds)\n",
    "\n",
    "assert [0, 1, 1, 2, 1, 1] == token_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def gather_token_predictions(preds):\n",
    "    \"\"\"Gather potentially overlapping token predictions\"\"\"\n",
    "    labels = defaultdict(list)\n",
    "\n",
    "    # print(len(text.input_tokens))\n",
    "    # print(preds)\n",
    "    for start, lbls in preds.items():\n",
    "        for i, label in enumerate(lbls):\n",
    "            labels[int(start) + i].append(label)\n",
    "    # print('LABELS')\n",
    "    # print(labels)\n",
    "    return dict(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "token_predictions = {0: [0, 0, 0, 0, 0], 1: [0, 0, 0, 0, 0], 2: [0, 0, 0, 0, 0]}\n",
    "actual = gather_token_predictions(token_predictions)\n",
    "expected = {\n",
    "    0: [0],\n",
    "    1: [0, 0],\n",
    "    2: [0, 0, 0],\n",
    "    3: [0, 0, 0],\n",
    "    4: [0, 0, 0],\n",
    "    5: [0, 0],\n",
    "    6: [0],\n",
    "}\n",
    "\n",
    "assert expected == actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def labels2label_str(labels, text_key):\n",
    "    label_str = []\n",
    "    i = 0\n",
    "\n",
    "    for token in labels:\n",
    "        # print(i, token)\n",
    "        while i < token:\n",
    "            logger.warning(f'Missing predictions for token {i} in \"{text_key}\"')\n",
    "            # Predictions are missing (input text was truncated)\n",
    "            # Add 0 to make sure token indices remain correct\n",
    "            label_str.append(\"0\")\n",
    "            i += 1\n",
    "\n",
    "        if 2 in labels[i]:\n",
    "            label_str.append(\"2\")\n",
    "        elif 1 in labels[i]:\n",
    "            label_str.append(\"1\")\n",
    "        else:\n",
    "            label_str.append(\"0\")\n",
    "        i += 1\n",
    "\n",
    "    label_str = \"\".join(label_str)\n",
    "    return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "labels = {0: [0], 1: [1], 2: [2], 3: [0, 0, 1], 4: [0, 1, 2]}\n",
    "\n",
    "label_str = labels2label_str(labels, \"test1\")\n",
    "\n",
    "assert label_str == \"01212\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 18:01:30.225 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 3 in \"test2\"\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "labels = {0: [0], 1: [1], 2: [2], 4: [0, 0, 1], 5: [0, 1, 2]}\n",
    "\n",
    "label_str = labels2label_str(labels, \"test2\")\n",
    "\n",
    "assert label_str == \"012012\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 18:01:30.229 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 4 in \"test3\"\n",
      "2023-08-06 18:01:30.229 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 5 in \"test3\"\n",
      "2023-08-06 18:01:30.230 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 6 in \"test3\"\n",
      "2023-08-06 18:01:30.230 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 7 in \"test3\"\n",
      "2023-08-06 18:01:30.230 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 8 in \"test3\"\n",
      "2023-08-06 18:01:30.230 | WARNING  | __main__:labels2label_str:9 - Missing predictions for token 9 in \"test3\"\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "labels = {0: [0], 1: [1], 2: [2], 3: [0, 0, 1], 10: [0, 1, 2]}\n",
    "\n",
    "label_str = labels2label_str(labels, \"test3\")\n",
    "\n",
    "assert label_str == \"01210000002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_icdar_output(label_str, input_tokens):\n",
    "    keys = {}\n",
    "    started = False\n",
    "    start_idx = -1\n",
    "    num_tokens = 0\n",
    "    for input_token, label in zip(input_tokens, label_str):\n",
    "        if label == \"1\":\n",
    "            if started:\n",
    "                keys[start_idx] = num_tokens\n",
    "                started = False\n",
    "                start_idx = -1\n",
    "                num_tokens = 0\n",
    "\n",
    "            started = True\n",
    "            start_idx = input_token.start\n",
    "            num_tokens += 1\n",
    "        elif label == \"2\":\n",
    "            if not started:\n",
    "                started = True\n",
    "                start_idx = input_token.start\n",
    "            num_tokens += 1\n",
    "        else:\n",
    "            # label = '0'\n",
    "            if started:\n",
    "                keys[start_idx] = num_tokens\n",
    "                started = False\n",
    "                start_idx = -1\n",
    "                num_tokens = 0\n",
    "    # Add final ocr mistake\n",
    "    if started:\n",
    "        keys[start_idx] = num_tokens\n",
    "\n",
    "    text_output = {}\n",
    "    for offset, num_tokens in keys.items():\n",
    "        text_output[f\"{offset}:{num_tokens}\"] = {}\n",
    "\n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"1\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"bal\",\n",
    "        gs=\"bla\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    )\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"0:1\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "label_str = \"01\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"4:1\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"12\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"0:2\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"112\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=8,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"0:1\": {}, \"4:2\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"02\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"4:1\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"101\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tree\",\n",
    "        gs=\"three\",\n",
    "        start=9,\n",
    "        len_ocr=4,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"0:1\": {}, \"9:1\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Overlapping predictions\n",
    "label_str = \"122\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tree\",\n",
    "        gs=\"three\",\n",
    "        start=9,\n",
    "        len_ocr=4,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {\"0:3\": {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _predictions2label_str(samples, predictions, tokenizer):\n",
    "    \"\"\"Convert predictions into label strings\"\"\"\n",
    "    # print('samples', len(samples))\n",
    "    # print(samples)\n",
    "    # print(samples[0].keys())\n",
    "    # for sample in samples:\n",
    "    #    print(sample.keys())\n",
    "\n",
    "    tokenized_samples = tokenizer(\n",
    "        samples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    # print(samples)\n",
    "\n",
    "    # for sample in samples:\n",
    "    #    print(sample.keys())\n",
    "\n",
    "    # convert predictions to labels (label_ids)\n",
    "    # p = np.argmax(predictions, axis=2)\n",
    "    # print(p)\n",
    "\n",
    "    converted = defaultdict(dict)\n",
    "\n",
    "    for i, (sample, preds) in enumerate(zip(samples, predictions)):\n",
    "        # print(sample.keys())\n",
    "        # label = sample['tags']\n",
    "        # print(label)\n",
    "        # print(len(preds), preds)\n",
    "        word_ids = tokenized_samples.word_ids(\n",
    "            batch_index=i\n",
    "        )  # Map tokens to their respective word.\n",
    "        result = separate_subtoken_predictions(word_ids, preds)\n",
    "        new_tags = merge_subtoken_predictions(result)\n",
    "\n",
    "        # print('pred', len(new_tags), new_tags)\n",
    "        # print('tags', len(label), label)\n",
    "\n",
    "        # print(sample)\n",
    "        # print(sample['key'], sample['start_token_id'])\n",
    "        converted[sample[\"key\"]][sample[\"start_token_id\"]] = new_tags\n",
    "\n",
    "    output = {}\n",
    "    for key, preds in converted.items():\n",
    "        labels = defaultdict(list)\n",
    "        # print(key)\n",
    "        labels = gather_token_predictions(preds)\n",
    "        label_str = labels2label_str(labels, key)\n",
    "        output[key] = label_str\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def predictions2icdar_output(samples, predictions, tokenizer, data_test):\n",
    "    \"\"\"Convert predictions into icdar output format\"\"\"\n",
    "    converted = _predictions2label_str(samples, predictions, tokenizer)\n",
    "    output = {}\n",
    "    for key, label_str in converted.items():\n",
    "        try:\n",
    "            text = data_test[key]\n",
    "            output[key] = extract_icdar_output(label_str, text.input_tokens)\n",
    "        except KeyError:\n",
    "            logger.warning(f\"No data found for text {key}\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "bert_base_model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_base_model_name)\n",
    "\n",
    "tokens = [\"the\" for i in range(1000)]\n",
    "\n",
    "r = tokenizer(tokens, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 733.98it/s]\n",
      "4it [00:00, 2491.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "# Create tokenizer\n",
    "bert_base_model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_base_model_name)\n",
    "\n",
    "# Create data\n",
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "data, md = generate_data(data_dir)\n",
    "sentence_df = generate_sentences(md, data, size=2, step=1)\n",
    "dataset = Dataset.from_pandas(sentence_df)\n",
    "\n",
    "# Create predictions\n",
    "\n",
    "# b x max seq x # classes\n",
    "predictions = np.zeros((len(dataset), 10, 3))\n",
    "\n",
    "# Always predict 1\n",
    "predictions[:, :, 1] = 1\n",
    "predictions = predictions_to_labels(predictions)\n",
    "\n",
    "# Generate icdar output (task 1)\n",
    "actual = predictions2icdar_output(dataset, predictions, tokenizer, data)\n",
    "\n",
    "# Expected output has an entry of lenght 1 for every input token\n",
    "expected = defaultdict(dict)\n",
    "for key, text in data.items():\n",
    "    for token in text.input_tokens:\n",
    "        expected[key][f\"{token.start}:1\"] = {}\n",
    "\n",
    "assert dict(expected) == actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert predictions into entities output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def create_entity(entity_tokens):\n",
    "    start = entity_tokens[0].start\n",
    "    end = entity_tokens[-1].start + entity_tokens[-1].len_ocr\n",
    "    word = \" \".join([token.ocr for token in entity_tokens])\n",
    "    return {\"entity\": \"OCR mistake\", \"word\": word, \"start\": start, \"end\": end}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the entitiy output format, an entity looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'OCR mistake', 'word': 'one tow', 'start': 0, 'end': 7}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "create_entity(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity output format consists of a list of such entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "entity_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "expected = {\"entity\": \"OCR mistake\", \"word\": \"one tow\", \"start\": 0, \"end\": 7}\n",
    "actual = create_entity(entity_tokens)\n",
    "assert actual == expected, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_entity_output(label_str: str, input_tokens):\n",
    "    \"\"\"Convert label string to the entity output format\"\"\"\n",
    "    entity_tokens = []\n",
    "    entities = []\n",
    "    for token, label in zip(input_tokens, label_str):\n",
    "        if label == \"0\":\n",
    "            if len(entity_tokens) > 0:\n",
    "                entities.append(create_entity(entity_tokens))\n",
    "                entity_tokens = []\n",
    "        elif label == \"1\":\n",
    "            if len(entity_tokens) > 0:\n",
    "                entities.append(create_entity(entity_tokens))\n",
    "                entity_tokens = []\n",
    "            entity_tokens.append(token)\n",
    "        elif label == \"2\":\n",
    "            entity_tokens.append(token)\n",
    "\n",
    "    # Final token\n",
    "    if len(entity_tokens) > 0:\n",
    "        entities.append(create_entity(entity_tokens))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "label_str = \"02\"\n",
    "input_tokens = [\n",
    "    InputToken(\n",
    "        ocr=\"one\",\n",
    "        gs=\"one\",\n",
    "        start=0,\n",
    "        len_ocr=3,\n",
    "        label=0,\n",
    "    ),\n",
    "    InputToken(\n",
    "        ocr=\"tow\",\n",
    "        gs=\"two\",\n",
    "        start=4,\n",
    "        len_ocr=3,\n",
    "        label=1,\n",
    "    ),\n",
    "]\n",
    "expected = [{\"entity\": \"OCR mistake\", \"word\": \"tow\", \"start\": 4, \"end\": 7}]\n",
    "actual = extract_entity_output(label_str, input_tokens)\n",
    "assert expected == actual, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def predictions2entity_output(samples, predictions, tokenizer, data_test):\n",
    "    \"\"\"Convert predictions into entity output format\"\"\"\n",
    "    converted = _predictions2label_str(samples, predictions, tokenizer)\n",
    "    output = {}\n",
    "    for key, label_str in converted.items():\n",
    "        try:\n",
    "            text = data_test[key]\n",
    "            output[key] = extract_entity_output(label_str, text.input_tokens)\n",
    "        except KeyError:\n",
    "            logger.warning(f\"No data found for text {key}\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 996.15it/s]\n",
      "4it [00:00, 2094.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "# Create tokenizer\n",
    "bert_base_model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_base_model_name)\n",
    "\n",
    "# Create data\n",
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "data, md = generate_data(data_dir)\n",
    "sentence_df = generate_sentences(md, data, size=2, step=1)\n",
    "dataset = Dataset.from_pandas(sentence_df)\n",
    "\n",
    "# Create predictions\n",
    "\n",
    "# b x max seq x # classes\n",
    "predictions = np.zeros((len(dataset), 10, 3))\n",
    "\n",
    "# Always predict 1\n",
    "predictions[:, :, 1] = 1\n",
    "predictions = predictions_to_labels(predictions)\n",
    "\n",
    "# Generate icdar output (task 1)\n",
    "actual = predictions2entity_output(dataset, predictions, tokenizer, data)\n",
    "\n",
    "# Expected output has an entity for every input token\n",
    "expected = defaultdict(dict)\n",
    "for key, text in data.items():\n",
    "    entities = [create_entity([token]) for token in text.input_tokens]\n",
    "    expected[key] = entities\n",
    "\n",
    "assert expected == actual, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_perfect_icdar_output(data):\n",
    "    output = {}\n",
    "    for key, text_obj in data.items():\n",
    "        label_str = \"\".join([str(t.label) for t in text_obj.input_tokens])\n",
    "        output[key] = extract_icdar_output(label_str, data[key].input_tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example.txt\"\n",
    "text = process_text(in_file)\n",
    "\n",
    "test_input = {\"key\": text}\n",
    "\n",
    "actual = create_perfect_icdar_output(test_input)\n",
    "\n",
    "# Indices (the first number) refer to the ocr input text\n",
    "assert actual == {\"key\": {\"8:1\": {}, \"10:1\": {}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the ICDAR evaluation script\n",
    "\n",
    "This code was taken from the original [evalTool_ICDAR2017.py](https://git.univ-lr.fr/gchiro01/icdar2017/blob/master/evalTool_ICDAR2017.py) (CC0 License) via [Kotwic4/ocr-correction](https://github.com/Kotwic4/ocr-correction/blob/master/ocr_correction/dataset/icdar/evalTool_ICDAR2017.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "maxNbCandidate = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "################# CLASS FOR STORING CURRENT FILE CONTEXT  ################\n",
    "class EvalContext:\n",
    "    # Default symbols used for the alignment and for ignoring some tokens\n",
    "    charExtend = r\"@\"\n",
    "    charIgnore = r\"#\"\n",
    "\n",
    "    # Different texts versions provided\n",
    "    ocrAligned, gsAligned, ocrOriginal = \"\", \"\", \"\"\n",
    "\n",
    "    # Alignment map (ocrOriginal <=> ocrAligned and gsAligned)\n",
    "    aMap = []\n",
    "\n",
    "    def __init__(self, filePath, verbose=False):\n",
    "        assert os.path.exists(filePath), \"[ERROR] : File %s not found !\" % filePath\n",
    "\n",
    "        self.filePath = filePath\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Load file data\n",
    "        with open(filePath, \"r\") as f:\n",
    "            text = f.read().strip()\n",
    "            self.ocrOriginal, self.ocrAligned, self.gsAligned = [\n",
    "                txt[14:] for txt in re.split(r\"\\r?\\n\", text)\n",
    "            ]\n",
    "            # text.strip() removes trailing space from gs aligned, but not from the other texts.\n",
    "            # This causes problems when calculating recall. The solution is to also remove\n",
    "            # trailing space from ocr original and ocr aligned.\n",
    "            self.ocrOriginal = self.ocrOriginal.rstrip()\n",
    "            self.ocrAligned = self.ocrAligned.rstrip()\n",
    "\n",
    "            if self.charExtend in self.ocrOriginal:\n",
    "                print(f\"{self.charExtend} found in ocrOriginal. Removing...\")\n",
    "                self.ocrOriginal = self.ocrOriginal.replace(self.charExtend, \"\")\n",
    "\n",
    "        # Check file integrity\n",
    "        assert (\n",
    "            self.ocrOriginal == re.sub(self.charExtend, \"\", self.ocrAligned).rstrip()\n",
    "        ), (\n",
    "            '[ERROR] : [OCR_aligned] without \"%s\" doesn\\'t correspond to [OCR_toInput] in file %s'\n",
    "            % (self.charExtend, filePath)\n",
    "        )\n",
    "\n",
    "        # Build the alignment map\n",
    "        self.aMap = [\n",
    "            x.start() - i\n",
    "            for i, x in enumerate(re.finditer(self.charExtend + r\"|$\", self.ocrAligned))\n",
    "        ]\n",
    "\n",
    "        # print(\"%s\\n%s\\n%s\" % (self.ocrOriginal, self.ocrAligned, self.gsAligned))\n",
    "\n",
    "    # Get the alignment shift for a position in the orginal OCR to the corresponding postion in the aligned OCR/GS\n",
    "    def get_aligned_shift(self, posOriginal):\n",
    "        return next((i for i, e in enumerate(self.aMap) if e >= posOriginal), 0)\n",
    "\n",
    "    # Get the alignment shift for a position in the orginal OCR to the corresponding postion in the aligned OCR/GS\n",
    "    def get_original_shift(self, posAligned):\n",
    "        return self.ocrAligned.count(self.charExtend, 0, posAligned)\n",
    "\n",
    "    # Get bounds in \"Aligned OCR/GS\" from a token position in the non-aligned OCR.\n",
    "    def get_aligned_token_bounds(self, tokenPos, nbToken=1):\n",
    "        assert (tokenPos == 0) or (\n",
    "            self.ocrOriginal[tokenPos - 1] == \" \"\n",
    "        ), \"[ERROR] : %d is not a token start position (%s)\" % (tokenPos, self.filePath)\n",
    "\n",
    "        alignedPos = tokenPos + self.get_aligned_shift(tokenPos)\n",
    "        seqLen = nbToken - 1  # Init with number of spaces\n",
    "        iterOcrAlignedSpace = re.finditer(r\"$|\\ \", self.ocrAligned[alignedPos:])\n",
    "        for nbt in range(nbToken):\n",
    "            matchSpace = next(iterOcrAlignedSpace, None)\n",
    "            if matchSpace is None:\n",
    "                print(\n",
    "                    \"[WARNING] : At pos %d, could not iterate forward over tokens, end of the sequence reached\"\n",
    "                    % tokenPos\n",
    "                )\n",
    "                break\n",
    "            seqLen = matchSpace.start()  # Look for last space before next token\n",
    "\n",
    "        return alignedPos, seqLen\n",
    "\n",
    "    # Get statistics (erroneous tokens' posisitions, corrections, ect...) on errors\n",
    "    def get_errors_stats(self):\n",
    "        # results = {}\n",
    "        nbTokens, nbErrTokens, nbErrTokensAlpha = 0, 0, 0\n",
    "\n",
    "        # Iterate over GS tokens\n",
    "        lastTokenPos = 0\n",
    "        for spacePos in re.finditer(r\"$|\\ \", self.gsAligned):\n",
    "            tokenEndPos = spacePos.start()\n",
    "            tokenInOcr = re.sub(\n",
    "                self.charExtend, \"\", self.ocrAligned[lastTokenPos:tokenEndPos+1]\n",
    "            )\n",
    "            tokenInGs = re.sub(\n",
    "                self.charExtend, \"\", self.gsAligned[lastTokenPos:tokenEndPos+1]\n",
    "            )\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Indices aligned: start: {lastTokenPos}, end: {tokenEndPos}\")\n",
    "                print(f\"Token in ocr: '{tokenInOcr}' (aligned: '{self.ocrAligned[lastTokenPos:tokenEndPos+1]}')\")\n",
    "                print(f\"Token in gs: '{tokenInGs}' (aligned: '{self.gsAligned[lastTokenPos:tokenEndPos+1]}')\")\n",
    "                print('---')\n",
    "\n",
    "            if self.charIgnore in tokenInGs:\n",
    "                lastTokenPos = tokenEndPos + 1\n",
    "                continue\n",
    "\n",
    "            # if (tokenInOcr != tokenInGs):\n",
    "            #    results[lastTokenPos] = [tokenInOcr, tokenInGs]\n",
    "            #    #print(\"_%s_%s_\" % (tokenInOcr, tokenInGs))\n",
    "\n",
    "            lastTokenPos = tokenEndPos + 1\n",
    "\n",
    "            nbTokens += 1\n",
    "            nbErrTokens += tokenInOcr != tokenInGs\n",
    "            nbErrTokensAlpha += (\n",
    "                tokenInOcr != tokenInGs\n",
    "            ) and tokenInGs.strip().isalpha()\n",
    "\n",
    "        return nbTokens, nbErrTokens, nbErrTokensAlpha\n",
    "\n",
    "    def collectErrorPos(self):\n",
    "        errorList = []\n",
    "\n",
    "        # Add tolerance to hyphens : work on a new GS where hyphens founds in OCR are considered to be ignored in task1\n",
    "        gsAlignedHyphensIgnored = self.gsAligned\n",
    "        for hToken in re.finditer(\n",
    "            r\"[^ ]*((\\ ?-[^ ])|([^ ]-\\ ?))[^ ]*\", self.ocrAligned\n",
    "        ):\n",
    "            gsAlignedHyphensIgnored = (\n",
    "                gsAlignedHyphensIgnored[: hToken.start()]\n",
    "                + self.charIgnore * (hToken.end() - hToken.start())\n",
    "                + gsAlignedHyphensIgnored[hToken.end() :]\n",
    "            )\n",
    "\n",
    "        gsSpacePos = set(\n",
    "            [\n",
    "                spacePos.start()\n",
    "                for spacePos in re.finditer(r\"^|$|\\ \", gsAlignedHyphensIgnored)\n",
    "            ]\n",
    "        )\n",
    "        ocrSpacePos = set(\n",
    "            [spacePos.start() for spacePos in re.finditer(r\"^|$|\\ \", self.ocrAligned)]\n",
    "        )\n",
    "        commonSpacePos = sorted(gsSpacePos.intersection(ocrSpacePos))\n",
    "        # print(commonSpacePos)\n",
    "\n",
    "        for i in range(len(commonSpacePos) - 1):\n",
    "            tokenStartPos = commonSpacePos[i] + 1 * (\n",
    "                gsAlignedHyphensIgnored[commonSpacePos[i]] == \" \"\n",
    "            )\n",
    "            tokenEndPos = commonSpacePos[i + 1]\n",
    "\n",
    "            tokenInGs = re.sub(\n",
    "                self.charExtend, \"\", gsAlignedHyphensIgnored[tokenStartPos:tokenEndPos]\n",
    "            )\n",
    "            tokenInOcr = re.sub(\n",
    "                self.charExtend, \"\", self.ocrAligned[tokenStartPos:tokenEndPos]\n",
    "            )\n",
    "\n",
    "            # Get not aligned pos\n",
    "            tokenStartPosOriginal = tokenStartPos - self.get_original_shift(\n",
    "                tokenStartPos\n",
    "            )\n",
    "\n",
    "            # Ignore the \"#\" in GS\n",
    "            if not (self.charIgnore in tokenInGs):\n",
    "                if tokenInOcr != tokenInGs:\n",
    "                    # print(\"%d:%d|%d=%s=>%s\" % (tokenStartPosOriginal, tokenInOcr.count(\" \")+1, tokenStartPos , tokenInOcr, tokenInGs))\n",
    "                    errorList.append(\n",
    "                        \"%d:%d\" % (tokenStartPosOriginal, tokenInOcr.count(\" \") + 1)\n",
    "                    )\n",
    "                    # errorList.append(\"%d:%d|%d=%s=>%s\" % (tokenStartPosOriginal, tokenInOcr.count(\" \") + 1, tokenStartPos, tokenInOcr, tokenInGs))\n",
    "\n",
    "        return errorList\n",
    "\n",
    "    def task1_eval(self, inputErroneousTokens, print_sets=False):\n",
    "        # Add tolerance to hyphens : work on a new GS where hyphens founds in OCR are considered to be ignored in task1\n",
    "        gsAlignedHyphensIgnored = self.gsAligned\n",
    "        for hToken in re.finditer(\n",
    "            r\"[^ ]*((\\ ?-[^ ])|([^ ]-\\ ?))[^ ]*\", self.ocrAligned\n",
    "        ):\n",
    "            gsAlignedHyphensIgnored = (\n",
    "                gsAlignedHyphensIgnored[: hToken.start()]\n",
    "                + self.charIgnore * (hToken.end() - hToken.start())\n",
    "                + gsAlignedHyphensIgnored[hToken.end() :]\n",
    "            )\n",
    "\n",
    "        # 1) Prepare input results : unfold overlapping n tokens in tokenPosErr\n",
    "        detectedErrPosUnfolded = {}\n",
    "        for errPos, val in inputErroneousTokens.items():\n",
    "            tokenStartPos = 0\n",
    "            iterTokens = re.finditer(r\"$|\\ \", self.ocrOriginal[errPos:])\n",
    "            for t in range(val[\"nbToken\"]):\n",
    "                alignedPos, seqLen = self.get_aligned_token_bounds(\n",
    "                    (errPos + tokenStartPos), 1\n",
    "                )\n",
    "                rawTokenInAlignedGs = gsAlignedHyphensIgnored[\n",
    "                    alignedPos : (alignedPos + seqLen + 1)\n",
    "                ]\n",
    "\n",
    "                if not (self.charIgnore in rawTokenInAlignedGs):\n",
    "                    # Check if there is no overlapping errors/corrections\n",
    "                    assert alignedPos not in detectedErrPosUnfolded, (\n",
    "                        \"[ERROR] : Error at pos %d is overlapping another given error ! Pay attention to the number of overlapping tokens.\"\n",
    "                        % (errPos + tokenStartPos)\n",
    "                    )\n",
    "                    detectedErrPosUnfolded[alignedPos] = [\n",
    "                        rawTokenInAlignedGs,\n",
    "                        val[\"candidates\"],\n",
    "                    ]\n",
    "\n",
    "                tokenEndMatch = next(iterTokens, None)\n",
    "                if tokenEndMatch is None:\n",
    "                    break\n",
    "\n",
    "                tokenStartPos = tokenEndMatch.start() + 1\n",
    "\n",
    "        if self.verbose:\n",
    "            EvalContext.printDicoSortedByKey(\n",
    "                detectedErrPosUnfolded, \"1) detectedErrPosUnfolded\"\n",
    "            )\n",
    "\n",
    "        # 2) Prepare real results\n",
    "        realErrPosUnfolded = {}\n",
    "        iterTokens = re.finditer(r\"$|\\ \", self.ocrAligned)\n",
    "        tokenStartPos = 0\n",
    "        tokenEndMatch = next(iterTokens, None)\n",
    "\n",
    "        while tokenEndMatch is not None:\n",
    "            tokenEndPos = tokenEndMatch.start() + 1  # Include following char\n",
    "\n",
    "            tokenInGs = re.sub(\n",
    "                self.charExtend,\n",
    "                \"\",\n",
    "                gsAlignedHyphensIgnored[max(0, tokenStartPos - 1) : tokenEndPos],\n",
    "            )\n",
    "            tokenInOcr = re.sub(\n",
    "                self.charExtend,\n",
    "                \"\",\n",
    "                self.ocrAligned[max(0, tokenStartPos - 1) : tokenEndPos],\n",
    "            )\n",
    "\n",
    "            # Ignore the \"#\" in GS\n",
    "            if not (self.charIgnore in tokenInGs):\n",
    "                if tokenInOcr != tokenInGs:\n",
    "                    realErrPosUnfolded[tokenStartPos] = [tokenInOcr, tokenInGs]\n",
    "\n",
    "            tokenStartPos = tokenEndPos\n",
    "            tokenEndMatch = next(iterTokens, None)\n",
    "\n",
    "        if self.verbose:\n",
    "            EvalContext.printDicoSortedByKey(\n",
    "                realErrPosUnfolded, \"2) realErrPosUnfolded\"\n",
    "            )\n",
    "\n",
    "        realErrPos = set(realErrPosUnfolded.keys())\n",
    "\n",
    "        # 3) Compute classical metrics\n",
    "        setErrPos = set(detectedErrPosUnfolded.keys())\n",
    "        errTP = len(setErrPos.intersection(realErrPos))  # TruePositive\n",
    "        errFP = len(setErrPos.difference(realErrPos))  # TrueNegative\n",
    "        errFN = len(realErrPos.difference(setErrPos))  # FalseNegative\n",
    "\n",
    "        # Possible division per 0\n",
    "        prec = (errTP / float(errTP + errFP)) if (errTP + errFP) > 0 else 0\n",
    "        recall = (errTP / float(errTP + errFN)) if (errTP + errFN) > 0 else 0\n",
    "        fmes = (\n",
    "            (2.0 * float(prec * recall) / float(prec + recall))\n",
    "            if (prec + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Debug test\n",
    "        if self.verbose or print_sets:\n",
    "            print(\n",
    "                \"TASK 1) ErrTP %d / errFP %d / errFN %d /\" % (errTP, errFP, errFN)\n",
    "                + \" prec %0.2f / recall %0.2f / fmes %0.2f\" % (prec, recall, fmes)\n",
    "            )\n",
    "        if print_sets:\n",
    "            print(\"False positives:\", setErrPos.difference(realErrPos))\n",
    "            print(\"False negatives:\", realErrPos.difference(setErrPos))\n",
    "\n",
    "        return prec, recall, fmes\n",
    "\n",
    "    def task2_eval(self, inputErroneousTokens, useFirstCandidateOnly=False):\n",
    "        # Init list of tokens' levenshtein distances\n",
    "        originalDistance, correctedDistance = [0], [0]\n",
    "        nbSymbolsConsidered = 0\n",
    "\n",
    "        # Get tokens (or sequences including hyphens)\n",
    "        splitRegExp = r\"(?=([^-]\\ [^-]))\"  # Now supporting overlapping\n",
    "        spaceOCR = set(\n",
    "            [sp.start() + 1 for sp in re.finditer(splitRegExp, self.ocrAligned)]\n",
    "        )\n",
    "        spaceGS = set(\n",
    "            [sp.start() + 1 for sp in re.finditer(splitRegExp, self.gsAligned)]\n",
    "        )\n",
    "\n",
    "        # Collect running tokens defined in correction...\n",
    "        spaceCorToRemove = []\n",
    "        inputErroneousTokensAligned = {}\n",
    "        for p, details in inputErroneousTokens.items():\n",
    "            posAligned = p + self.get_aligned_shift(p)\n",
    "            inputErroneousTokensAligned[posAligned] = details\n",
    "            for m in itertools.islice(\n",
    "                re.finditer(r\"$|\\ \", self.ocrAligned[posAligned:]),\n",
    "                details[\"nbToken\"] - 1,\n",
    "            ):\n",
    "                spaceCorToRemove.append(m.start() + posAligned)\n",
    "\n",
    "        # print(\"spaceCommon %s\" % str(sorted(spaceOCR.intersection(spaceGS))))\n",
    "        # print(\"spaceCorToRemove %s\" % str(sorted(spaceCorToRemove)))\n",
    "\n",
    "        spaceCommon = (spaceOCR.intersection(spaceGS)).difference(spaceCorToRemove)\n",
    "        spaceCommon.add(len(self.ocrAligned))\n",
    "\n",
    "        # Iterate over comparable sequences\n",
    "        lastTokenStartPos = 0\n",
    "        for tokenEndPos in sorted(spaceCommon):\n",
    "            tokenInGs = self.gsAligned[lastTokenStartPos:tokenEndPos]\n",
    "            tokenInOcr = self.ocrAligned[lastTokenStartPos:tokenEndPos]\n",
    "\n",
    "            # Get corrections concerned by this sequence :\n",
    "            tokensPosToCorrect = set(\n",
    "                range(lastTokenStartPos, tokenEndPos)\n",
    "            ).intersection(inputErroneousTokensAligned.keys())\n",
    "\n",
    "            listCombinaisons = []\n",
    "            for p in tokensPosToCorrect:\n",
    "                listCombinaisons.append(\n",
    "                    [\n",
    "                        [p, c]\n",
    "                        for c, w in inputErroneousTokensAligned[p][\"candidates\"].items()\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            tokenProposed = {}\n",
    "            for combi in itertools.product(*iter(listCombinaisons)):\n",
    "                # Default\n",
    "                corToken = list(tokenInOcr)\n",
    "\n",
    "                prodWeight = 1.0\n",
    "                for pc in sorted(combi, key=lambda k: k[0], reverse=True):\n",
    "                    offsetStart = (\n",
    "                        inputErroneousTokensAligned[pc[0]][\"boundsAligned\"][0]\n",
    "                        - lastTokenStartPos\n",
    "                    )\n",
    "                    offsetEnd = (\n",
    "                        offsetStart\n",
    "                        + inputErroneousTokensAligned[pc[0]][\"boundsAligned\"][1]\n",
    "                    )\n",
    "\n",
    "                    corToken[offsetStart:offsetEnd] = pc[1]\n",
    "                    prodWeight = (\n",
    "                        prodWeight\n",
    "                        * inputErroneousTokensAligned[pc[0]][\"candidates\"][pc[1]]\n",
    "                    )\n",
    "\n",
    "                tokenProposed[\n",
    "                    re.sub(self.charExtend, \"\", \"\".join(corToken))\n",
    "                ] = prodWeight\n",
    "\n",
    "            # Fix GC 15/06/2017\n",
    "            if len(tokenProposed) == 0:\n",
    "                tokenProposed = {tokenInOcr: 1.0}\n",
    "\n",
    "            # In case we want to consider only the highest candidate\n",
    "            if useFirstCandidateOnly and len(tokenProposed) > 0:\n",
    "                tokenProposed = {max(tokenProposed, key=tokenProposed.get): 1.0}\n",
    "\n",
    "            # Consider results only if no \"#\" found in the token (or tokens sequence).\n",
    "            if not (self.charIgnore in tokenInGs):\n",
    "                # Update damerau_levenshtein_distance distances' lists\n",
    "                ignoreList = [\" -\", \"- \", \"-\", self.charExtend]\n",
    "                originalDistance.append(\n",
    "                    EvalContext.damerau_levenshtein(\n",
    "                        tokenInOcr, tokenInGs, ignoreList=ignoreList\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                weightedSum = sum(\n",
    "                    [\n",
    "                        EvalContext.damerau_levenshtein(\n",
    "                            token, tokenInGs, ignoreList=ignoreList\n",
    "                        )\n",
    "                        * float(w)\n",
    "                        for token, w in tokenProposed.items()\n",
    "                    ]\n",
    "                )\n",
    "                correctedDistance.append(weightedSum)\n",
    "                nbSymbolsConsidered += len(tokenInOcr)\n",
    "\n",
    "            else:\n",
    "                # print(\"[IGNORED] Token _%s_%s_ => %s\" % (tokenInOcr, tokenInGs, tokenProposed))\n",
    "                pass\n",
    "\n",
    "            # print(\"(%d=>%d) %s | %s | %s \" % (lastTokenStartPos,tokenEndPos, re.sub(self.charExtend, \"\", tokenInOcr), re.sub(self.charExtend, \"\", tokenInGs), str(tokenProposed) ))\n",
    "            lastTokenStartPos = tokenEndPos + 1\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"TASK 2) correctedDistance %d vs originalDistance %d\"\n",
    "                % (sum(correctedDistance), sum(originalDistance))\n",
    "            )\n",
    "\n",
    "        # print(correctedDistance)\n",
    "\n",
    "        return sum(correctedDistance), sum(originalDistance), nbSymbolsConsidered\n",
    "\n",
    "    # --- Damerau-Levenshtein distance between 2 strings ---\n",
    "    # Slightly modified version of https://github.com/jamesturk/jellyfish\n",
    "    # under Copyright 2015, James TurkJames Turk, Sunlight Foundation\n",
    "    # with LICENSE BSD 2: https://github.com/jamesturk/jellyfish/blob/master/LICENSE\n",
    "    @staticmethod\n",
    "    def damerau_levenshtein(s1, s2, ignoreList=[\" -\", \"- \", \"-\"]):\n",
    "        # Add tolerence on some characters (e.g. hyphens) cause GS is not always perfect.\n",
    "        s1 = re.sub(\"(\" + \")|(\".join(ignoreList) + \")\", \"\", s1)\n",
    "        s2 = re.sub(\"(\" + \")|(\".join(ignoreList) + \")\", \"\", s2)\n",
    "\n",
    "        if s1 == s2:\n",
    "            return 0\n",
    "\n",
    "        len1 = len(s1)\n",
    "        len2 = len(s2)\n",
    "        infinite = len1 + len2\n",
    "\n",
    "        # character array\n",
    "        da = collections.defaultdict(int)\n",
    "\n",
    "        # distance matrix\n",
    "        score = [[0] * (len2 + 2) for x in range(len1 + 2)]\n",
    "\n",
    "        score[0][0] = infinite\n",
    "        for i in range(0, len1 + 1):\n",
    "            score[i + 1][0] = infinite\n",
    "            score[i + 1][1] = i\n",
    "        for i in range(0, len2 + 1):\n",
    "            score[0][i + 1] = infinite\n",
    "            score[1][i + 1] = i\n",
    "\n",
    "        for i in range(1, len1 + 1):\n",
    "            db = 0\n",
    "            for j in range(1, len2 + 1):\n",
    "                i1 = da[s2[j - 1]]\n",
    "                j1 = db\n",
    "                cost = 1\n",
    "                if s1[i - 1] == s2[j - 1]:\n",
    "                    cost = 0\n",
    "                    db = j\n",
    "\n",
    "                score[i + 1][j + 1] = min(\n",
    "                    score[i][j] + cost,\n",
    "                    score[i + 1][j] + 1,\n",
    "                    score[i][j + 1] + 1,\n",
    "                    score[i1][j1] + (i - i1 - 1) + 1 + (j - j1 - 1),\n",
    "                )\n",
    "            da[s1[i - 1]] = i\n",
    "\n",
    "        return score[len1 + 1][len2 + 1]\n",
    "\n",
    "    # For debugging\n",
    "    @staticmethod\n",
    "    def printDicoSortedByKey(d, dicoName=\"Dico\"):\n",
    "        sortedKeysDic = list(d.keys())\n",
    "        sortedKeysDic.sort()\n",
    "        print(\"\\n#########---- Print Sorted %s ----######### \" % dicoName)\n",
    "        for k in sortedKeysDic:\n",
    "            print(\"%s:%s\" % (str(k), str(d[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices aligned: start: 0, end: 4\n",
      "Token in ocr: 'This ' (aligned: 'This ')\n",
      "Token in gs: 'This ' (aligned: 'This ')\n",
      "---\n",
      "Indices aligned: start: 5, end: 7\n",
      "Token in ocr: 'is ' (aligned: 'is ')\n",
      "Token in gs: 'is ' (aligned: 'is ')\n",
      "---\n",
      "Indices aligned: start: 8, end: 10\n",
      "Token in ocr: 'a ' (aligned: 'a@ ')\n",
      "Token in gs: 'an ' (aligned: 'an ')\n",
      "---\n",
      "Indices aligned: start: 11, end: 21\n",
      "Token in ocr: 'cxample...' (aligned: 'cxample...')\n",
      "Token in gs: 'example.' (aligned: 'example.@@')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "\n",
    "n_tokens, n_errors, _ = eval_context.get_errors_stats()\n",
    "\n",
    "assert n_tokens == 4, f\"Number of tokens is {n_tokens}, not 4\"\n",
    "assert n_errors == 2, f\"Number of erroneous tokens is {n_errors}, not 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices aligned: start: 0, end: 5\n",
      "Token in ocr: 'regne ' (aligned: 'regne ')\n",
      "Token in gs: 'regne ' (aligned: 'regne ')\n",
      "---\n",
      "Indices aligned: start: 6, end: 8\n",
      "Token in ocr: 'le ' (aligned: 'le ')\n",
      "Token in gs: 'le ' (aligned: 'le ')\n",
      "---\n",
      "Indices aligned: start: 9, end: 13\n",
      "Token in ocr: 'neuf' (aligned: 'neuf@')\n",
      "Token in gs: 'neuf ' (aligned: 'neuf ')\n",
      "---\n",
      "Indices aligned: start: 14, end: 21\n",
      "Token in ocr: 'viesme.' (aligned: 'viesme.')\n",
      "Token in gs: 'viesme.' (aligned: 'viesme.')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# ???\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example7.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "\n",
    "n_tokens, n_errors, _ = eval_context.get_errors_stats()\n",
    "\n",
    "assert n_tokens == 4, f\"Number of tokens is {n_tokens}, not 4\"\n",
    "assert n_errors == 1, f\"Number of erroneous tokens is {n_errors}, not 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def reshape_input_errors(tokenPosErr, evalContext, verbose=False):\n",
    "    # Store tokens' positions in mem\n",
    "    tokensPos = [0] + [\n",
    "        spacePos.start() + 1 for spacePos in re.finditer(r\"\\ \", evalContext.ocrOriginal)\n",
    "    ]\n",
    "\n",
    "    # 1) Check JSON result format (ex: positions correspond to tokens)\n",
    "    # 2) Reshape data \"pos\":{\"nbTokens\":..., \"boundsAligned\":..., candidates:... }\"\n",
    "    # 3) Locally normalize candidates' weights id needed\n",
    "    tokenPosErrReshaped = {}\n",
    "    for pos_nbtokens, candidates in tokenPosErr.items():\n",
    "        pos, nbOverlappingToken = [int(i) for i in pos_nbtokens.split(\":\")]\n",
    "        boundsAligned = evalContext.get_aligned_token_bounds(pos, nbOverlappingToken)\n",
    "\n",
    "        # Check pos targets a existing token (first char)\n",
    "        assert pos in tokensPos, (\n",
    "            \"[ERROR] : Error at pos %s does not target the first char of a token (space separated sequences).\"\n",
    "            % pos\n",
    "        )\n",
    "\n",
    "        assert evalContext.ocrOriginal[pos:].count(\" \") >= nbOverlappingToken - 1, (\n",
    "            \"[ERROR] : Error at pos %d spreads overs %d tokens which goes ouside the sequence.\"\n",
    "            % (pos, nbOverlappingToken)\n",
    "        )\n",
    "\n",
    "        # Normalize candidates weights if needed\n",
    "        normCandidates = {}\n",
    "\n",
    "        # Limit the number of candiates\n",
    "        for k, v in sorted(candidates.items(), key=lambda kv: kv[1], reverse=True):\n",
    "            normCandidates[k] = v\n",
    "            if len(normCandidates) >= maxNbCandidate:\n",
    "                break\n",
    "\n",
    "        if len(normCandidates) > 0 and sum(normCandidates.values()) != 1:\n",
    "            print(\n",
    "                \"[WARNING] : Normalizing weights at %s:%s\"\n",
    "                % (pos_nbtokens, str(normCandidates))\n",
    "            )\n",
    "            normCandidates = {\n",
    "                cor: float(x) / sum(normCandidates.values())\n",
    "                for cor, x in normCandidates.items()\n",
    "            }\n",
    "\n",
    "        tokenPosErrReshaped[pos] = {\n",
    "            \"nbToken\": nbOverlappingToken,\n",
    "            \"boundsAligned\": boundsAligned,\n",
    "            \"ocrSeq\": re.sub(\n",
    "                evalContext.charExtend,\n",
    "                \"\",\n",
    "                evalContext.ocrAligned[\n",
    "                    boundsAligned[0] : boundsAligned[0] + boundsAligned[1]\n",
    "                ],\n",
    "            ),\n",
    "            \"gsSeq\": re.sub(\n",
    "                evalContext.charExtend,\n",
    "                \"\",\n",
    "                evalContext.gsAligned[\n",
    "                    boundsAligned[0] : boundsAligned[0] + boundsAligned[1]\n",
    "                ],\n",
    "            ),\n",
    "            \"candidates\": normCandidates,\n",
    "        }\n",
    "\n",
    "    # Debug test\n",
    "    if verbose:\n",
    "        EvalContext.printDicoSortedByKey(tokenPosErrReshaped, \"tokenPosErrReshaped\")\n",
    "\n",
    "    return tokenPosErrReshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file)\n",
    "text = process_text(in_file)\n",
    "test_input = {\"key\": text}\n",
    "error_input = create_perfect_icdar_output(test_input)\n",
    "\n",
    "actual = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "expected = {\n",
    "    8: {\n",
    "        \"nbToken\": 1,\n",
    "        \"boundsAligned\": (8, 2),\n",
    "        \"ocrSeq\": \"a\",\n",
    "        \"gsSeq\": \"an\",\n",
    "        \"candidates\": {},\n",
    "    },\n",
    "    10: {\n",
    "        \"nbToken\": 1,\n",
    "        \"boundsAligned\": (11, 10),\n",
    "        \"ocrSeq\": \"cxample...\",\n",
    "        \"gsSeq\": \"example.\",\n",
    "        \"candidates\": {},\n",
    "    },\n",
    "}\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########---- Print Sorted 1) detectedErrPosUnfolded ----######### \n",
      "8:['an ', {}]\n",
      "11:['example.', {}]\n",
      "\n",
      "#########---- Print Sorted 2) realErrPosUnfolded ----######### \n",
      "8:[' a ', ' an ']\n",
      "11:[' cxample.', ' example.']\n",
      "TASK 1) ErrTP 2 / errFP 0 / errFN 0 / prec 1.00 / recall 1.00 / fmes 1.00\n",
      "False positives: set()\n",
      "False negatives: set()\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "# This file contains trailing spaces (on purpose) to test for a specific problem\n",
    "# when calculating task 1 performance.\n",
    "# Be careful when opening this file. Some IDEs remove trailing spaces on save!\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example2.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "test_input = {\"key\": text}\n",
    "error_input = create_perfect_icdar_output(test_input)\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "prec, recall, _fmeasure = eval_context.task1_eval(reshaped_errors, print_sets=True)\n",
    "\n",
    "assert prec == 1.0, f\"Precision is {prec}, not 1.0\"\n",
    "assert recall == 1.0, f\"Recall is {recall}, not 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########---- Print Sorted 1) detectedErrPosUnfolded ----######### \n",
      "8:['x ', {}]\n",
      "\n",
      "#########---- Print Sorted 2) realErrPosUnfolded ----######### \n",
      "8:[' & ', ' x ']\n",
      "10:[' ', ' bla']\n",
      "TASK 1) ErrTP 1 / errFP 0 / errFN 1 / prec 1.00 / recall 0.50 / fmes 0.67\n",
      "False positives: set()\n",
      "False negatives: {10}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Token missing from ocr input (original):\n",
    "# [OCR_toInput] example &\n",
    "# [OCR_aligned] example & @@@\n",
    "# [ GS_aligned] example x bla\n",
    "\n",
    "# Because we only get the OCR text, we can't know the actual length of the text\n",
    "\n",
    "# ocr aligned has a trailing space after removing the alignment characters\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example3.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "test_input = {\"key\": text}\n",
    "error_input = create_perfect_icdar_output(test_input)\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "prec, recall, _fmeasure = eval_context.task1_eval(reshaped_errors, print_sets=True)\n",
    "\n",
    "assert prec == 1.0, f\"Precision is {prec}, not 1.0\"\n",
    "assert recall == 0.5, f\"Recall is {recall}, not 0.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########---- Print Sorted 1) detectedErrPosUnfolded ----######### \n",
      "0:['example.', {}]\n",
      "\n",
      "#########---- Print Sorted 2) realErrPosUnfolded ----######### \n",
      "0:['example, ', 'example.']\n",
      "9:[' —', '']\n",
      "TASK 1) ErrTP 1 / errFP 0 / errFN 1 / prec 1.00 / recall 0.50 / fmes 0.67\n",
      "False positives: set()\n",
      "False negatives: {9}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# OCR text ends with a hyphen, but the hyphen is not in the GS\n",
    "# (example adapted from SL/SL1/0.txt):\n",
    "# [OCR_toInput] example, —\n",
    "# [OCR_aligned] example, —\n",
    "# [ GS_aligned] example.\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example4.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "test_input = {\"key\": text}\n",
    "error_input = create_perfect_icdar_output(test_input)\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "prec, recall, _fmeasure = eval_context.task1_eval(reshaped_errors, print_sets=True)\n",
    "\n",
    "assert prec == 1.0, f\"Precision is {prec}, not 1.0\"\n",
    "assert recall == 0.5, f\"Recall is {recall}, not 0.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(ocr_text='INEVR Hrfl 124879 Major Long ow.', tokens=[AlignedToken(ocr='INEVR', gs='I NEVER', ocr_aligned='I@NEV@R', gs_aligned='I NEVER', start=0, len_ocr=5), AlignedToken(ocr='Hrfl 124879', gs='', ocr_aligned='Hrfl 124879', gs_aligned='###########', start=6, len_ocr=11), AlignedToken(ocr='Major', gs='Major', ocr_aligned='Major', gs_aligned='Major', start=18, len_ocr=5), AlignedToken(ocr='Long ow.', gs='Longhow.', ocr_aligned='Long ow.', gs_aligned='Longhow.', start=24, len_ocr=8)], input_tokens=[InputToken(ocr='INEVR', gs='I NEVER', start=0, len_ocr=5, label=1), InputToken(ocr='Hrfl', gs='', start=6, len_ocr=4, label=1), InputToken(ocr='124879', gs='', start=11, len_ocr=6, label=2), InputToken(ocr='Major', gs='Major', start=18, len_ocr=5, label=0), InputToken(ocr='Long', gs='Longhow.', start=24, len_ocr=4, label=1), InputToken(ocr='ow.', gs='', start=29, len_ocr=3, label=2)], score=0.4375)\n",
      "\n",
      "#########---- Print Sorted 1) detectedErrPosUnfolded ----######### \n",
      "0:['I NEVER ', {}]\n",
      "20:['Major ', {}]\n",
      "26:['Longh', {}]\n",
      "31:['ow.', {}]\n",
      "\n",
      "#########---- Print Sorted 2) realErrPosUnfolded ----######### \n",
      "0:['INEVR ', 'I NEVER ']\n",
      "26:[' Long ', ' Longh']\n",
      "31:[' ow.', 'how.']\n",
      "TASK 1) ErrTP 3 / errFP 1 / errFN 0 / prec 0.75 / recall 1.00 / fmes 0.86\n",
      "False positives: {20}\n",
      "False negatives: set()\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example5.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "print(text)\n",
    "test_input = {\"key\": text}\n",
    "# Start positions of all input tokens\n",
    "error_input = {\"key\": {\"0:1\": {}, \"18:1\": {}, \"24:1\": {}, \"29:1\": {}}}\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "prec, recall, _fmeasure = eval_context.task1_eval(reshaped_errors, print_sets=True)\n",
    "\n",
    "assert prec == 0.75, f\"Precision is {prec}, not 0.75\"\n",
    "assert recall == 1.0, f\"Recall is {recall}, not 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(ocr_text='gewiffer  ihrecflicher | Grnft vo eir,', tokens=[AlignedToken(ocr='gewiffer', gs='gewiſſer', ocr_aligned='gewiffer', gs_aligned='gewiſſer', start=0, len_ocr=8), AlignedToken(ocr='ihrecflicher |', gs='ſchrecklicher', ocr_aligned=' ihrecflicher |', gs_aligned='ſchrecklicher@@', start=9, len_ocr=15), AlignedToken(ocr='Grnft vo', gs='Ernſtwo', ocr_aligned='Grnft vo', gs_aligned='Ernſt@wo', start=25, len_ocr=8), AlignedToken(ocr='eir,', gs='er,', ocr_aligned='eir,', gs_aligned='e@r,', start=34, len_ocr=4)], input_tokens=[InputToken(ocr='gewiffer', gs='gewiſſer', start=0, len_ocr=8, label=1), InputToken(ocr='ihrecflicher', gs='ſchrecklicher', start=9, len_ocr=12, label=1), InputToken(ocr='|', gs='', start=23, len_ocr=1, label=2), InputToken(ocr='Grnft', gs='Ernſtwo', start=25, len_ocr=5, label=1), InputToken(ocr='vo', gs='', start=31, len_ocr=2, label=2), InputToken(ocr='eir,', gs='er,', start=34, len_ocr=4, label=1)], score=0.3157894736842105)\n",
      "\n",
      "#########---- Print Sorted 1) detectedErrPosUnfolded ----######### \n",
      "0:['gewiſſer ', {}]\n",
      "9:['ſ', {}]\n",
      "23:['@ ', {}]\n",
      "25:['Ernſt@', {}]\n",
      "31:['wo ', {}]\n",
      "34:['e@r,', {}]\n",
      "\n",
      "#########---- Print Sorted 2) realErrPosUnfolded ----######### \n",
      "0:['gewiffer ', 'gewiſſer ']\n",
      "9:['  ', ' ſ']\n",
      "10:[' ihrecflicher ', 'ſchrecklicher']\n",
      "23:[' | ', ' ']\n",
      "25:[' Grnft ', ' Ernſt']\n",
      "31:[' vo ', 'wo ']\n",
      "34:[' eir,', ' er,']\n",
      "TASK 1) ErrTP 6 / errFP 0 / errFN 1 / prec 1.00 / recall 0.86 / fmes 0.92\n",
      "False positives: set()\n",
      "False negatives: {10}\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# ocr aligned text with leading whitespace\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example6.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "print(text)\n",
    "test_input = {\"key\": text}\n",
    "# Start positions of all input tokens\n",
    "error_input = {\"key\": {f\"{token.start}:1\": {} for token in text.input_tokens}}\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "prec, recall, _fmeasure = eval_context.task1_eval(reshaped_errors, print_sets=True)\n",
    "\n",
    "assert prec == 1.0, f\"Precision is {prec}, not 1.0\"\n",
    "assert float(np.round(recall, 2)) == 0.86, f\"Recall is {recall}, not 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text(ocr_text='regne le neufviesme.', tokens=[AlignedToken(ocr='regne', gs='regne', ocr_aligned='regne', gs_aligned='regne', start=0, len_ocr=5), AlignedToken(ocr='le', gs='le', ocr_aligned='le', gs_aligned='le', start=6, len_ocr=2), AlignedToken(ocr='neufviesme.', gs='neuf viesme.', ocr_aligned='neuf@viesme.', gs_aligned='neuf viesme.', start=9, len_ocr=11)], input_tokens=[InputToken(ocr='regne', gs='regne', start=0, len_ocr=5, label=0), InputToken(ocr='le', gs='le', start=6, len_ocr=2, label=0), InputToken(ocr='neufviesme.', gs='neuf viesme.', start=9, len_ocr=11, label=1)], score=0.047619047619047616)\n",
      "Indices aligned: start: 0, end: 5\n",
      "Token in ocr: 'regne ' (aligned: 'regne ')\n",
      "Token in gs: 'regne ' (aligned: 'regne ')\n",
      "---\n",
      "Indices aligned: start: 6, end: 8\n",
      "Token in ocr: 'le ' (aligned: 'le ')\n",
      "Token in gs: 'le ' (aligned: 'le ')\n",
      "---\n",
      "Indices aligned: start: 9, end: 13\n",
      "Token in ocr: 'neuf' (aligned: 'neuf@')\n",
      "Token in gs: 'neuf ' (aligned: 'neuf ')\n",
      "---\n",
      "Indices aligned: start: 14, end: 21\n",
      "Token in ocr: 'viesme.' (aligned: 'viesme.')\n",
      "Token in gs: 'viesme.' (aligned: 'viesme.')\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# ???\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example7.txt\"\n",
    "\n",
    "eval_context = EvalContext(in_file, verbose=True)\n",
    "text = process_text(in_file)\n",
    "print(text)\n",
    "test_input = {\"key\": text}\n",
    "# Start positions of all input tokens\n",
    "error_input = {\"key\": {f\"{token.start}:1\": {} for token in text.input_tokens}}\n",
    "reshaped_errors = reshape_input_errors(error_input[\"key\"], eval_context)\n",
    "\n",
    "nbTokens, nbErrTokens, nbErrTokensAlpha = eval_context.get_errors_stats()\n",
    "\n",
    "print(nbErrTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def runEvaluation(\n",
    "    datasetDirPath,  # path to the dataset directory (ex: r\"./dataset_sample\")\n",
    "    pathInputJsonErrorsCorrections,  # # input path to the JSON result (ex: r\"./inputErrCor_sample.json\"), format given on https://sites.google.com/view/icdar2017-postcorrectionocr/evaluation)\n",
    "    pathOutputCsv,  # output path to the CSV evaluation results (ex: r\"./outputEval.csv\")\n",
    "    verbose=False,\n",
    "):  # Show verbose output\n",
    "    \"\"\"Main evaluation method\"\"\"\n",
    "\n",
    "    # Load results JSON file\n",
    "    with codecs.open(\n",
    "        pathInputJsonErrorsCorrections, \"r\", encoding=\"utf-8\"\n",
    "    ) as data_file:\n",
    "        formatedRes = json.loads(data_file.read())\n",
    "\n",
    "    # CSV header fields\n",
    "    csvHeader = [\n",
    "        \"File\",\n",
    "        \"NbTokens\",\n",
    "        \"NbErroneousTokens\",\n",
    "        \"NbSymbolsConsidered\",  # NbTokens furtherly used to weight file's metrics \\\n",
    "        \"T1_Precision\",\n",
    "        \"T1_Recall\",\n",
    "        \"T1_Fmesure\",  # Task 1) Metrics \\\n",
    "        \"T2_AvgLVDistOriginal\",\n",
    "        \"T2_AvgLVDistCorrected\",\n",
    "    ]  # Task 2) Metrics\n",
    "\n",
    "    # Write CSV file's header into a new output file\n",
    "    with open(pathOutputCsv, \"w\") as outputFile:\n",
    "        outputFile.write(\";\".join(csvHeader) + \"\\n\")\n",
    "\n",
    "    # Print CSV header into the console file\n",
    "    print(\"\\t\".join(csvHeader))\n",
    "\n",
    "    # Iterate over all the file's paths given in the input results\n",
    "    for filePath, tokenPosErr in formatedRes.items():\n",
    "        # Load the context : [OCR_toInput], [OCR_aligned] and [ GS_aligned]\n",
    "        evalContext = EvalContext(\n",
    "            os.path.join(datasetDirPath, filePath), verbose=verbose\n",
    "        )\n",
    "\n",
    "        # Compute some intrinsic statistics\n",
    "        nbTokens, nbErrTokens, nbErrTokensAlpha = evalContext.get_errors_stats()\n",
    "\n",
    "        tokenPosErrReshaped = reshape_input_errors(tokenPosErr, evalContext, verbose)\n",
    "\n",
    "        # Task 1) Run the evaluation : Detection of the position of erroneous tokens\n",
    "        prec, recall, fmes = evalContext.task1_eval(tokenPosErrReshaped)\n",
    "\n",
    "        # Task 2) Run the evaluation : Correction of the erroneous tokens\n",
    "        (\n",
    "            sumCorrectedDistance,\n",
    "            sumOriginalDistance,\n",
    "            nbSymbolsConsidered,\n",
    "        ) = evalContext.task2_eval(tokenPosErrReshaped, useFirstCandidateOnly=False)\n",
    "\n",
    "        # Manage division per zero\n",
    "        avgCorrectedDistance = (\n",
    "            sumCorrectedDistance / float(nbSymbolsConsidered)\n",
    "            if nbSymbolsConsidered > 0\n",
    "            else 0\n",
    "        )\n",
    "        avgOriginalDistance = (\n",
    "            sumOriginalDistance / float(nbSymbolsConsidered)\n",
    "            if nbSymbolsConsidered > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Format results in CSV\n",
    "        strRes = \"%s;%d;%d;%d;%0.02f;%0.02f;%0.02f;%0.02f;%0.02f\" % (\n",
    "            filePath,\n",
    "            nbTokens,\n",
    "            nbErrTokens,\n",
    "            nbSymbolsConsidered,\n",
    "            prec,\n",
    "            recall,\n",
    "            fmes,\n",
    "            avgOriginalDistance,\n",
    "            avgCorrectedDistance,\n",
    "        )\n",
    "\n",
    "        # Write results in output file\n",
    "        with open(pathOutputCsv, \"a\") as outputFile:\n",
    "            outputFile.write(strRes + \"\\n\")\n",
    "\n",
    "        # Print results in the console\n",
    "        print(strRes.replace(\";\", \"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def read_results(csv_file):\n",
    "    \"\"\"Read csv with evaluation results\"\"\"\n",
    "    data = pd.read_csv(csv_file, sep=\";\")\n",
    "    data[\"language\"] = data.File.apply(lambda x: x[:2])\n",
    "    data[\"subset\"] = data.File.apply(lambda x: x.split(\"/\")[1])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ICDAR output format to SimpleCorrectionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def icdar_output2simple_correction_dataset_df(\n",
    "    output: Dict[str, Dict[str, Dict]], data: Dict[str, Text], dataset: str = \"test\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Convert the icdar data error detection output to input for SimpleCorrectionDataset\n",
    "    \n",
    "    Because gold standard for input_tokens is not available, the dataset dataframe cannot\n",
    "    be used for evaluation anymore.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for key, mistakes in output.items():\n",
    "        text = data[key]\n",
    "        for token in mistakes:\n",
    "            sample = {}\n",
    "            parts = token.split(\":\")\n",
    "            start_idx = int(parts[0])\n",
    "            num_tokens = int(parts[0])\n",
    "            for at in text.input_tokens:\n",
    "                if at.start == start_idx:\n",
    "                    sample[\"ocr\"] = at.ocr\n",
    "                    sample[\"gs\"] = at.gs\n",
    "                    sample[\"start\"] = at.start\n",
    "                    sample[\"text\"] = key\n",
    "                    sample[\"token\"] = token\n",
    "                    sample[\"len_ocr\"] = at.len_ocr\n",
    "                    sample[\"len_gs\"] = len(at.gs)\n",
    "                    parts = key.split(\"/\")\n",
    "                    sample[\"language\"] = parts[0]\n",
    "                    sample[\"subset\"] = parts[1]\n",
    "                    sample[\"dataset\"] = dataset\n",
    "\n",
    "            if sample == {}:\n",
    "                raise ValueError(f\"No token found for {key}, start index: {start_idx}\")\n",
    "            samples.append(sample)\n",
    "    return pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1583.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame contains 40 samples\n",
      "Dataset contains 37 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "data, md = generate_data(data_dir)\n",
    "\n",
    "detection_output = create_perfect_icdar_output(data)\n",
    "\n",
    "df = icdar_output2simple_correction_dataset_df(detection_output, data)\n",
    "print(f\"DataFrame contains {df.shape[0]} samples\")\n",
    "\n",
    "dataset = SimpleCorrectionDataset(df, max_len=10)\n",
    "print(f\"Dataset contains {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "assert df.shape[0] == 40\n",
    "assert len(dataset) == 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize icdar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def read_results(csv_file):\n",
    "    data = pd.read_csv(csv_file, sep=';')\n",
    "    data['language'] = data.File.apply(lambda x: x[:2])\n",
    "    data['subset'] = data.File.apply(lambda x: x.split('/')[1])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def aggregate_results(csv_file):\n",
    "    data = read_results(csv_file)\n",
    "\n",
    "    return data.groupby(\"language\").mean()[[\"T1_Precision\", \"T1_Recall\", \"T1_Fmesure\"]]\n",
    "\n",
    "\n",
    "def aggregate_ed_results(csv_file):\n",
    "    data = read_results(csv_file)\n",
    "\n",
    "    data['ed_diff'] = data['T2_AvgLVDistOriginal'] - data['T2_AvgLVDistCorrected']\n",
    "    data['%ed_improvement'] = data['ed_diff'] / data['T2_AvgLVDistOriginal'] * 100\n",
    "\n",
    "    # If `T2_AvgLVDistOriginal` == 0.0 and `T2_AvgLVDistCorrected` > 0, `ed_diff` is a\n",
    "    # negative number and `ed_diff`/`T2_AvgLVDistOriginal` = -inf.\n",
    "    # The mean of numbers which include -inf is nan.\n",
    "    # Therefore, -inf is replaced by -100.0%.\n",
    "    data['%ed_improvement'].replace([-np.inf], -100.0, inplace=True)\n",
    "\n",
    "    # If `T2_AvgLVDistOriginal` == 0.0 and and `T2_AvgLVDistCorrected` == 0.0,\n",
    "    # the % improvement is nan. The mean of numbers which include nan is nan. \n",
    "    # So, in this case, the value should be replaced with 0.0.\n",
    "    data['%ed_improvement'].fillna(0.0, inplace=True)\n",
    "\n",
    "    return data.groupby(\"language\").mean()[['%ed_improvement']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          %ed_improvement\n",
      "language                 \n",
      "FR                 -100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/cygr04pj6qd6hm4vs13np89r0000gn/T/ipykernel_40587/3253537010.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  return data.groupby(\"language\").mean()[['%ed_improvement']]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Case: T2_AvgLVDistOriginal == 0.0 and T2_AvgLVDistCorrected > 0\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"icdar_csv\" / \"ed_improvement_inf.csv\"\n",
    "\n",
    "result = aggregate_ed_results(in_file)\n",
    "print(result)\n",
    "\n",
    "assert result.loc['FR']['%ed_improvement'] == -100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          %ed_improvement\n",
      "language                 \n",
      "FR                    0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/cygr04pj6qd6hm4vs13np89r0000gn/T/ipykernel_40587/3253537010.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  return data.groupby(\"language\").mean()[['%ed_improvement']]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Case: T2_AvgLVDistOriginal == 0.0 and T2_AvgLVDistCorrected == 0.0\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"icdar_csv\" / \"ed_improvement_nan.csv\"\n",
    "\n",
    "result = aggregate_ed_results(in_file)\n",
    "print(result)\n",
    "\n",
    "assert result.loc['FR']['%ed_improvement'] == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def reduce_dataset(dataset, n=5):\n",
    "    \"\"\"Return dataset with the first n samples for each split\"\"\"\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].select(range(n))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
