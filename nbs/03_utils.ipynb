{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Utils\n",
    "output-file: utils.html\n",
    "description: Util functionality\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from functools import partial\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from ocrpostcorrection.icdar_data import InputToken, generate_data, generate_sentences, process_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert predictions into ICDAR output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def predictions_to_labels(predictions):\n",
    "    return np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b x max seq x # classes\n",
    "predictions = np.zeros((16, 10, 3))\n",
    "\n",
    "# Always predict 1\n",
    "predictions[:, :, 1] = 1\n",
    "\n",
    "output = predictions_to_labels(predictions)\n",
    "\n",
    "assert np.array_equal(np.ones((16, 10)), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# shape: b x max seq x # classes\n",
    "predictions = np.array([np.identity(5)])\n",
    "\n",
    "result = predictions_to_labels(predictions)\n",
    "\n",
    "assert np.array_equal(np.array([0, 1, 2, 3, 4]) , result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def separate_subtoken_predictions(word_ids, preds):\n",
    "    #print(len(word_ids), word_ids)\n",
    "    result = defaultdict(list)\n",
    "    for word_idx, p_label in zip(word_ids, preds):\n",
    "        #print(word_idx, p_label)\n",
    "        if word_idx is not None:\n",
    "            result[word_idx].append(p_label)\n",
    "    return dict(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 0, 0], 1: [0, 0, 1], 2: [1, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "word_ids = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "preds =    [0, 0, 0, 0, 0, 1, 1, 2, 2]\n",
    "\n",
    "token_preds = separate_subtoken_predictions(word_ids, preds)\n",
    "print(token_preds)\n",
    "\n",
    "assert token_preds == {0: [0, 0, 0], 1: [0, 0, 1], 2: [1, 2, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_subtoken_predictions(subtoken_predictions):\n",
    "    token_level_predictions = []\n",
    "    for word_idx, preds in subtoken_predictions.items():\n",
    "        token_label = 0\n",
    "        c = Counter(preds)\n",
    "        #print(c)\n",
    "        if c[1] > 0 and c[1] >= c[2]:\n",
    "            token_label = 1\n",
    "        elif c[2] > 0 and c[2] >= c[1]:\n",
    "            token_label = 2\n",
    "\n",
    "        token_level_predictions.append(token_label)\n",
    "    return token_level_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "subtoken_predictions = {0: [0, 0, 0],  # 0\n",
    "                        1: [1, 1, 0],  # 1\n",
    "                        2: [1, 2],     # 1\n",
    "                        3: [2, 2, 1],  # 2\n",
    "                        4: [0, 1, 2],  # 1\n",
    "                        5: [0, 1, 0]}  # 1\n",
    "\n",
    "token_preds = merge_subtoken_predictions(subtoken_predictions)\n",
    "print(token_preds)\n",
    "\n",
    "assert [0, 1, 1, 2, 1, 1] == token_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gather_token_predictions(preds):\n",
    "    \"\"\"Gather potentially overlapping token predictions\"\"\"\n",
    "    labels = defaultdict(list)\n",
    "        \n",
    "    #print(len(text.input_tokens))\n",
    "    #print(preds)\n",
    "    for start, lbls in preds.items():\n",
    "        for i, label in enumerate(lbls):\n",
    "            labels[int(start)+i].append(label)\n",
    "    #print('LABELS')\n",
    "    #print(labels)\n",
    "    return dict(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "token_predictions = {0: [0, 0, 0, 0, 0],\n",
    "                     1: [0, 0, 0, 0, 0],\n",
    "                     2: [0, 0, 0, 0, 0]}\n",
    "actual = gather_token_predictions(token_predictions)\n",
    "expected = {0: [0], 1: [0, 0], 2: [0, 0, 0], 3: [0, 0, 0], 4: [0, 0, 0], 5: [0, 0], 6: [0]}\n",
    "\n",
    "assert expected == actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def labels2label_str(labels):\n",
    "    label_str = []\n",
    "\n",
    "    for i, token in enumerate(labels):\n",
    "        #print(i, token, labels[i])\n",
    "        if 2 in labels[i]:\n",
    "            label_str.append('2')\n",
    "        elif 1 in labels[i]:\n",
    "            label_str.append('1')\n",
    "        else:\n",
    "            label_str.append('0')\n",
    "    label_str = ''.join(label_str)\n",
    "    return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "labels = [[0], [1], [2], [0, 0, 1], [0, 1, 2]]\n",
    "\n",
    "label_str = labels2label_str(labels)\n",
    "\n",
    "assert label_str == '01212'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def extract_icdar_output(label_str, input_tokens):\n",
    "    #print(label_str, input_tokens)\n",
    "    #print(len(label_str), len(input_tokens))\n",
    "    text_output = {}\n",
    "\n",
    "    # Correct use of 2 (always following a 1)\n",
    "    regex = r'12*'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group())\n",
    "        #print(match.start(), len(input_tokens))\n",
    "        idx = input_tokens[match.start()].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "\n",
    "    # Incorrect use of 2 (following a 0) -> interpret first 2 as 1\n",
    "    regex = r'02+'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group()) - 1\n",
    "        idx = input_tokens[match.start()+1].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "    \n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "label_str = '1'\n",
    "input_tokens = [InputToken(ocr='bal', gs='bla', start=0, len_ocr=3, label=1)]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {'0:1': {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "label_str = '01'\n",
    "input_tokens = [InputToken(ocr='one', gs='one', start=0, len_ocr=3, label=0),\n",
    "                InputToken(ocr='tow', gs='two', start=4, len_ocr=3, label=1)]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {'4:1': {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "label_str = '12'\n",
    "input_tokens = [InputToken(ocr='one', gs='one', start=0, len_ocr=3, label=0),\n",
    "                InputToken(ocr='tow', gs='two', start=4, len_ocr=3, label=1)]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {'0:2': {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "label_str = '112'\n",
    "input_tokens = [InputToken(ocr='one', gs='one', start=0, len_ocr=3, label=0),\n",
    "                InputToken(ocr='one', gs='one', start=4, len_ocr=3, label=0),\n",
    "                InputToken(ocr='tow', gs='two', start=8, len_ocr=3, label=1)]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {'0:1': {}, '4:2': {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "label_str = '02'\n",
    "input_tokens = [InputToken(ocr='one', gs='one', start=0, len_ocr=3, label=0),\n",
    "                InputToken(ocr='tow', gs='two', start=4, len_ocr=3, label=1)]\n",
    "output = extract_icdar_output(label_str, input_tokens)\n",
    "assert output == {'4:1': {}}, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def predictions2icdar_output(samples, predictions, tokenizer, data_test):\n",
    "    \"\"\"Convert predictions into icdar output format\"\"\"\n",
    "    #print('samples', len(samples))\n",
    "    #print(samples)\n",
    "    #print(samples[0].keys())\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys()) \n",
    "\n",
    "    tokenized_samples = tokenizer(samples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    #print(samples)\n",
    "\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys())\n",
    "    \n",
    "    # convert predictions to labels (label_ids)\n",
    "    #p = np.argmax(predictions, axis=2)\n",
    "    #print(p)\n",
    "\n",
    "    converted = defaultdict(dict)\n",
    "\n",
    "    for i, (sample, preds) in enumerate(zip(samples, predictions)):\n",
    "        #print(sample.keys())\n",
    "        #label = sample['tags']\n",
    "        #print(label)\n",
    "        #print(len(preds), preds)\n",
    "        word_ids = tokenized_samples.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        result = separate_subtoken_predictions(word_ids, preds)\n",
    "        new_tags = merge_subtoken_predictions(result)\n",
    "\n",
    "        #print('pred', len(new_tags), new_tags)\n",
    "        #print('tags', len(label), label)\n",
    "        \n",
    "        #print(sample)\n",
    "        #print(sample['key'], sample['start_token_id'])\n",
    "        converted[sample['key']][sample['start_token_id']] = new_tags\n",
    "    \n",
    "    output = {}\n",
    "    for key, preds in converted.items():\n",
    "        labels = defaultdict(list)\n",
    "        #print(key)\n",
    "        labels = gather_token_predictions(preds)\n",
    "        label_str = labels2label_str(labels)\n",
    "        try:\n",
    "            text = data_test[key]\n",
    "            output[key] = extract_icdar_output(label_str, text.input_tokens)\n",
    "        except KeyError:\n",
    "            logger.warning(f'No data found for text {key}')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 420.69it/s]\n",
      "4it [00:00, 540.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# Create tokenizer\n",
    "bert_base_model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_base_model_name)\n",
    "\n",
    "# Create data\n",
    "data_dir = Path(os.getcwd())/'data'/'dataset_training_sample'\n",
    "data, md = generate_data(data_dir)\n",
    "sentence_df = generate_sentences(md, data, size=2, step=1)\n",
    "dataset = Dataset.from_pandas(sentence_df)\n",
    "\n",
    "# Create predictions\n",
    "\n",
    "# b x max seq x # classes\n",
    "predictions = np.zeros((len(dataset), 10, 3))\n",
    "\n",
    "# Always predict 1\n",
    "predictions[:, :, 1] = 1\n",
    "predictions = predictions_to_labels(predictions)\n",
    "\n",
    "# Generate icdar output (task 1)\n",
    "actual = predictions2icdar_output(dataset, predictions, tokenizer, data)\n",
    "\n",
    "# Expected output has an entry of lenght 1 for every input token\n",
    "expected = defaultdict(dict)\n",
    "for key, text in data.items():\n",
    "    for token in text.input_tokens:\n",
    "        expected[key][f'{token.start}:1'] = {}\n",
    "        \n",
    "assert expected == actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_perfect_icdar_output(data):\n",
    "    output = {}\n",
    "    for key, text_obj in data.items():\n",
    "        label_str = ''.join([str(t.label) for t in text_obj.input_tokens])\n",
    "        output[key] = extract_icdar_output(label_str, data[key].input_tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "in_file = Path(os.getcwd())/'data'/'example.txt'\n",
    "text = process_text(in_file)\n",
    "\n",
    "test_input = {'key': text}\n",
    "\n",
    "actual = create_perfect_icdar_output(test_input)\n",
    "\n",
    "# Indices (the first number) refer to the ocr input text\n",
    "assert actual == {'key': {'8:1': {}, '10:1': {}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize icdar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def aggregate_results(csv_file):\n",
    "    data = pd.read_csv(csv_file, sep=';')\n",
    "    data['language'] = data.File.apply(lambda x: x[:2])\n",
    "    data['subset'] = data.File.apply(lambda x: x.split('/')[1])\n",
    "\n",
    "    return data.groupby('language').mean()[['T1_Precision', 'T1_Recall', 'T1_Fmesure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reduce_dataset(dataset, n=5):\n",
    "    \"\"\"Return dataset with the first n samples for each split\"\"\"\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].select(range(n))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('nlp4dutch')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
