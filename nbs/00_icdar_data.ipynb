{
 "cells": [
  {
   "cell_type": "raw",
   "id": "posted-thailand",
   "metadata": {},
   "source": [
    "---\n",
    "title: ICDAR Data\n",
    "output-file: icdar_data.html\n",
    "description: Functionality for working with the ICDAR dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp icdar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from typing import Text as TypingText\n",
    "from typing import Tuple\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import edlib\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48cfe0",
   "metadata": {},
   "source": [
    "\n",
    "The [ICDAR 2019 Competition on Post-OCR Text Correction \n",
    "dataset](https://sites.google.com/view/icdar2019-postcorrectionocr/dataset) \n",
    "([zenodo record](https://zenodo.org/record/3515403#.YwULoi0RoWI))\n",
    "contains text files in the following format:\n",
    "\n",
    "```\n",
    "[OCR_toInput] This is a cxample...\n",
    "[OCR_aligned] This is a@ cxample...\n",
    "[ GS_aligned] This is an example.@@\n",
    "01234567890123\n",
    "```\n",
    "\n",
    "The first line contains the ocr input text. The second line contains the aligned \n",
    "ocr and the third line contains the aligned gold standard.\n",
    "`@` is the aligment character and `#` represents characters in the OCR that do \n",
    "not occur in the gold standard.\n",
    "\n",
    "For working with this data, the first 14 characters have to be removed. We also \n",
    "remove leading and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def remove_label_and_nl(line: str):\n",
    "    return line.strip()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "assert (\n",
    "    remove_label_and_nl(\"[OCR_toInput] This is a cxample...\\n\")\n",
    "    == \"This is a cxample...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a85ac1",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Task 1 of the competition is about finding tokens with OCR mistakes. In this context a token\n",
    "refers to a string between two whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class AlignedToken:\n",
    "    \"\"\"Dataclass for storing aligned tokens\"\"\"\n",
    "\n",
    "    ocr: str  # String in the OCR text\n",
    "    gs: str  # String in the gold standard\n",
    "    ocr_aligned: str  # String in the aligned OCR text (without aligmnent characters)\n",
    "    gs_aligned: str  # String in the aligned GS text (without aligmnent characters)\n",
    "    start: int  # The index of the first character in the OCR text\n",
    "    len_ocr: int  # The lentgh of the OCR string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def tokenize_aligned(ocr_aligned: str, gs_aligned: str) -> List[AlignedToken]:\n",
    "    \"\"\"Get a list of AlignedTokens from the aligned OCR and GS strings\"\"\"\n",
    "\n",
    "    ocr_cursor = 0\n",
    "    start = 0\n",
    "\n",
    "    ocr_token_chars: List[str] = []\n",
    "    gs_token_chars: List[str] = []\n",
    "    ocr_token_chars_aligned: List[str] = []\n",
    "    gs_token_chars_aligned: List[str] = []\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for ocr_aligned_char, gs_aligned_char in zip(ocr_aligned, gs_aligned):\n",
    "        # print(ocr_aligned_char, gs_aligned_char, ocr_cursor)\n",
    "        # The # character in ocr is not an aligment character!\n",
    "        if ocr_aligned_char != \"@\":\n",
    "            ocr_cursor += 1\n",
    "\n",
    "        if ocr_aligned_char == \" \" and gs_aligned_char == \" \":\n",
    "            # print('TOKEN')\n",
    "            # print('OCR:', repr(''.join(ocr_token_chars)))\n",
    "            # print(' GS:', repr(''.join(gs_token_chars)))\n",
    "            # print('start:', start_char)\n",
    "            # ocr_cursor += 1\n",
    "\n",
    "            # Ignore 'tokens' without representation in the ocr text\n",
    "            # (these tokens do not consist of characters)\n",
    "            ocr = (\"\".join(ocr_token_chars)).strip()\n",
    "            if ocr != \"\":\n",
    "                tokens.append(\n",
    "                    AlignedToken(\n",
    "                        ocr,\n",
    "                        \"\".join(gs_token_chars),\n",
    "                        \"\".join(ocr_token_chars_aligned),\n",
    "                        \"\".join(gs_token_chars_aligned),\n",
    "                        start,\n",
    "                        len(\"\".join(ocr_token_chars)),\n",
    "                    )\n",
    "                )\n",
    "            start = ocr_cursor\n",
    "\n",
    "            ocr_token_chars = []\n",
    "            gs_token_chars = []\n",
    "            ocr_token_chars_aligned = []\n",
    "            gs_token_chars_aligned = []\n",
    "        else:\n",
    "            ocr_token_chars_aligned.append(ocr_aligned_char)\n",
    "            gs_token_chars_aligned.append(gs_aligned_char)\n",
    "            # The # character in ocr is not an aligment character!\n",
    "            if ocr_aligned_char != \"@\":\n",
    "                ocr_token_chars.append(ocr_aligned_char)\n",
    "            if gs_aligned_char != \"@\" and gs_aligned_char != \"#\":\n",
    "                gs_token_chars.append(gs_aligned_char)\n",
    "\n",
    "    # Final token (if there is one)\n",
    "    ocr = (\"\".join(ocr_token_chars)).strip()\n",
    "    if ocr != \"\":\n",
    "        tokens.append(\n",
    "            AlignedToken(\n",
    "                ocr,\n",
    "                \"\".join(gs_token_chars),\n",
    "                \"\".join(ocr_token_chars_aligned),\n",
    "                \"\".join(gs_token_chars_aligned),\n",
    "                start,\n",
    "                len(\"\".join(ocr_token_chars)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25b44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4),\n",
       " AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2),\n",
       " AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1),\n",
       " AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.##', start=10, len_ocr=10)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_aligned(\"This is a@ cxample...\", \"This is an example.##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2b1dd",
   "metadata": {},
   "source": [
    "The OCR text of an AlignedToken may still consist of multiple tokens. This is the \n",
    "case when the OCR text contains one or more spaces. To make sure the (sub)tokenization\n",
    "of a token is the same, no matter if it was not yet tokenized completely, \n",
    "another round of tokenization is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class InputToken:\n",
    "    \"\"\"Dataclass for the tokenization within AlignedTokens\"\"\"\n",
    "\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    start: int\n",
    "    len_ocr: int\n",
    "    label: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63971813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def leading_whitespace_offset(string: str) -> int:\n",
    "    \"\"\"\n",
    "    Return the leading whitespace offset for an aligned ocr string\n",
    "\n",
    "    If an aligned ocr string contains leading whitespace, the offset needs to be added\n",
    "    to the start index of the respective input tokens.\n",
    "\n",
    "    Args:\n",
    "        string (str): aligned ocr string to determine the leading whitespace offset for\n",
    "\n",
    "    Returns:\n",
    "        int: leading whitespace offset for input tokens\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "\n",
    "    regex = r\"^@*(?P<whitespace>\\ +)\"\n",
    "    match = re.match(regex, string)\n",
    "    if match:\n",
    "        offset = len(match.group(\"whitespace\"))\n",
    "\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08965423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "string = \"bla\"\n",
    "\n",
    "assert leading_whitespace_offset(string) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf772c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "string = \" bla\"\n",
    "\n",
    "assert leading_whitespace_offset(string) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "string = \"@ bla\"\n",
    "\n",
    "assert leading_whitespace_offset(string) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "string = \"@  bla\"\n",
    "\n",
    "assert leading_whitespace_offset(string) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_input_tokens(aligned_token: AlignedToken):\n",
    "    \"\"\"Tokenize an AlignedToken into subtokens and assign task 1 labels\"\"\"\n",
    "    input_tokens = []\n",
    "    if aligned_token.ocr == aligned_token.gs:\n",
    "        input_tokens.append(\n",
    "            InputToken(\n",
    "                ocr=aligned_token.ocr,\n",
    "                gs=aligned_token.gs,\n",
    "                start=aligned_token.start,\n",
    "                len_ocr=len(aligned_token.ocr),\n",
    "                label=0,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        parts = aligned_token.ocr.split(\" \")\n",
    "        new_start = aligned_token.start\n",
    "        offset = leading_whitespace_offset(aligned_token.ocr_aligned)\n",
    "        for i, part in enumerate(parts):\n",
    "            if i == 0:\n",
    "                token = InputToken(\n",
    "                    ocr=part,\n",
    "                    gs=aligned_token.gs,\n",
    "                    start=aligned_token.start,\n",
    "                    len_ocr=len(part),\n",
    "                    label=1,\n",
    "                )\n",
    "            else:\n",
    "                token = InputToken(\n",
    "                    ocr=part,\n",
    "                    gs=\"\",\n",
    "                    start=new_start,\n",
    "                    len_ocr=len(part),\n",
    "                    label=2,\n",
    "                )\n",
    "                token.start += offset\n",
    "            new_start += len(part) + 1\n",
    "            input_tokens.append(token)\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='Major', gs='Major', ocr_aligned='Major', gs_aligned='Major', start=19, len_ocr=5)\n",
      "InputToken(ocr='Major', gs='Major', start=19, len_ocr=5, label=0)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken(\"Major\", \"Major\", \"Major\", \"Major\", 19, 5)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fded73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == [\"Major\"]\n",
    "assert labels == [0]\n",
    "assert \"\".join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='INEVR', gs='I NEVER', ocr_aligned='I@NEV@R', gs_aligned='I NEVER', start=0, len_ocr=5)\n",
      "InputToken(ocr='INEVR', gs='I NEVER', start=0, len_ocr=5, label=1)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken(\"INEVR\", \"I NEVER\", \"I@NEV@R\", \"I NEVER\", 0, 5)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == [\"INEVR\"]\n",
    "assert labels == [1]\n",
    "assert \"\".join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='Long ow.', gs='Longhow.', ocr_aligned='Long ow.', gs_aligned='Longhow.', start=24, len_ocr=8)\n",
      "InputToken(ocr='Long', gs='Longhow.', start=24, len_ocr=4, label=1)\n",
      "InputToken(ocr='ow.', gs='', start=29, len_ocr=3, label=2)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken(\"Long ow.\", \"Longhow.\", \"Long ow.\", \"Longhow.\", 24, 8)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac965c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == [\"Long\", \"ow.\"]\n",
    "assert labels == [1, 2]\n",
    "assert \"\".join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2694cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='1 2', gs='1', ocr_aligned='1 2', gs_aligned='1@@', start=324, len_ocr=3)\n",
      "InputToken(ocr='1', gs='1', start=324, len_ocr=1, label=1)\n",
      "InputToken(ocr='2', gs='', start=326, len_ocr=1, label=2)\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Example where only the second input token does not match (the first input token does\n",
    "# match)\n",
    "t = AlignedToken(\n",
    "    ocr=\"1 2\", gs=\"1\", ocr_aligned=\"1 2\", gs_aligned=\"1@@\", start=324, len_ocr=3\n",
    ")\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == [\"1\", \"2\"]\n",
    "assert labels == [1, 2]\n",
    "assert \"\".join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340a137",
   "metadata": {},
   "source": [
    "## Process a text file\n",
    "\n",
    "Next, we need functions for processing a text in the ICDAR data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class Text:\n",
    "    \"\"\"Dataclass for storing a text in the ICDAR data format\"\"\"\n",
    "\n",
    "    ocr_text: str\n",
    "    tokens: list\n",
    "    input_tokens: list\n",
    "    score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def clean(string: str) -> str:\n",
    "    \"\"\"Remove alignment characters from a text\"\"\"\n",
    "    string = string.replace(\"@\", \"\")\n",
    "    string = string.replace(\"#\", \"\")\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def normalized_ed(ed: int, ocr: str, gs: str) -> float:\n",
    "    \"\"\"Returns the normalized editdistance\"\"\"\n",
    "    score = 0.0\n",
    "    longest = max(len(ocr), len(gs))\n",
    "    if longest > 0:\n",
    "        score = ed / longest\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def process_text(in_file: Path) -> Text:\n",
    "    \"\"\"Process a text from the ICDAR dataset\n",
    "\n",
    "    Extract AlignedTokens, InputTokens, and calculate normalized editdistance.\n",
    "    \"\"\"\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # The # character in ocr input is not an aligment character, but the @\n",
    "    # character is!\n",
    "    ocr_input = remove_label_and_nl(lines[0]).replace(\"@\", \"\")\n",
    "    ocr_aligned = remove_label_and_nl(lines[1])\n",
    "    gs_aligned = remove_label_and_nl(lines[2])\n",
    "\n",
    "    # print('ocr input:', ocr_input)\n",
    "    # print('ocr aligned:', ocr_aligned)\n",
    "    # print('gs aligned:',gs_aligned)\n",
    "\n",
    "    tokens = tokenize_aligned(ocr_aligned, gs_aligned)\n",
    "\n",
    "    # Check data\n",
    "    for token in tokens:\n",
    "        input_token = ocr_input[token.start : token.start + token.len_ocr]\n",
    "        try:\n",
    "            assert token.ocr == input_token.strip()\n",
    "        except AssertionError:\n",
    "            logger.warning(\n",
    "                f\"OCR != aligned OCR: Text: {str(in_file)}; ocr: {repr(token.ocr)}; \"\n",
    "                + f\"ocr_input: {repr(input_token)}\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    ocr = clean(ocr_aligned)\n",
    "    gs = clean(gs_aligned)\n",
    "\n",
    "    try:\n",
    "        ed = edlib.align(gs, ocr)[\"editDistance\"]\n",
    "        score = normalized_ed(ed, ocr, gs)\n",
    "    except UnicodeEncodeError:\n",
    "        logger.warning(f\"UnicodeEncodeError for text {in_file}; setting score to 1\")\n",
    "        score = 1\n",
    "\n",
    "    input_tokens = []\n",
    "    for token in tokens:\n",
    "        for inp_tok in get_input_tokens(token):\n",
    "            input_tokens.append(inp_tok)\n",
    "\n",
    "    return Text(ocr_input, tokens, input_tokens, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf19c72",
   "metadata": {},
   "source": [
    "Processing the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09381f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(ocr_text='This is a cxample...', tokens=[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4), AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2), AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1), AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.@@', start=10, len_ocr=10)], input_tokens=[InputToken(ocr='This', gs='This', start=0, len_ocr=4, label=0), InputToken(ocr='is', gs='is', start=5, len_ocr=2, label=0), InputToken(ocr='a', gs='an', start=8, len_ocr=1, label=1), InputToken(ocr='cxample...', gs='example.', start=10, len_ocr=10, label=1)], score=0.2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_file = Path(os.getcwd()) / \"data\" / \"example.txt\"\n",
    "text = process_text(in_file)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "assert len(text.tokens) == 4\n",
    "assert len(text.input_tokens) == 4\n",
    "assert text.tokens[2].ocr == \"a\"\n",
    "assert text.score == 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# Token missing from ocr input (original):\n",
    "# [OCR_toInput] example &\n",
    "# [OCR_aligned] example & @@@\n",
    "# [ GS_aligned] example x bla\n",
    "\n",
    "# Because we only get the OCR text, we can't know the actual length of the text\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example3.txt\"\n",
    "\n",
    "text = process_text(in_file)\n",
    "\n",
    "assert len(text.tokens) == 2\n",
    "assert len(text.input_tokens) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631e506",
   "metadata": {},
   "source": [
    "## Process the entire dataset\n",
    "\n",
    "File structure of the ICDAR dataset\n",
    "```\n",
    ".\n",
    "├── <data_dir>\n",
    "│   ├── <language>\n",
    "│   │   ├── <language (set)>1\n",
    "│   │   ...\n",
    "│   │   └── <language (set)>n\n",
    "│   ...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def generate_data(in_dir: Path) -> Tuple[Dict[str, Text], pd.DataFrame]:\n",
    "    \"\"\"Process all texts in the dataset and return a dataframe with metadata\"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    file_languages = []\n",
    "    file_subsets = []\n",
    "    file_names = []\n",
    "    scores = []\n",
    "    num_tokens = []\n",
    "    num_input_tokens = []\n",
    "\n",
    "    for language_dir in tqdm(in_dir.iterdir()):\n",
    "        for text_file in language_dir.rglob(\"*.txt\"):\n",
    "            # print(text_file)\n",
    "            # print(text_file.relative_to(in_dir))\n",
    "            key = str(text_file.relative_to(in_dir))\n",
    "            data[key] = process_text(text_file)\n",
    "\n",
    "            language, subset, _ = key.split(os.sep)\n",
    "\n",
    "            file_languages.append(language)\n",
    "            file_subsets.append(subset)\n",
    "            file_names.append(key)\n",
    "            scores.append(data[key].score)\n",
    "            num_tokens.append(len(data[key].tokens))\n",
    "            num_input_tokens.append(len(data[key].input_tokens))\n",
    "    md = pd.DataFrame(\n",
    "        {\n",
    "            \"language\": file_languages,\n",
    "            \"subset\": file_subsets,\n",
    "            \"file_name\": file_names,\n",
    "            \"score\": scores,\n",
    "            \"num_tokens\": num_tokens,\n",
    "            \"num_input_tokens\": num_input_tokens,\n",
    "        }\n",
    "    )\n",
    "    md.sort_values(\"file_name\", inplace=True)\n",
    "    md.reset_index(inplace=True, drop=True)\n",
    "    return data, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae45311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 956.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "\n",
    "data, md = generate_data(data_dir)\n",
    "\n",
    "assert sorted(list(data.keys())) == md.file_name.to_list()\n",
    "assert md.index.to_list() == [i for i in range(md.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_icdar_data(out_dir: TypingText, zip_file: TypingText) -> Tuple[Path, Path]:\n",
    "    with ZipFile(zip_file, \"r\") as zip_object:\n",
    "        zip_object.extractall(path=out_dir)\n",
    "\n",
    "    # Copy Finnish data\n",
    "    path = Path(out_dir)\n",
    "    in_dir = path / \"TOOLS_for_Finnish_data\"\n",
    "    inputs = {\n",
    "        \"evaluation\": \"ICDAR2019_POCR_competition_evaluation_4M_without_Finnish\",\n",
    "        \"full\": \"ICDAR2019_POCR_competition_full_22M_without_Finnish\",\n",
    "        \"training\": \"ICDAR2019_POCR_competition_training_18M_without_Finnish\",\n",
    "    }\n",
    "    for from_dir, to_dir in inputs.items():\n",
    "        for in_file in (in_dir / \"output\" / from_dir).iterdir():\n",
    "            if in_file.is_file():\n",
    "                out_file = path / to_dir / \"FI\" / \"FI1\" / in_file.name\n",
    "                shutil.copy2(in_file, out_file)\n",
    "\n",
    "    # Get paths for train and test data\n",
    "    train_path = (\n",
    "        Path(out_dir) / \"ICDAR2019_POCR_competition_training_18M_without_Finnish\"\n",
    "    )\n",
    "    test_path = (\n",
    "        Path(out_dir) / \"ICDAR2019_POCR_competition_evaluation_4M_without_Finnish\"\n",
    "    )\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "\n",
    "def get_intermediate_data(\n",
    "    zip_file: TypingText,\n",
    ") -> Tuple[Dict[str, Text], pd.DataFrame, Dict[str, Text], pd.DataFrame]:\n",
    "    \"\"\"Get the data and metadata for the train and test sets from the zip file.\"\"\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        train_path, test_path = extract_icdar_data(tmp_dir, zip_file)\n",
    "\n",
    "        data, md = generate_data(train_path)\n",
    "        data_test, md_test = generate_data(test_path)\n",
    "\n",
    "    return (data, md, data_test, md_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1da77",
   "metadata": {},
   "source": [
    "## Generate input 'sentences'\n",
    "\n",
    "The following functions can be used to generate sequences of a certain length with possible overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def window(iterable, size=2):\n",
    "    \"\"\"Given an iterable, return all subsequences of a certain size\"\"\"\n",
    "    i = iter(iterable)\n",
    "    win = []\n",
    "    for e in range(0, size):\n",
    "        try:\n",
    "            win.append(next(i))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    yield win\n",
    "    for e in i:\n",
    "        win = win[1:] + [e]\n",
    "        yield win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "it = [1, 2, 3, 4]\n",
    "result = [w for w in window(it, 2)]\n",
    "\n",
    "assert result == [[1, 2], [2, 3], [3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _process_sequence(\n",
    "    key: str,\n",
    "    i: int,\n",
    "    res,\n",
    "    sents: List[List[str]],\n",
    "    labels: List[List[int]],\n",
    "    keys: List[str],\n",
    "    start_tokens: List[int],\n",
    "    scores: List[float],\n",
    "    languages: List[str],\n",
    ") -> Tuple[\n",
    "    List[List[str]], List[List[int]], List[str], List[int], List[float], List[str]\n",
    "]:\n",
    "    ocr = [t.ocr for t in res]\n",
    "    lbls = [t.label for t in res]\n",
    "    gs = []\n",
    "    for t in res:\n",
    "        if t.gs != \"\":\n",
    "            gs.append(t.gs)\n",
    "    ocr_str = \" \".join(ocr)\n",
    "    gs_str = \" \".join(gs)\n",
    "    ed = edlib.align(ocr_str, gs_str)[\"editDistance\"]\n",
    "    score = normalized_ed(ed, ocr_str, gs_str)\n",
    "\n",
    "    if len(ocr_str) > 0:\n",
    "        sents.append(ocr)\n",
    "        labels.append(lbls)\n",
    "        keys.append(key)\n",
    "        start_tokens.append(i)\n",
    "        scores.append(score)\n",
    "        languages.append(key[:2])\n",
    "    else:\n",
    "        logger.info(f'Empty sample for text \"{key}\"')\n",
    "        logger.info(f\"ocr_str: {ocr_str}\")\n",
    "        logger.info(f\"start token: {i}\")\n",
    "\n",
    "    return (sents, labels, keys, start_tokens, scores, languages)\n",
    "\n",
    "\n",
    "def generate_sentences(\n",
    "    df: pd.DataFrame, data: Dict[str, Text], size: int = 15, step: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate sequences of a certain length and possible overlap\"\"\"\n",
    "    sents: List[List[str]] = []\n",
    "    labels: List[List[int]] = []\n",
    "    keys: List[str] = []\n",
    "    start_tokens: List[int] = []\n",
    "    scores: List[float] = []\n",
    "    languages: List[str] = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        key = row.file_name\n",
    "        tokens = data[key].input_tokens\n",
    "\n",
    "        # print(len(tokens))\n",
    "        # print(key)\n",
    "        for i, res in enumerate(window(tokens, size=size)):\n",
    "            if i % step == 0:\n",
    "                (\n",
    "                    sents,\n",
    "                    labels,\n",
    "                    keys,\n",
    "                    start_tokens,\n",
    "                    scores,\n",
    "                    languages,\n",
    "                ) = _process_sequence(\n",
    "                    key, i, res, sents, labels, keys, start_tokens, scores, languages\n",
    "                )\n",
    "        # Add final sequence\n",
    "        (sents, labels, keys, start_tokens, scores, languages) = _process_sequence(\n",
    "            key, i, res, sents, labels, keys, start_tokens, scores, languages\n",
    "        )\n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"key\": keys,\n",
    "            \"start_token_id\": start_tokens,\n",
    "            \"score\": scores,\n",
    "            \"tokens\": sents,\n",
    "            \"tags\": labels,\n",
    "            \"language\": languages,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Adding the final sequence may lead to duplicate rows. Remove those\n",
    "    output.drop_duplicates(\n",
    "        subset=[\"key\", \"start_token_id\"], keep=\"first\", inplace=True, ignore_index=True\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 2695.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "in_file = Path(os.getcwd()) / \"data\" / \"example.txt\"\n",
    "text = process_text(in_file)\n",
    "data = {\"en/en1/1.txt\": text}\n",
    "md = pd.DataFrame(\n",
    "    {\n",
    "        \"language\": [\"en\"],\n",
    "        \"file_name\": [\"en/en1/1.txt\"],\n",
    "        \"score\": [text.score],\n",
    "        \"num_tokens\": [len(text.tokens)],\n",
    "        \"num_input_tokens\": [len(text.input_tokens)],\n",
    "    }\n",
    ")\n",
    "\n",
    "df = generate_sentences(md, data, size=2, step=2)\n",
    "\n",
    "assert 2 == df.shape[0]\n",
    "assert [0, 2] == list(df.start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ba632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def process_input_ocr(text: str) -> Text:\n",
    "    \"\"\"Generate Text object for OCR input text (without aligned gold standard)\"\"\"\n",
    "    tokens = []\n",
    "    for match in re.finditer(r\"\\b\\S+(\\s|$)\", text):\n",
    "        ocr = match.group().strip()\n",
    "        gs = ocr\n",
    "        start = match.start()\n",
    "        len_ocr = len(ocr)\n",
    "        label = 0\n",
    "\n",
    "        tokens.append(\n",
    "            InputToken(\n",
    "                ocr=ocr,\n",
    "                gs=gs,\n",
    "                start=start,\n",
    "                len_ocr=len_ocr,\n",
    "                label=label,\n",
    "            )\n",
    "        )\n",
    "    return Text(text, tokens=[], input_tokens=tokens, score=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed78d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 4275.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>start_token_id</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocr_input</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[This, is]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>oc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocr_input</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[a, cxample...]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>oc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         key  start_token_id  score           tokens    tags language\n",
       "0  ocr_input               0    0.0       [This, is]  [0, 0]       oc\n",
       "1  ocr_input               2    0.0  [a, cxample...]  [0, 0]       oc"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = process_input_ocr(\"This is a cxample...\")\n",
    "data = {\"ocr_input\": text}\n",
    "md = pd.DataFrame(\n",
    "    {\n",
    "        \"language\": [\"?\"],\n",
    "        \"file_name\": [\"ocr_input\"],\n",
    "        \"score\": [text.score],\n",
    "        \"num_tokens\": [len(text.tokens)],\n",
    "        \"num_input_tokens\": [len(text.input_tokens)],\n",
    "    }\n",
    ")\n",
    "\n",
    "df = generate_sentences(md, data, size=2, step=2)\n",
    "\n",
    "assert 2 == df.shape[0]\n",
    "assert [0, 2] == list(df.start_token_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
