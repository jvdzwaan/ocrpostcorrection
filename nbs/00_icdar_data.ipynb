{
 "cells": [
  {
   "cell_type": "raw",
   "id": "posted-thailand",
   "metadata": {},
   "source": [
    "---\n",
    "title: ICDAR Data\n",
    "output-file: icdar_data.html\n",
    "description: Functionality for working with the ICDAR dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp icdar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import edlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48cfe0",
   "metadata": {},
   "source": [
    "\n",
    "The [ICDAR 2019 Competition on Post-OCR Text Correction \n",
    "dataset](https://sites.google.com/view/icdar2019-postcorrectionocr/dataset) \n",
    "([zenodo record](https://zenodo.org/record/3515403#.YwULoi0RoWI))\n",
    "contains text files in the following format:\n",
    "\n",
    "```\n",
    "[OCR_toInput] This is a cxample...\n",
    "[OCR_aligned] This is a@ cxample...\n",
    "[ GS_aligned] This is an example.##\n",
    "01234567890123\n",
    "```\n",
    "\n",
    "The first line contains the ocr input text. The second line contains the aligned \n",
    "ocr and the third line contains the aligned gold standard.\n",
    "`@` is the aligment character and `#` represents characters in the OCR that do \n",
    "not occur in the gold standard.\n",
    "\n",
    "For working with this data, the first 14 characters have to be removed. We also \n",
    "remove leading and trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_label_and_nl(line: str):\n",
    "    return line.strip()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert remove_label_and_nl('[OCR_toInput] This is a cxample...\\n') == 'This is a cxample...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a85ac1",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Task 1 of the competition is about finding tokens with OCR mistakes. In this context a token\n",
    "refers to a string between two whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class AlignedToken:\n",
    "    \"\"\"Dataclass for storing aligned tokens\"\"\"\n",
    "    ocr: str  # String in the OCR text\n",
    "    gs: str  # String in the gold standard\n",
    "    ocr_aligned: str  # String in the aligned OCR text (without aligmnent characters)\n",
    "    gs_aligned: str  # String in the aligned GS text (without aligmnent characters)\n",
    "    start: int  # The index of the first character in the OCR text\n",
    "    len_ocr: int  # The lentgh of the OCR string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_aligned(ocr_aligned: str, gs_aligned: str):\n",
    "    \"\"\"Get a list of AlignedTokens from the aligned OCR and GS strings\"\"\"\n",
    "\n",
    "    ocr_cursor = 0\n",
    "    start = 0\n",
    "\n",
    "    ocr_token_chars = []\n",
    "    gs_token_chars = []\n",
    "    ocr_token_chars_aligned = []\n",
    "    gs_token_chars_aligned = []\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for ocr_aligned_char, gs_aligned_char in zip(ocr_aligned, gs_aligned):\n",
    "        #print(ocr_aligned_char, gs_aligned_char, ocr_cursor)\n",
    "        # The # character in ocr is not an aligment character!\n",
    "        if ocr_aligned_char != '@':\n",
    "            ocr_cursor += 1\n",
    "\n",
    "        if ocr_aligned_char == ' ' and gs_aligned_char == ' ':\n",
    "            #print('TOKEN')\n",
    "            #print('OCR:', repr(''.join(ocr_token_chars)))\n",
    "            #print(' GS:', repr(''.join(gs_token_chars)))\n",
    "            #print('start:', start_char)\n",
    "            #ocr_cursor += 1\n",
    "\n",
    "            # Ignore 'tokens' without representation in the ocr text\n",
    "            # (these tokens do not consist of characters)\n",
    "            ocr = (''.join(ocr_token_chars)).strip()\n",
    "            if ocr != '':\n",
    "                tokens.append(AlignedToken(ocr,\n",
    "                                          ''.join(gs_token_chars),\n",
    "                                          ''.join(ocr_token_chars_aligned),\n",
    "                                          ''.join(gs_token_chars_aligned),\n",
    "                                          start,\n",
    "                                          len(''.join(ocr_token_chars))))\n",
    "            start = ocr_cursor\n",
    "\n",
    "            ocr_token_chars = []\n",
    "            gs_token_chars = []\n",
    "            ocr_token_chars_aligned = []\n",
    "            gs_token_chars_aligned = []\n",
    "        else:\n",
    "            ocr_token_chars_aligned.append(ocr_aligned_char)\n",
    "            gs_token_chars_aligned.append(gs_aligned_char)\n",
    "            # The # character in ocr is not an aligment character!\n",
    "            if ocr_aligned_char != '@':\n",
    "                ocr_token_chars.append(ocr_aligned_char)\n",
    "            if gs_aligned_char != '@' and gs_aligned_char != '#':\n",
    "                gs_token_chars.append(gs_aligned_char)\n",
    "\n",
    "    # Final token (if there is one)\n",
    "    ocr = (''.join(ocr_token_chars)).strip()\n",
    "    if ocr != '':\n",
    "        tokens.append(AlignedToken(ocr,\n",
    "                                   ''.join(gs_token_chars),\n",
    "                                   ''.join(ocr_token_chars_aligned),\n",
    "                                   ''.join(gs_token_chars_aligned),\n",
    "                                   start,\n",
    "                                   len(''.join(ocr_token_chars))))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25b44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4),\n",
       " AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2),\n",
       " AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1),\n",
       " AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.##', start=10, len_ocr=10)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_aligned('This is a@ cxample...', 'This is an example.##')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2b1dd",
   "metadata": {},
   "source": [
    "The OCR text of an AlignedToken may still consist of multiple tokens. This is the \n",
    "case when the OCR text contains one or more spaces. To make sure the (sub)tokenization\n",
    "of a token is the same, no matter if it was not yet tokenized completely, \n",
    "another round of tokenization is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class InputToken:\n",
    "    \"\"\"Dataclass for the tokenization within AlignedTokens\"\"\"\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    start: int\n",
    "    len_ocr: int\n",
    "    label: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_input_tokens(aligned_token: AlignedToken):\n",
    "    \"\"\"Tokenize an AlignedToken into subtokens and assign task 1 labels\"\"\"\n",
    "    if aligned_token.ocr == aligned_token.gs:\n",
    "            yield InputToken(aligned_token.ocr, aligned_token.gs,\n",
    "                             aligned_token.start, len(aligned_token.ocr), 0)\n",
    "    else:\n",
    "        parts = aligned_token.ocr.split(' ')\n",
    "        new_start = aligned_token.start\n",
    "        for i, part in enumerate(parts):\n",
    "            if i == 0:\n",
    "                yield InputToken(part, aligned_token.gs, aligned_token.start,\n",
    "                                 len(part), 1)\n",
    "            else:\n",
    "                yield InputToken(part, '', new_start, len(part), 2)\n",
    "            new_start += len(part) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='Major', gs='Major', ocr_aligned='Major', gs_aligned='Major', start=19, len_ocr=5)\n",
      "InputToken(ocr='Major', gs='Major', start=19, len_ocr=5, label=0)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken('Major', 'Major', 'Major', 'Major', 19, 5)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fded73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "    \n",
    "assert tokens == ['Major']\n",
    "assert labels == [0]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='INEVR', gs='I NEVER', ocr_aligned='I@NEV@R', gs_aligned='I NEVER', start=0, len_ocr=5)\n",
      "InputToken(ocr='INEVR', gs='I NEVER', start=0, len_ocr=5, label=1)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken('INEVR', 'I NEVER', 'I@NEV@R', 'I NEVER', 0, 5)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "    \n",
    "assert tokens == ['INEVR']\n",
    "assert labels == [1]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedToken(ocr='Long ow.', gs='Longhow.', ocr_aligned='Long ow.', gs_aligned='Longhow.', start=24, len_ocr=8)\n",
      "InputToken(ocr='Long', gs='Longhow.', start=24, len_ocr=4, label=1)\n",
      "InputToken(ocr='ow.', gs='', start=29, len_ocr=3, label=2)\n"
     ]
    }
   ],
   "source": [
    "t = AlignedToken('Long ow.', 'Longhow.', 'Long ow.', 'Longhow.', 24, 8)\n",
    "print(t)\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac965c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['Long', 'ow.']\n",
    "assert labels == [1, 2]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340a137",
   "metadata": {},
   "source": [
    "## Process a text file\n",
    "\n",
    "Next, we need functions for processing a text in the ICDAR data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Text:\n",
    "    \"\"\"Dataclass for storing a text in the ICDAR data format\"\"\"\n",
    "    ocr_text: str\n",
    "    tokens: list\n",
    "    input_tokens: list\n",
    "    score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean(string: str):\n",
    "    \"\"\"Remove alignment characters from a text\"\"\"\n",
    "    string = string.replace('@', '')\n",
    "    string = string.replace('#', '')\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalized_ed(ed: int, \n",
    "                  ocr: str, \n",
    "                  gs: str):\n",
    "    \"\"\"Returns the normalized editdistance\"\"\"\n",
    "    score = 0.0\n",
    "    l = max(len(ocr), len(gs))\n",
    "    if l > 0:\n",
    "        score = ed / l\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_text(in_file: Path) -> Text:\n",
    "    \"\"\"Extract AlignedTokens, InputTokens from a text file and calculate normalized editdistance\"\"\"\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # The # character in ocr input is not an aligment character, but the @\n",
    "    # character is!\n",
    "    ocr_input = remove_label_and_nl(lines[0]).replace('@', '')\n",
    "    ocr_aligned = remove_label_and_nl(lines[1])\n",
    "    gs_aligned = remove_label_and_nl(lines[2])\n",
    "\n",
    "    #print('ocr input:', ocr_input)\n",
    "    #print('ocr aligned:', ocr_aligned)\n",
    "    #print('gs aligned:',gs_aligned)\n",
    "\n",
    "    tokens = tokenize_aligned(ocr_aligned, gs_aligned)\n",
    "\n",
    "    # Check data\n",
    "    for token in tokens:\n",
    "        input_token = ocr_input[token.start:token.start+token.len_ocr]\n",
    "        try:\n",
    "            assert token.ocr == input_token.strip()\n",
    "        except AssertionError:\n",
    "            logger.warning(f'OCR != aligned OCR: Text: {str(in_file)}; ocr: {repr(token.ocr)}; ocr_input: {repr(input_token)}')\n",
    "            raise\n",
    "\n",
    "    ocr = clean(ocr_aligned)\n",
    "    gs = clean(gs_aligned)\n",
    "\n",
    "    try:\n",
    "        ed = edlib.align(gs, ocr)['editDistance']\n",
    "        score = normalized_ed(ed, ocr, gs)\n",
    "    except UnicodeEncodeError:\n",
    "        logger.warning(f'UnicodeEncodeError for text {in_file}; setting score to 1')\n",
    "        score = 1\n",
    "\n",
    "    input_tokens = []\n",
    "    for token in tokens:\n",
    "        for inp_tok in get_input_tokens(token):\n",
    "            input_tokens.append(inp_tok)\n",
    "\n",
    "    return Text(ocr_input, tokens, input_tokens, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf19c72",
   "metadata": {},
   "source": [
    "Processing the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09381f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(ocr_text='This is a cxample...', tokens=[AlignedToken(ocr='This', gs='This', ocr_aligned='This', gs_aligned='This', start=0, len_ocr=4), AlignedToken(ocr='is', gs='is', ocr_aligned='is', gs_aligned='is', start=5, len_ocr=2), AlignedToken(ocr='a', gs='an', ocr_aligned='a@', gs_aligned='an', start=8, len_ocr=1), AlignedToken(ocr='cxample...', gs='example.', ocr_aligned='cxample...', gs_aligned='example.##', start=10, len_ocr=10)], input_tokens=[InputToken(ocr='This', gs='This', start=0, len_ocr=4, label=0), InputToken(ocr='is', gs='is', start=5, len_ocr=2, label=0), InputToken(ocr='a', gs='an', start=8, len_ocr=1, label=1), InputToken(ocr='cxample...', gs='example.', start=10, len_ocr=10, label=1)], score=0.2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "in_file = Path(os.getcwd())/'data'/'example.txt'\n",
    "text = process_text(in_file)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert len(text.tokens) == 4\n",
    "assert len(text.input_tokens) == 4\n",
    "assert text.tokens[2].ocr == 'a'\n",
    "assert text.score == 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631e506",
   "metadata": {},
   "source": [
    "## Process the entire dataset\n",
    "\n",
    "File structure of the ICDAR dataset\n",
    "```\n",
    ".\n",
    "├── <data_dir>\n",
    "│   ├── <language>\n",
    "│   │   ├── <language (set)>1\n",
    "│   │   ...\n",
    "│   │   └── <language (set)>n\n",
    "│   ...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_data(in_dir: Path):\n",
    "    \"\"\"Process all texts in the dataset and return a dataframe with metadata\"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    file_languages = []\n",
    "    file_names = []\n",
    "    scores = []\n",
    "    num_tokens = []\n",
    "    num_input_tokens = []\n",
    "\n",
    "    for language_dir in tqdm(in_dir.iterdir()):\n",
    "        #print(language_dir.stem)\n",
    "        language = language_dir.stem\n",
    "\n",
    "        for text_file in language_dir.rglob('*.txt'):\n",
    "            #print(text_file)\n",
    "            #print(text_file.relative_to(in_dir))\n",
    "            key = str(text_file.relative_to(in_dir))\n",
    "            data[key] = process_text(text_file)\n",
    "\n",
    "            file_languages.append(language)\n",
    "            file_names.append(key)\n",
    "            scores.append(data[key].score)\n",
    "            num_tokens.append(len(data[key].tokens))\n",
    "            num_input_tokens.append(len(data[key].input_tokens))\n",
    "    md = pd.DataFrame({'language': file_languages,\n",
    "                    'file_name': file_names,\n",
    "                    'score': scores,\n",
    "                    'num_tokens': num_tokens,\n",
    "                    'num_input_tokens': num_input_tokens})\n",
    "    return data, md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1da77",
   "metadata": {},
   "source": [
    "## Generate input 'sentences'\n",
    "\n",
    "The following functions can be used to generate sequences of a certain length with possible overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def window(iterable, size=2):\n",
    "    \"\"\"Given an iterable, return all subsequences of a certain size\"\"\"\n",
    "    i = iter(iterable)\n",
    "    win = []\n",
    "    for e in range(0, size):\n",
    "        try:\n",
    "            win.append(next(i))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    yield win\n",
    "    for e in i:\n",
    "        win = win[1:] + [e]\n",
    "        yield win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_sentences(df, data, size=15, step=10):\n",
    "    \"\"\"Generate sequences of a certain length and possible overlap\"\"\"\n",
    "    sents = []\n",
    "    labels = []\n",
    "    keys = []\n",
    "    start_tokens = []\n",
    "    scores = []\n",
    "    languages = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        key = row.file_name\n",
    "        tokens = data[key].input_tokens\n",
    "\n",
    "        # print(len(tokens))\n",
    "        # print(key)\n",
    "        for i, res in enumerate(window(tokens, size=size)):\n",
    "            if i % step == 0:\n",
    "                ocr = [t.ocr for t in res]\n",
    "                lbls = [t.label for t in res]\n",
    "                gs = []\n",
    "                for t in res:\n",
    "                    if t.gs != '':\n",
    "                        gs.append(t.gs)\n",
    "                ocr_str = ' '.join(ocr)\n",
    "                gs_str = ' '.join(gs)\n",
    "                ed = edlib.align(ocr_str, gs_str)['editDistance']\n",
    "                score = normalized_ed(ed, ocr_str, gs_str)\n",
    "\n",
    "                if len(ocr_str) > 0:\n",
    "\n",
    "                    sents.append(ocr)\n",
    "                    labels.append(lbls)\n",
    "                    keys.append(key)\n",
    "                    start_tokens.append(i)\n",
    "                    scores.append(score)\n",
    "                    languages.append(key[:2])\n",
    "                else:\n",
    "                    logger.info(f'Empty sample for text \"{key}\"')\n",
    "                    logger.info(f'ocr_str: {ocr_str}')\n",
    "                    logger.info(f'start token: {i}')\n",
    "    data = pd.DataFrame({\n",
    "        'key': keys,\n",
    "        'start_token_id': start_tokens,\n",
    "        'score': scores,\n",
    "        'tokens': sents,\n",
    "        'tags': labels,\n",
    "        'language': languages\n",
    "    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('nlp4dutch')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
