{
 "cells": [
  {
   "cell_type": "raw",
   "id": "posted-thailand",
   "metadata": {},
   "source": [
    "---\n",
    "title: icdar_data\n",
    "output-file: icdar_data.html\n",
    "description: Functionality to do with the ICDAR dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp icdar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_label_and_nl(line: str):\n",
    "    return line.strip()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalized_ed(ed: int, \n",
    "                  ocr: str, \n",
    "                  gs: str):\n",
    "    \"\"\"Returns the normalized editdistance\"\"\"\n",
    "    score = 0.0\n",
    "    l = max(len(ocr), len(gs))\n",
    "    if l > 0:\n",
    "        score = ed / l\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class AlignedToken:\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    ocr_aligned: str\n",
    "    gs_aligned: str\n",
    "    start: int\n",
    "    len_ocr: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class InputToken:\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    start: int\n",
    "    len_ocr: int\n",
    "    label: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_input_tokens(aligned_token):\n",
    "    if aligned_token.ocr == aligned_token.gs:\n",
    "            yield InputToken(aligned_token.ocr, aligned_token.gs,\n",
    "                             aligned_token.start, len(aligned_token.ocr), 0)\n",
    "    else:\n",
    "        parts = aligned_token.ocr.split(' ')\n",
    "        new_start = aligned_token.start\n",
    "        for i, part in enumerate(parts):\n",
    "            if i == 0:\n",
    "                yield InputToken(part, aligned_token.gs, aligned_token.start,\n",
    "                                 len(part), 1)\n",
    "            else:\n",
    "                yield InputToken(part, '', new_start, len(part), 2)\n",
    "            new_start += len(part) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('Long ow.', 'Longhow.', 'Long ow.', 'Longhow.', 24, 8)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['Long', 'ow.']\n",
    "assert labels == [1, 2]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('INEVR', 'I NEVER', 'I@NEV@R', 'I NEVER', 0, 5)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['INEVR']\n",
    "assert labels == [1]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('Major', 'Major', 'Major', 'Major', 19, 5)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['Major']\n",
    "assert labels == [0]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize_aligned(ocr_aligned, gs_aligned):\n",
    "\n",
    "    ocr_cursor = 0\n",
    "    start = 0\n",
    "\n",
    "    ocr_token_chars = []\n",
    "    gs_token_chars = []\n",
    "    ocr_token_chars_aligned = []\n",
    "    gs_token_chars_aligned = []\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for ocr_aligned_char, gs_aligned_char in zip(ocr_aligned, gs_aligned):\n",
    "        #print(ocr_aligned_char, gs_aligned_char, ocr_cursor)\n",
    "        # The # character in ocr is not an aligment character!\n",
    "        if ocr_aligned_char != '@':\n",
    "            ocr_cursor += 1\n",
    "\n",
    "        if ocr_aligned_char == ' ' and gs_aligned_char == ' ':\n",
    "            #print('TOKEN')\n",
    "            #print('OCR:', repr(''.join(ocr_token_chars)))\n",
    "            #print(' GS:', repr(''.join(gs_token_chars)))\n",
    "            #print('start:', start_char)\n",
    "            #ocr_cursor += 1\n",
    "\n",
    "            # Ignore 'tokens' without representation in the ocr text\n",
    "            # (these tokens do not consist of characters)\n",
    "            ocr = (''.join(ocr_token_chars)).strip()\n",
    "            if ocr != '':\n",
    "                tokens.append(AlignedToken(ocr,\n",
    "                                          ''.join(gs_token_chars),\n",
    "                                          ''.join(ocr_token_chars_aligned),\n",
    "                                          ''.join(gs_token_chars_aligned),\n",
    "                                          start,\n",
    "                                          len(''.join(ocr_token_chars))))\n",
    "            start = ocr_cursor\n",
    "\n",
    "            ocr_token_chars = []\n",
    "            gs_token_chars = []\n",
    "            ocr_token_chars_aligned = []\n",
    "            gs_token_chars_aligned = []\n",
    "        else:\n",
    "            ocr_token_chars_aligned.append(ocr_aligned_char)\n",
    "            gs_token_chars_aligned.append(gs_aligned_char)\n",
    "            # The # character in ocr is not an aligment character!\n",
    "            if ocr_aligned_char != '@':\n",
    "                ocr_token_chars.append(ocr_aligned_char)\n",
    "            if gs_aligned_char != '@' and gs_aligned_char != '#':\n",
    "                gs_token_chars.append(gs_aligned_char)\n",
    "\n",
    "    ocr = (''.join(ocr_token_chars)).strip()\n",
    "    if ocr != '':\n",
    "        tokens.append(AlignedToken(ocr,\n",
    "                                   ''.join(gs_token_chars),\n",
    "                                   ''.join(ocr_token_chars_aligned),\n",
    "                                   ''.join(gs_token_chars_aligned),\n",
    "                                   start,\n",
    "                                   len(''.join(ocr_token_chars))))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def window(iterable, size=2):\n",
    "    i = iter(iterable)\n",
    "    win = []\n",
    "    for e in range(0, size):\n",
    "        try:\n",
    "            win.append(next(i))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    yield win\n",
    "    for e in i:\n",
    "        win = win[1:] + [e]\n",
    "        yield win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Text:\n",
    "    ocr_text: str\n",
    "    tokens: list\n",
    "    input_tokens: list\n",
    "    score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def clean(string):\n",
    "    string = string.replace('@', '')\n",
    "    string = string.replace('#', '')\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_text(in_file):\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # The # character in ocr input is not an aligment character, but the @\n",
    "    # character is!\n",
    "    ocr_input = remove_label_and_nl(lines[0]).replace('@', '')\n",
    "    ocr_aligned = remove_label_and_nl(lines[1])\n",
    "    gs_aligned = remove_label_and_nl(lines[2])\n",
    "\n",
    "    #print('ocr input:', ocr_input)\n",
    "    #print('ocr aligned:', ocr_aligned)\n",
    "    #print('gs aligned:',gs_aligned)\n",
    "\n",
    "    tokens = tokenize_aligned(ocr_aligned, gs_aligned)\n",
    "\n",
    "    # Check data\n",
    "    for token in tokens:\n",
    "        input_token = ocr_input[token.start:token.start+token.len_ocr]\n",
    "        try:\n",
    "            assert token.ocr == input_token.strip()\n",
    "        except AssertionError:\n",
    "            logger.warning(f'OCR != aligned OCR: Text: {str(in_file)}; ocr: {repr(token.ocr)}; ocr_input: {repr(input_token)}')\n",
    "            raise\n",
    "\n",
    "    ocr = clean(ocr_aligned)\n",
    "    gs = clean(gs_aligned)\n",
    "\n",
    "    try:\n",
    "        ed = edlib.align(gs, ocr)['editDistance']\n",
    "        score = normalized_ed(ed, ocr, gs)\n",
    "    except UnicodeEncodeError:\n",
    "        logger.warning(f'UnicodeEncodeError for text {in_file}; setting score to 1')\n",
    "        score = 1\n",
    "\n",
    "    input_tokens = []\n",
    "    for token in tokens:\n",
    "        for inp_tok in get_input_tokens(token):\n",
    "            input_tokens.append(inp_tok)\n",
    "\n",
    "    return Text(ocr_input, tokens, input_tokens, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_data(in_dir):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    file_languages = []\n",
    "    file_names = []\n",
    "    scores = []\n",
    "    num_tokens = []\n",
    "    num_input_tokens = []\n",
    "\n",
    "    for language_dir in tqdm(in_dir.iterdir()):\n",
    "        #print(language_dir.stem)\n",
    "        language = language_dir.stem\n",
    "\n",
    "        for text_file in language_dir.rglob('*.txt'):\n",
    "            #print(text_file)\n",
    "            #print(text_file.relative_to(in_dir))\n",
    "            key = str(text_file.relative_to(in_dir))\n",
    "            data[key] = process_text(text_file)\n",
    "\n",
    "            file_languages.append(language)\n",
    "            file_names.append(key)\n",
    "            scores.append(data[key].score)\n",
    "            num_tokens.append(len(data[key].tokens))\n",
    "            num_input_tokens.append(len(data[key].input_tokens))\n",
    "    md = pd.DataFrame({'language': file_languages,\n",
    "                    'file_name': file_names,\n",
    "                    'score': scores,\n",
    "                    'num_tokens': num_tokens,\n",
    "                    'num_input_tokens': num_input_tokens})\n",
    "    return data, md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_sentences(df, data, size=15, step=10):\n",
    "    sents = []\n",
    "    labels = []\n",
    "    keys = []\n",
    "    start_tokens = []\n",
    "    scores = []\n",
    "    languages = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        key = row.file_name\n",
    "        tokens = data[key].input_tokens\n",
    "\n",
    "        # print(len(tokens))\n",
    "        # print(key)\n",
    "        for i, res in enumerate(window(tokens, size=size)):\n",
    "            if i % step == 0:\n",
    "                ocr = [t.ocr for t in res]\n",
    "                lbls = [t.label for t in res]\n",
    "                gs = []\n",
    "                for t in res:\n",
    "                    if t.gs != '':\n",
    "                        gs.append(t.gs)\n",
    "                ocr_str = ' '.join(ocr)\n",
    "                gs_str = ' '.join(gs)\n",
    "                ed = edlib.align(ocr_str, gs_str)['editDistance']\n",
    "                score = normalized_ed(ed, ocr_str, gs_str)\n",
    "\n",
    "                if len(ocr_str) > 0:\n",
    "\n",
    "                    sents.append(ocr)\n",
    "                    labels.append(lbls)\n",
    "                    keys.append(key)\n",
    "                    start_tokens.append(i)\n",
    "                    scores.append(score)\n",
    "                    languages.append(key[:2])\n",
    "                else:\n",
    "                    logger.info(f'Empty sample for text \"{key}\"')\n",
    "                    logger.info(f'ocr_str: {ocr_str}')\n",
    "                    logger.info(f'start token: {i}')\n",
    "    data = pd.DataFrame({\n",
    "        'key': keys,\n",
    "        'start_token_id': start_tokens,\n",
    "        'score': scores,\n",
    "        'tokens': sents,\n",
    "        'tags': labels,\n",
    "        'language': languages\n",
    "    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
