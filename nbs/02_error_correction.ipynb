{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Error Correction\n",
    "output-file: error_correction.html\n",
    "description: Functionality for error correction (task 2)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp error_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import dataclasses\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from ocrpostcorrection.icdar_data import Text, normalized_ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import edlib\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ocrpostcorrection.icdar_data import generate_data\n",
    "from ocrpostcorrection.simple_correction_data import GreedySearchDecoder, SimpleCorrectionDataset, collate_fn, predict_and_convert_to_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "A dataset for token correction consists of the OCR text and gold standard of AlignedTokens. \n",
    "These can be extracted from the Text objects using the `get_tokens_with_OCR_mistakes` function.\n",
    "This function also adds data properties that can be used for calculating statistics \n",
    "about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_tokens_with_OCR_mistakes(\n",
    "    data: Dict[str, Text], data_test: Dict[str, Text], val_files: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return pandas dataframe with all OCR mistakes from train, val, and test\"\"\"\n",
    "    tokens = []\n",
    "    # Train and val\n",
    "    for key, d in data.items():\n",
    "        for token in d.tokens:\n",
    "            if token.ocr.strip() != token.gs.strip():\n",
    "                r = dataclasses.asdict(token)\n",
    "                r[\"language\"] = key[:2]\n",
    "                r[\"subset\"] = key.split(\"/\")[1]\n",
    "\n",
    "                if key in val_files:\n",
    "                    r[\"dataset\"] = \"val\"\n",
    "                else:\n",
    "                    r[\"dataset\"] = \"train\"\n",
    "\n",
    "                tokens.append(r)\n",
    "    # Test\n",
    "    for key, d in data_test.items():\n",
    "        for token in d.tokens:\n",
    "            if token.ocr.strip() != token.gs.strip():\n",
    "                r = dataclasses.asdict(token)\n",
    "                r[\"language\"] = key[:2]\n",
    "                r[\"subset\"] = key.split(\"/\")[1]\n",
    "                r[\"dataset\"] = \"test\"\n",
    "\n",
    "                tokens.append(r)\n",
    "    tdata = pd.DataFrame(tokens)\n",
    "    tdata = _add_update_data_properties(tdata)\n",
    "\n",
    "    return tdata\n",
    "\n",
    "\n",
    "def _add_update_data_properties(tdata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add and update data properties for calculating statistics\"\"\"\n",
    "    tdata[\"ocr\"] = tdata[\"ocr\"].apply(lambda x: x.strip())\n",
    "    tdata[\"gs\"] = tdata[\"gs\"].apply(lambda x: x.strip())\n",
    "    tdata[\"len_ocr\"] = tdata.apply(lambda row: len(row.ocr), axis=1)\n",
    "    tdata[\"len_gs\"] = tdata.apply(lambda row: len(row.gs), axis=1)\n",
    "    tdata[\"diff\"] = tdata.len_ocr - tdata.len_gs\n",
    "    return tdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code example shows how use this function. \n",
    "For simplicity, in the example below, the `data` dictionary (which contain \n",
    "`<file name>: Text` pairs) is used both as train/val and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 606.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>gs</th>\n",
       "      <th>ocr_aligned</th>\n",
       "      <th>gs_aligned</th>\n",
       "      <th>start</th>\n",
       "      <th>len_ocr</th>\n",
       "      <th>language</th>\n",
       "      <th>subset</th>\n",
       "      <th>dataset</th>\n",
       "      <th>len_gs</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test- AAA</td>\n",
       "      <td>test-.AAA</td>\n",
       "      <td>test- AAA</td>\n",
       "      <td>test-.AAA</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-BBB</td>\n",
       "      <td>test- BBB</td>\n",
       "      <td>test@-BBB</td>\n",
       "      <td>test- BBB</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-CCC</td>\n",
       "      <td>test- CCC</td>\n",
       "      <td>test-@CCC</td>\n",
       "      <td>test- CCC</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-DDD</td>\n",
       "      <td>DDD</td>\n",
       "      <td>-DDD</td>\n",
       "      <td>DDD</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test- EEE</td>\n",
       "      <td>test-EEE</td>\n",
       "      <td>test- EEE</td>\n",
       "      <td>test-@EEE</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr_sample</td>\n",
       "      <td>train</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ocr         gs ocr_aligned gs_aligned  start  len_ocr language  \\\n",
       "0  test- AAA  test-.AAA   test- AAA  test-.AAA      0        9       fr   \n",
       "1   test-BBB  test- BBB   test@-BBB  test- BBB     10        8       fr   \n",
       "2   test-CCC  test- CCC   test-@CCC  test- CCC     19        8       fr   \n",
       "3       -DDD        DDD        -DDD        DDD     33        4       fr   \n",
       "4  test- EEE   test-EEE   test- EEE  test-@EEE     38        9       fr   \n",
       "\n",
       "      subset dataset  len_gs  diff  \n",
       "0  fr_sample   train       9     0  \n",
       "1  fr_sample   train       9    -1  \n",
       "2  fr_sample   train       9    -1  \n",
       "3  fr_sample   train       3     1  \n",
       "4  fr_sample   train       8     1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "data, md = generate_data(data_dir)\n",
    "val_files = [\"en/eng_sample/2.txt\"]\n",
    "\n",
    "token_data = get_tokens_with_OCR_mistakes(data, data, val_files)\n",
    "token_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabularies\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define special symbols and indices and make sure the tokens are in order of their indices to properly insert them in vocababulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def yield_tokens(data, col):\n",
    "    \"\"\"Helper function to create vocabulary containing characters\"\"\"\n",
    "    for token in data[col].to_list():\n",
    "        for char in token:\n",
    "            yield char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_vocabs(train):\n",
    "    \"\"\"Generate ocr and gs vocabularies from the train set\"\"\"\n",
    "    vocab_transform = {}\n",
    "    for name in (\"ocr\", \"gs\"):\n",
    "        vocab_transform[name] = build_vocab_from_iterator(\n",
    "            yield_tokens(train, name),\n",
    "            min_freq=1,\n",
    "            specials=special_symbols,\n",
    "            special_first=True,\n",
    "        )\n",
    "    # Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "    for name in (\"ocr\", \"gs\"):\n",
    "        vocab_transform[name].set_default_index(UNK_IDX)\n",
    "\n",
    "    return vocab_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trainset to create the ocr and gs vocabularies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_transform = generate_vocabs(token_data.query('dataset == \"train\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 44)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform[\"ocr\"]), len(vocab_transform[\"gs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation\n",
    "\n",
    "The character sequences need to be transformed into vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"Helper function to club together sequential operations\"\"\"\n",
    "\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    \"\"\"Function to add BOS/EOS and create tensor for input sequence indices\"\"\"\n",
    "    return torch.cat((torch.tensor(token_ids), torch.tensor([EOS_IDX])))\n",
    "\n",
    "\n",
    "def get_text_transform(vocab_transform):\n",
    "    \"\"\"Returns text transforms to convert raw strings into tensors indices\"\"\"\n",
    "    text_transform = {}\n",
    "    for name in (\"ocr\", \"gs\"):\n",
    "        text_transform[name] = sequential_transforms(\n",
    "            vocab_transform[name], tensor_transform  # Numericalization (char -> idx)\n",
    "        )  # Add BOS/EOS and create tensor\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  4,  7, 10, 13, 13, 13,  3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transform = get_text_transform(vocab_transform)\n",
    "\n",
    "text_transform[\"ocr\"]([\"t\", \"e\", \"s\", \"t\", \"-\", \" \", \"A\", \"A\", \"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  0, 21, 34, 22, 33,  5,  3])\n",
      "tensor([ 5,  0, 21, 27, 23, 26,  5,  3])\n"
     ]
    }
   ],
   "source": [
    "text_transform = get_text_transform(vocab_transform)\n",
    "\n",
    "print(text_transform[\"ocr\"]([\"e\", \"x\", \"a\", \"m\", \"p\", \"l\", \"e\"]))\n",
    "print(text_transform[\"gs\"]([\"e\", \"x\", \"a\", \"m\", \"p\", \"l\", \"e\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # print('Encoder')\n",
    "        # print('input size', input.size())\n",
    "        # print('hidden size', hidden.size())\n",
    "        embedded = self.embedding(input)\n",
    "        # print('embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "        # print('embedded size met view', embedded.view(1, 1, -1).size())\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # print('embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "        embedded = torch.permute(embedded, (1, 0, 2))\n",
    "        # print('permuted embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "\n",
    "        # print('hidden size', hidden.size())\n",
    "        # print(hidden)\n",
    "\n",
    "        # print('permuted embedded[0] size', embedded[0].size())\n",
    "        # print('hidden[0] size', hidden[0].size())\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )\n",
    "\n",
    "        # print('attn_weights', attn_weights.size())\n",
    "        # print('attn_weights unsqueeze(1)', attn_weights.unsqueeze(1).size())\n",
    "        # print('encoder outputs', encoder_outputs.size())\n",
    "\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        # print('embedded[0]', embedded[0].size())\n",
    "        # print('attn_applied', attn_applied.size())\n",
    "        # print('attn_applied squeeze', attn_applied.squeeze().size())\n",
    "        output = torch.cat((embedded[0], attn_applied.squeeze(1)), 1)\n",
    "        # print('output', output.size())\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # print('output', output.size())\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        # print(f'output: {output.size()}; hidden: {hidden.size()}; attn_weigts: {attn_weights.size()}')\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class SimpleCorrectionSeq2seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        dropout,\n",
    "        max_length,\n",
    "        teacher_forcing_ratio,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(SimpleCorrectionSeq2seq, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.max_length = max_length + 1\n",
    "\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(\n",
    "            hidden_size, output_size, dropout_p=dropout, max_length=self.max_length\n",
    "        )\n",
    "\n",
    "    def forward(self, input, encoder_hidden, target):\n",
    "        # input is src seq len x batch size\n",
    "        # input voor de encoder (1 stap) moet zijn input seq len x batch size x 1\n",
    "        input_tensor = input.unsqueeze(2)\n",
    "        # print('input tensor size', input_tensor.size())\n",
    "\n",
    "        input_length = input.size(0)\n",
    "\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Encoder part\n",
    "        encoder_outputs = torch.zeros(\n",
    "            batch_size, self.max_length, self.encoder.hidden_size, device=self.device\n",
    "        )\n",
    "        # print('encoder outputs size', encoder_outputs.size())\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            # print(f'Index {ei}; input size: {input_tensor[ei].size()}; encoder hidden size: {encoder_hidden.size()}')\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                input_tensor[ei], encoder_hidden\n",
    "            )\n",
    "            # print('Index', ei)\n",
    "            # print('encoder output size', encoder_output.size())\n",
    "            # print('encoder outputs size', encoder_outputs.size())\n",
    "            # print('output selection size', encoder_output[:, 0].size())\n",
    "            # print('ouput to save', encoder_outputs[:,ei].size())\n",
    "            encoder_outputs[:, ei] = encoder_output[0, 0]\n",
    "\n",
    "        # print('encoder outputs', encoder_outputs)\n",
    "        # print('encoder hidden', encoder_hidden)\n",
    "\n",
    "        # Decoder part\n",
    "        # Target = seq len x batch size\n",
    "        # Decoder input moet zijn: batch_size x 1 (van het eerste token = BOS)\n",
    "        target_length = target.size(0)\n",
    "\n",
    "        decoder_input = torch.tensor(\n",
    "            [[BOS_IDX] for _ in range(batch_size)], device=self.device\n",
    "        )\n",
    "        # print('decoder input size', decoder_input.size())\n",
    "\n",
    "        decoder_outputs = torch.zeros(\n",
    "            batch_size, self.max_length, self.decoder.output_size, device=self.device\n",
    "        )\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = (\n",
    "            True if random.random() < self.teacher_forcing_ratio else False\n",
    "        )\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            if use_teacher_forcing:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target[di, :].unsqueeze(1)  # Teacher forcing\n",
    "                # print('decoder input size:', decoder_input.size())\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()  # detach from history as input\n",
    "                # print('decoder input size:', decoder_input.size())\n",
    "\n",
    "            # print(f'Index {di}; decoder output size: {decoder_output.size()}; decoder input size: {decoder_input.size()}')\n",
    "            decoder_outputs[:, di] = decoder_output\n",
    "\n",
    "        # Zero out probabilities for padded chars\n",
    "        target_masks = (target != PAD_IDX).float()\n",
    "\n",
    "        # Compute log probability of generating true target words\n",
    "        # print('P (decoder_outputs)', decoder_outputs.size())\n",
    "        # print(target.transpose(0, 1))\n",
    "        # print('Index', target.size(), target.transpose(0, 1).unsqueeze(-1))\n",
    "        target_gold_std_log_prob = torch.gather(\n",
    "            decoder_outputs, index=target.transpose(0, 1).unsqueeze(-1), dim=-1\n",
    "        ).squeeze(-1) * target_masks.transpose(0, 1)\n",
    "        # print(target_gold_std_log_prob)\n",
    "        scores = target_gold_std_log_prob.sum(dim=1)\n",
    "\n",
    "        # print(scores)\n",
    "\n",
    "        return scores, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-24.4678, -19.0528], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 2\n",
    "hidden_size = 5\n",
    "dropout = 0.1\n",
    "max_token_len = 10\n",
    "\n",
    "model = SimpleCorrectionSeq2seq(\n",
    "    len(vocab_transform[\"ocr\"]),\n",
    "    hidden_size,\n",
    "    len(vocab_transform[\"gs\"]),\n",
    "    dropout,\n",
    "    max_token_len,\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "input = torch.tensor([[6, 4], [22, 30], [0, 6], [18, 4], [11, 3], [3, 1]])\n",
    "encoder_hidden = model.encoder.initHidden(batch_size=batch_size, device=device)\n",
    "\n",
    "target = torch.tensor([[6, 4], [23, 5], [16, 6], [16, 4], [11, 4], [3, 1]])\n",
    "\n",
    "losses, _ = model(input, encoder_hidden, target)\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCorrectionSeq2seq(\n",
       "  (encoder): EncoderRNN(\n",
       "    (embedding): Embedding(46, 5)\n",
       "    (gru): GRU(5, 5, batch_first=True)\n",
       "  )\n",
       "  (decoder): AttnDecoderRNN(\n",
       "    (embedding): Embedding(44, 5)\n",
       "    (attn): Linear(in_features=10, out_features=11, bias=True)\n",
       "    (attn_combine): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (gru): GRU(5, 5)\n",
       "    (out): Linear(in_features=5, out_features=44, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = Path(os.getcwd()) / \"data\" / \"model.rar\"\n",
    "\n",
    "checkpoint = torch.load(model_save_path)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def indices2string(indices, itos):\n",
    "    output = []\n",
    "    for idxs in indices:\n",
    "        # print(idxs)\n",
    "        string = []\n",
    "        for idx in idxs:\n",
    "            if idx not in (UNK_IDX, PAD_IDX, BOS_IDX):\n",
    "                if idx == EOS_IDX:\n",
    "                    break\n",
    "                else:\n",
    "                    string.append(itos[idx])\n",
    "        word = \"\".join(string)\n",
    "        output.append(word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor(\n",
    "    [\n",
    "        [20, 34, 22, 6, 1, 1, 1, 1, 1, 1],\n",
    "        [22, 6, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [21, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [4, 5, 6, 4, 1, 1, 1, 1, 1, 1],\n",
    "        [29, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    ]\n",
    ")\n",
    "indices2string(indices, vocab_transform[\"gs\"].get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 201.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 5, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "decoder = GreedySearchDecoder(model)\n",
    "\n",
    "max_len = 10\n",
    "\n",
    "test = SimpleCorrectionDataset(token_data.query('dataset == \"test\"'), max_len=max_len)\n",
    "test_dataloader = DataLoader(test, batch_size=5, collate_fn=collate_fn(text_transform))\n",
    "\n",
    "\n",
    "output_strings = predict_and_convert_to_str(\n",
    "    model, test_dataloader, vocab_transform[\"gs\"], device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "test_data = (\n",
    "    token_data.query('dataset == \"test\"')\n",
    "    .query(f\"len_ocr <= {max_len}\")\n",
    "    .query(f\"len_gs <= {max_len}\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "test_data[\"pred\"] = output_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measure: mean normalized edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean (normalized) edit distance.\n",
    "    - Option: ignore -\n",
    "    - option: ignore case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35.000000\n",
       "mean      1.971429\n",
       "std       1.773758\n",
       "min       1.000000\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       2.000000\n",
       "max       8.000000\n",
       "Name: ed, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"ed\"] = test_data.apply(\n",
    "    lambda row: edlib.align(row.ocr, row.gs)[\"editDistance\"], axis=1\n",
    ")\n",
    "test_data.ed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35.000000\n",
       "mean      0.390952\n",
       "std       0.326909\n",
       "min       0.100000\n",
       "25%       0.125000\n",
       "50%       0.250000\n",
       "75%       0.583333\n",
       "max       1.000000\n",
       "Name: ed_norm, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"ed_norm\"] = test_data.apply(\n",
    "    lambda row: normalized_ed(row.ed, row.ocr, row.gs), axis=1\n",
    ")\n",
    "test_data.ed_norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35.000000\n",
       "mean      9.114286\n",
       "std       2.259248\n",
       "min       2.000000\n",
       "25%       9.000000\n",
       "50%      10.000000\n",
       "75%      10.500000\n",
       "max      11.000000\n",
       "Name: ed_pred, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"ed_pred\"] = test_data.apply(\n",
    "    lambda row: edlib.align(row.pred, row.gs)[\"editDistance\"], axis=1\n",
    ")\n",
    "test_data.ed_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35.000000\n",
       "mean      0.988456\n",
       "std       0.032790\n",
       "min       0.888889\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000\n",
       "Name: ed_norm_pred, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"ed_norm_pred\"] = test_data.apply(\n",
    "    lambda row: normalized_ed(row.ed_pred, row.pred, row.gs), axis=1\n",
    ")\n",
    "test_data.ed_norm_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
