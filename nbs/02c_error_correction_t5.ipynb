{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Error correction using T5\n",
    "output-file: error_correction_t5.html\n",
    "description: Functionality for error correction with T5.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp error_correction_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ocrpostcorrection.error_correction import get_tokens_with_OCR_mistakes\n",
    "from ocrpostcorrection.icdar_data import generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1520.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>gs</th>\n",
       "      <th>ocr_aligned</th>\n",
       "      <th>gs_aligned</th>\n",
       "      <th>start</th>\n",
       "      <th>len_ocr</th>\n",
       "      <th>key</th>\n",
       "      <th>language</th>\n",
       "      <th>subset</th>\n",
       "      <th>dataset</th>\n",
       "      <th>len_gs</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td></td>\n",
       "      <td>In</td>\n",
       "      <td>##</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>en/eng_sample/1.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>troe</td>\n",
       "      <td>tree</td>\n",
       "      <td>troe</td>\n",
       "      <td>tree</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>en/eng_sample/1.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peremial</td>\n",
       "      <td>perennial</td>\n",
       "      <td>perem@ial</td>\n",
       "      <td>perennial</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>en/eng_sample/1.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eLngated</td>\n",
       "      <td>elongated</td>\n",
       "      <td>eL@ngated</td>\n",
       "      <td>elongated</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>en/eng_sample/1.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stein,</td>\n",
       "      <td>stem,</td>\n",
       "      <td>stein,</td>\n",
       "      <td>stem@,</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>en/eng_sample/1.txt</td>\n",
       "      <td>en</td>\n",
       "      <td>eng_sample</td>\n",
       "      <td>test</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ocr         gs ocr_aligned gs_aligned  ...      subset  dataset len_gs diff\n",
       "0        In                     In         ##  ...  eng_sample     test      0    2\n",
       "1      troe       tree        troe       tree  ...  eng_sample     test      4    0\n",
       "2  peremial  perennial   perem@ial  perennial  ...  eng_sample     test      9   -1\n",
       "3  eLngated  elongated   eL@ngated  elongated  ...  eng_sample     test      9   -1\n",
       "4    stein,      stem,      stein,     stem@,  ...  eng_sample     test      5    1\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "data_dir = Path(os.getcwd()) / \"data\" / \"dataset_training_sample\"\n",
    "\n",
    "data, md = generate_data(data_dir)\n",
    "\n",
    "val_files = ['fr/fr_sample/2.txt']\n",
    "\n",
    "tdata = get_tokens_with_OCR_mistakes(data, data, val_files)\n",
    "tdata.drop_duplicates(subset=[\"ocr\", \"gs\", \"dataset\"], inplace=True)\n",
    "tdata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(tdata.shape)\n",
    "tdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ocr': 'troe',\n",
       " 'gs': 'tree',\n",
       " 'ocr_aligned': 'troe',\n",
       " 'gs_aligned': 'tree',\n",
       " 'start': 13,\n",
       " 'len_ocr': 4,\n",
       " 'key': 'en/eng_sample/1.txt',\n",
       " 'language': 'en',\n",
       " 'subset': 'eng_sample',\n",
       " 'dataset': 'train',\n",
       " 'len_gs': 4,\n",
       " 'diff': 0,\n",
       " '__index_level_0__': 31}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": Dataset.from_pandas(tdata.query('dataset == \"train\"')),\n",
    "            \"val\": Dataset.from_pandas(tdata.query('dataset == \"val\"')),\n",
    "            \"test\": Dataset.from_pandas(tdata.query('dataset == \"test\"')),\n",
    "        }\n",
    "    )\n",
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def filter_max_len(example: Dict, max_len: int):\n",
    "    if example[\"len_ocr\"] <= max_len and example[\"len_gs\"] <= max_len:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/byt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def preprocess_function(examples, tokenizer, add_task_prefix: bool=False):\n",
    "    input = examples[\"ocr\"]\n",
    "    if add_task_prefix:\n",
    "        input = [f\"{language}: {ocr_str}\" for ocr_str, language in zip(examples[\"ocr\"], examples['language'])]\n",
    "\n",
    "    model_inputs = tokenizer(input)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"gs\"])\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048d793791a24184a1a05e15af903d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6bc19aac144a6cbf116b6816d02101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0932b6def7490eb1e2c3bf084cb272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, fn_kwargs={\"tokenizer\": tokenizer}, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'troe</s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccd473ba91b47a6a79d408449adc60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc050230aded4a25bda31a4b75777f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae122add24614a86aa83413af9e548a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, fn_kwargs={\"tokenizer\": tokenizer, \"add_task_prefix\": True}, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en: troe</s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
