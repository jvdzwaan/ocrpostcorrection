{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/mntDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mntDrive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('/mntDrive/MyDrive/icdar-dataset-20220207')\n",
    "\n",
    "#out_dir = Path('icdar-dataset-20220207')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(out_dir/'task2_train.csv', index_col=0)\n",
    "val = pd.read_csv(out_dir/'task2_val.csv', index_col=0)\n",
    "test = pd.read_csv(out_dir/'task2_test.csv', index_col=0)\n",
    "\n",
    "train = train.fillna('')\n",
    "val = val.fillna('')\n",
    "test = test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(out_dir/'task2_train.jsonl', orient='records', lines=True)\n",
    "val.to_json(out_dir/'task2_val.jsonl', orient='records', lines=True)\n",
    "test.to_json(out_dir/'task2_test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {'train': str(out_dir/'task2_train.jsonl'),\n",
    "              'val': str(out_dir/'task2_val.jsonl'),\n",
    "              'test': str(out_dir/'task2_test.jsonl')}\n",
    "\n",
    "icdar_dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset['train'][6352]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mntDrive/MyDrive/results-0.3-20220207-no-checkpoints'\n",
    "#model_dir = '/Users/janneke/models/results-0.3-20220207'\n",
    "model_name = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(icdar_dataset['train'][6352]['ocr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_icdar = icdar_dataset.map(lambda sample: tokenizer(sample['ocr'], truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_icdar = tokenized_icdar.remove_columns(['gs', 'ocr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(model_dir).is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoModel, AutoModelForTokenClassification, AutoModelWithLMHead, AutoModelForPreTraining, Trainer, TrainingArguments\n",
    "\n",
    "model = BertModel.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_intermediary_data(dataset, batch_size=8):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=collator\n",
    "    )\n",
    "\n",
    "    out_path = out_dir/'task1_output'/str(dataset.split)\n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            #print(i)\n",
    "            batch.to(device=device)\n",
    "            #print(batch)\n",
    "            \n",
    "            output = model(**batch)\n",
    "            #print(output['pooler_output'].size())\n",
    "\n",
    "            samples = output['pooler_output'].detach().cpu()\n",
    "            out_file = out_path/f'task2_task1_output_{i}.pt'\n",
    "            torch.save(samples, out_file)\n",
    "            # with open(out_path/out_file, 'w') as f:\n",
    "            #   for sample in samples:\n",
    "            #       f.write(json.dumps({'task1_output': sample.tolist()}))\n",
    "            #       f.write('\\n')\n",
    "            del samples\n",
    "            del output\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "for split_name in ('test',):\n",
    "    create_intermediary_data(tokenized_icdar[split_name], batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
