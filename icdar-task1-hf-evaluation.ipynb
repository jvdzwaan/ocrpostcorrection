{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGURU_LEVEL'] = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "class InterceptHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        # Get corresponding Loguru level if it exists\n",
    "        try:\n",
    "            level = logger.level(record.levelname).name\n",
    "        except ValueError:\n",
    "            level = record.levelno\n",
    "\n",
    "        # Find caller from where originated the logged message\n",
    "        frame, depth = logging.currentframe(), 2\n",
    "        while frame.f_code.co_filename == logging.__file__:\n",
    "            frame = frame.f_back\n",
    "            depth += 1\n",
    "\n",
    "        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n",
    "\n",
    "logging.basicConfig(handlers=[InterceptHandler()], level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "icdar_dataset = load_from_disk('icdar-0.3')\n",
    "icdar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/Users/janneke/models/results-0.3'\n",
    "model_name = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://huggingface.co/docs/transformers/custom_datasets#token-classification-with-wnut-emerging-entities\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                            # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:              # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_icdar = icdar_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-0.3',          # output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_dir, num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=tokenized_icdar['train'],         # training dataset\n",
    "    eval_dataset=tokenized_icdar['val'],            # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(tokenized_icdar['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import generate_data\n",
    "\n",
    "in_dir = Path('../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_evaluation_4M_without_Finnish')\n",
    "data_test, X_test = generate_data(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import generate_sentences\n",
    "\n",
    "test_data = generate_sentences(X_test, data_test, size=35, step=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def convert_predictions(samples, pred):\n",
    "    #print('samples', len(samples))\n",
    "    #print(samples)\n",
    "    #print(samples[0].keys())\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys()) \n",
    "\n",
    "    tokenized_samples = tokenizer(samples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    print(type(tokenized_samples))\n",
    "    #print(samples)\n",
    "\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys())    \n",
    "    # convert predictions to labels (label_ids)\n",
    "    p = np.argmax(pred.predictions, axis=2)\n",
    "    #print(p)\n",
    "\n",
    "    converted = defaultdict(dict)\n",
    "\n",
    "    for i, (sample, preds) in enumerate(zip(samples, p)):\n",
    "        #print(sample.keys())\n",
    "        #label = sample['tags']\n",
    "        #print(label)\n",
    "        #print(len(preds), preds)\n",
    "        word_ids = tokenized_samples.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        #print(len(word_ids), word_ids)\n",
    "        result = defaultdict(list)\n",
    "        for word_idx, p_label in zip(word_ids, preds):\n",
    "            #print(word_idx, p_label)\n",
    "            if word_idx is not None:\n",
    "                result[word_idx].append(p_label)\n",
    "        \n",
    "        new_tags = []\n",
    "        for word_idx, preds in result.items():\n",
    "            new_tag = 0\n",
    "            c = Counter(preds)\n",
    "            #print(c)\n",
    "            if c[1] > 0 and c[1] >= c[2]:\n",
    "                new_tag = 1\n",
    "            elif c[2] > 0 and c[2] >= c[1]:\n",
    "                new_tag = 2\n",
    "            \n",
    "            new_tags.append(new_tag)\n",
    "\n",
    "        #print('pred', len(new_tags), new_tags)\n",
    "        #print('tags', len(label), label)\n",
    "        \n",
    "        #print(sample)\n",
    "        #print(sample['key'], sample['start_token_id'])\n",
    "        converted[sample['key']][sample['start_token_id']] = new_tags\n",
    "\n",
    "    return converted\n",
    "\n",
    "\n",
    "result = convert_predictions(icdar_dataset['test'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('condensed_predictions_task1.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('condensed_predictions_task1.json', 'r') as f:\n",
    "    result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_icdar_output(label_str, input_tokens):\n",
    "    text_output = {}\n",
    "\n",
    "    # Correct use of 2 (always following a 1)\n",
    "    regex = r'12*'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group())\n",
    "        idx = input_tokens[match.start()].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "\n",
    "    # Incorrect use of 2 (following a 0) -> interpret first 2 as 1\n",
    "    regex = r'02+'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group()) - 1\n",
    "        idx = input_tokens[match.start()+1].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "#label_str = '12200010011120020222'\n",
    "#output = extract_icdar_output(label_str, data['DE/DE3/1988.txt'].input_tokens)\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output = {}\n",
    "\n",
    "for key, preds in result.items():\n",
    "    labels = defaultdict(list)\n",
    "    #print(key)\n",
    "    try:\n",
    "        text = data_test[key]\n",
    "        #print(len(text.input_tokens))\n",
    "        #print(preds)\n",
    "        for start, lbls in preds.items():\n",
    "            for i, label in enumerate(lbls):\n",
    "                labels[int(start)+i].append(label)\n",
    "        #print('LABELS')\n",
    "        #print(labels)\n",
    "\n",
    "        label_str = []\n",
    "\n",
    "        for i, token in enumerate(text.input_tokens):\n",
    "            #print(i, token, labels[i])\n",
    "            if 2 in labels[i]:\n",
    "                label_str.append('2')\n",
    "            elif 1 in labels[i]:\n",
    "                label_str.append('1')\n",
    "            else:\n",
    "                label_str.append('0')\n",
    "        label_str = ''.join(label_str)\n",
    "\n",
    "        #print('LABEL STR')\n",
    "        #print(label_str)\n",
    "\n",
    "        output[key] = extract_icdar_output(label_str, text.input_tokens)\n",
    "    except KeyError:\n",
    "        logger.warning(f'No data found for text {key}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('results_task1.json', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evalTool_ICDAR2017.py ../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_evaluation_4M_without_Finnish results_task1.json results_task1.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
