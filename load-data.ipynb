{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGURU_LEVEL'] = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "class InterceptHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        # Get corresponding Loguru level if it exists\n",
    "        try:\n",
    "            level = logger.level(record.levelname).name\n",
    "        except ValueError:\n",
    "            level = record.levelno\n",
    "\n",
    "        # Find caller from where originated the logged message\n",
    "        frame, depth = logging.currentframe(), 2\n",
    "        while frame.f_code.co_filename == logging.__file__:\n",
    "            frame = frame.f_back\n",
    "            depth += 1\n",
    "\n",
    "        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n",
    "\n",
    "logging.basicConfig(handlers=[InterceptHandler()], level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = Path('../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_training_18M_without_Finnish')\n",
    "in_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label_and_nl(line):\n",
    "    return line.strip()[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import edlib\n",
    "\n",
    "def normalized_ed(ed, ocr, gs):\n",
    "    score = 0.0\n",
    "    l = max(len(ocr), len(gs))\n",
    "    if l > 0:\n",
    "        score = ed / l\n",
    "    return score\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AlignedToken:\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    ocr_aligned: str\n",
    "    gs_aligned: str\n",
    "    start: int\n",
    "    len_ocr: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputToken:\n",
    "    ocr: str\n",
    "    gs: str\n",
    "    start: int\n",
    "    label: int\n",
    "\n",
    "\n",
    "def get_input_tokens(aligned_token):\n",
    "    if aligned_token.ocr == aligned_token.gs:\n",
    "            yield InputToken(aligned_token.ocr, aligned_token.gs, aligned_token.start, 0)\n",
    "    else:\n",
    "        parts = aligned_token.ocr.split(' ')\n",
    "        new_start = aligned_token.start\n",
    "        for i, part in enumerate(parts):\n",
    "            if i == 0:\n",
    "                yield InputToken(part, aligned_token.gs, aligned_token.start, 1)\n",
    "            else:\n",
    "                yield InputToken(part, '', new_start, 2)\n",
    "            new_start += len(part) + 1\n",
    "\n",
    "\n",
    "def tokenize_aligned(ocr_aligned, gs_aligned):\n",
    "\n",
    "    ocr_cursor = 0\n",
    "    start = 0\n",
    "\n",
    "    ocr_token_chars = []\n",
    "    gs_token_chars = []\n",
    "    ocr_token_chars_aligned = []\n",
    "    gs_token_chars_aligned = []\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for ocr_aligned_char, gs_aligned_char in zip(ocr_aligned, gs_aligned):\n",
    "        #print(ocr_aligned_char, gs_aligned_char, ocr_cursor)\n",
    "        if ocr_aligned_char != '@' and ocr_aligned_char != '#':\n",
    "            ocr_cursor += 1\n",
    "\n",
    "        if ocr_aligned_char == ' ' and gs_aligned_char == ' ':\n",
    "            #print('TOKEN')\n",
    "            #print('OCR:', repr(''.join(ocr_token_chars)))\n",
    "            #print(' GS:', repr(''.join(gs_token_chars)))\n",
    "            #print('start:', start_char)\n",
    "            #ocr_cursor += 1\n",
    "\n",
    "            # Ignore 'tokens' without representation in the ocr text \n",
    "            # (these tokens do not consist of characters) \n",
    "            ocr = (''.join(ocr_token_chars)).strip()\n",
    "            if ocr != '':\n",
    "                tokens.append(AlignedToken(ocr, \n",
    "                                          ''.join(gs_token_chars), \n",
    "                                          ''.join(ocr_token_chars_aligned), \n",
    "                                          ''.join(gs_token_chars_aligned),\n",
    "                                          start,\n",
    "                                          len(''.join(ocr_token_chars))))\n",
    "            start = ocr_cursor\n",
    "\n",
    "            ocr_token_chars = []\n",
    "            gs_token_chars = []\n",
    "            ocr_token_chars_aligned = []\n",
    "            gs_token_chars_aligned = []\n",
    "        else:\n",
    "            ocr_token_chars_aligned.append(ocr_aligned_char)\n",
    "            gs_token_chars_aligned.append(gs_aligned_char)\n",
    "            if ocr_aligned_char != '@' and ocr_aligned_char != '#':\n",
    "                ocr_token_chars.append(ocr_aligned_char)\n",
    "            if gs_aligned_char != '@' and gs_aligned_char != '#':\n",
    "                gs_token_chars.append(gs_aligned_char)\n",
    "    \n",
    "    ocr = (''.join(ocr_token_chars)).strip()\n",
    "    if ocr != '':\n",
    "        tokens.append(AlignedToken(ocr, \n",
    "                                   ''.join(gs_token_chars), \n",
    "                                   ''.join(ocr_token_chars_aligned), \n",
    "                                   ''.join(gs_token_chars_aligned),\n",
    "                                   start,\n",
    "                                   len(''.join(ocr_token_chars))))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('Long ow.', 'Longhow.', 'Long ow.', 'Longhow.', 24, 8)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['Long', 'ow.']\n",
    "assert labels == [1, 2]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('INEVR', 'I NEVER', 'I@NEV@R', 'I NEVER', 0, 5)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['INEVR']\n",
    "assert labels == [1]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AlignedToken('Major', 'Major', 'Major', 'Major', 19, 5)\n",
    "print(t)\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "gs = []\n",
    "\n",
    "for inp_tok in get_input_tokens(t):\n",
    "    print(inp_tok)\n",
    "    tokens.append(inp_tok.ocr)\n",
    "    labels.append(inp_tok.label)\n",
    "    gs.append(inp_tok.gs)\n",
    "\n",
    "assert tokens == ['Major']\n",
    "assert labels == [0]\n",
    "assert ''.join(gs) == t.gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(iterable, size=2):\n",
    "    i = iter(iterable)\n",
    "    win = []\n",
    "    for e in range(0, size):\n",
    "        try:\n",
    "            win.append(next(i))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    yield win\n",
    "    for e in i:\n",
    "        win = win[1:] + [e]\n",
    "        yield win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import edlib\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    tokens: list\n",
    "    input_tokens: list\n",
    "    score: float\n",
    "\n",
    "\n",
    "def clean(string):\n",
    "    string = string.replace('@', '')\n",
    "    string = string.replace('#', '')\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "def process_text(in_file, size=15, overlap=10):\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    ocr_input = clean(remove_label_and_nl(lines[0]))\n",
    "    ocr_aligned = remove_label_and_nl(lines[1])\n",
    "    gs_aligned = remove_label_and_nl(lines[2])\n",
    "\n",
    "    #print('ocr input:', ocr_input)\n",
    "    #print('ocr aligned:', ocr_aligned)\n",
    "    #print('gs aligned:',gs_aligned)\n",
    "\n",
    "    tokens = tokenize_aligned(ocr_aligned, gs_aligned)\n",
    "\n",
    "    # Check data\n",
    "    for token in tokens:\n",
    "        input_token = ocr_input[token.start:token.start+token.len_ocr]\n",
    "        try:\n",
    "            assert token.ocr == input_token.strip()\n",
    "        except AssertionError:\n",
    "            print(f'Text: {str(in_file)}; ocr: {repr(token.ocr)}; ocr_input: {repr(input_token)}')\n",
    "            raise\n",
    "\n",
    "    ocr = clean(ocr_aligned)\n",
    "    gs = clean(gs_aligned)\n",
    "    ed = edlib.align(gs, ocr)['editDistance']\n",
    "    score = normalized_ed(ed, ocr, gs)\n",
    "\n",
    "    input_tokens = []\n",
    "    for token in tokens:\n",
    "        for inp_tok in get_input_tokens(token):\n",
    "            input_tokens.append(inp_tok)\n",
    "    \n",
    "    return Text(tokens, input_tokens, score)\n",
    "\n",
    "text = process_text(in_dir/'NL'/'NL1'/'17.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.tokens[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.input_tokens[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "in_dir = Path('../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_training_18M_without_Finnish')\n",
    "\n",
    "data = {}\n",
    "\n",
    "subsets = []\n",
    "file_languages = []\n",
    "file_names = []\n",
    "scores = []\n",
    "num_tokens = []\n",
    "num_input_tokens = []\n",
    "\n",
    "for language_dir in tqdm(in_dir.iterdir()):\n",
    "    #print(language_dir.stem)\n",
    "    language = language_dir.stem\n",
    "    \n",
    "    for text_file in language_dir.rglob('*.txt'):\n",
    "        #print(text_file)\n",
    "        #print(text_file.relative_to(in_dir))\n",
    "        key = str(text_file.relative_to(in_dir))\n",
    "        data[key] = process_text(text_file)\n",
    "\n",
    "        file_languages.append(language)\n",
    "        file_names.append(key)\n",
    "        scores.append(data[key].score)\n",
    "        num_tokens.append(len(data[key].tokens))\n",
    "        num_input_tokens.append(len(data[key].input_tokens))\n",
    "md = pd.DataFrame({'language': file_languages, \n",
    "                   'file_name': file_names,\n",
    "                   'score': scores,\n",
    "                   'num_tokens': num_tokens,\n",
    "                   'num_input_tokens': num_input_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.num_tokens.hist(bins=2000, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.num_input_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.num_input_tokens.hist(bins=2000, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.score.hist(bins=50, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.query('score <= 0.3').num_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, _, _ = train_test_split(md, md['file_name'], test_size=0.1, \n",
    "                                        shuffle=True, stratify=md['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path('results')\n",
    "\n",
    "X_train.to_csv(out_dir/'train.csv')\n",
    "X_val.to_csv(out_dir/'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path('results')\n",
    "\n",
    "X_train = pd.read_csv(out_dir/'train.csv')\n",
    "X_val = pd.read_csv(out_dir/'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'sentences' for train and val sets\n",
    "\n",
    "def generate_sentences(df, data, size=15, overlap=10):\n",
    "    sents = []\n",
    "    labels = []\n",
    "    keys = []\n",
    "    start_tokens = []\n",
    "    scores = []\n",
    "    languages = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        key = row.file_name\n",
    "        tokens = data[key].input_tokens\n",
    "\n",
    "        # print(len(tokens))\n",
    "        # print(key)\n",
    "        for i, res in enumerate(window(tokens, size=size)):\n",
    "            if i % overlap == 0:\n",
    "                ocr = [t.ocr for t in res]\n",
    "                lbls = [t.label for t in res]\n",
    "                gs = []\n",
    "                for t in res:\n",
    "                    if t.gs != '':\n",
    "                        gs.append(t.gs)\n",
    "                ocr_str = ' '.join(ocr)\n",
    "                gs_str = ' '.join(gs)\n",
    "                ed = edlib.align(ocr_str, gs_str)['editDistance']\n",
    "                score = normalized_ed(ed, ocr_str, gs_str)\n",
    "\n",
    "                if len(ocr_str) > 0:\n",
    "\n",
    "                    sents.append(ocr)\n",
    "                    labels.append(lbls)\n",
    "                    keys.append(key)\n",
    "                    start_tokens.append(i)\n",
    "                    scores.append(score)\n",
    "                    languages.append(key[:2])\n",
    "                else:\n",
    "                    logger.info(f'Empty sample for text \"{key}\"')\n",
    "                    logger.info(f'ocr_str: {ocr_str}')\n",
    "                    logger.info(f'start token: {i}')\n",
    "    data = pd.DataFrame({\n",
    "        'key': keys,\n",
    "        'start_token_id': start_tokens,\n",
    "        'score': scores,\n",
    "        'tokens': sents,\n",
    "        'tags': labels,\n",
    "        'language': languages\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = generate_sentences(X_train, data, size=25)\n",
    "val_data = generate_sentences(X_val, data, size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_json(out_dir/'icdar_train.jsonl', orient='records', lines=True)\n",
    "val_data.to_json(out_dir/'icdar_val.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "out_dir = Path('results')\n",
    "\n",
    "data_files = {'train': str(out_dir/'icdar_train.jsonl'),\n",
    "              'val': str(out_dir/'icdar_val.jsonl')}\n",
    "\n",
    "icdar_dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.score.hist(bins=50, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.score.hist(bins=50, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset = icdar_dataset.filter(lambda sample: sample['score'] <= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(icdar_dataset['val']):\n",
    "    if sample == {}:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset.save_to_disk('icdar-0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "icdar_dataset = load_from_disk('icdar-0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://huggingface.co/docs/transformers/custom_datasets#token-classification-with-wnut-emerging-entities\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:                            # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:              # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_icdar = icdar_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_tokenized = tokenized_icdar['train'].select(range(5))\n",
    "val_samples_tokenized = tokenized_icdar['val'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-0.3',          # output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_samples_tokenized,         # training dataset\n",
    "    eval_dataset=val_samples_tokenized,            # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(icdar_dataset['val']):\n",
    "    if sample == {}:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = icdar_dataset['val'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = samples.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(tokenized_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def convert_predictions(samples, pred):\n",
    "    print('samples', len(samples))\n",
    "    #print(samples)\n",
    "    #print(samples[0].keys())\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys()) \n",
    "\n",
    "    tokenized_samples = tokenizer(samples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    print(type(tokenized_samples))\n",
    "    #print(samples)\n",
    "\n",
    "    #for sample in samples:\n",
    "    #    print(sample.keys())    \n",
    "    # convert predictions to labels (label_ids)\n",
    "    p = np.argmax(pred.predictions, axis=2)\n",
    "    print(p)\n",
    "\n",
    "    converted = defaultdict(dict)\n",
    "\n",
    "    for i, (sample, preds) in enumerate(zip(samples, p)):\n",
    "        print(sample.keys())\n",
    "        #label = sample['tags']\n",
    "        #print(label)\n",
    "        print(len(preds), preds)\n",
    "        word_ids = tokenized_samples.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        print(len(word_ids), word_ids)\n",
    "        result = defaultdict(list)\n",
    "        for word_idx, p_label in zip(word_ids, preds):\n",
    "            print(word_idx, p_label)\n",
    "            if word_idx is not None:\n",
    "                result[word_idx].append(p_label)\n",
    "        \n",
    "        new_tags = []\n",
    "        for word_idx, preds in result.items():\n",
    "            new_tag = 0\n",
    "            c = Counter(preds)\n",
    "            print(c)\n",
    "            if c[1] > 0 and c[1] >= c[2]:\n",
    "                new_tag = 1\n",
    "            elif c[2] > 0 and c[2] >= c[1]:\n",
    "                new_tag = 2\n",
    "            \n",
    "            new_tags.append(new_tag)\n",
    "\n",
    "        print('pred', len(new_tags), new_tags)\n",
    "        #print('tags', len(label), label)\n",
    "        \n",
    "        print(sample)\n",
    "        print(sample['key'], sample['start_token_id'])\n",
    "        converted[sample['key']][sample['start_token_id']] = new_tags\n",
    "\n",
    "    return converted\n",
    "\n",
    "\n",
    "result = convert_predictions(samples, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_icdar_output(label_str, input_tokens):\n",
    "    text_output = {}\n",
    "\n",
    "    # Correct use of 2 (always following a 1)\n",
    "    regex = r'12*'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group())\n",
    "        idx = input_tokens[match.start()].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "\n",
    "    # Incorrect use of 2 (following a 0) -> interpret first 2 as 1\n",
    "    regex = r'02+'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group()) - 1\n",
    "        idx = input_tokens[match.start()+1].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "label_str = '12200010011120020222'\n",
    "output = extract_icdar_output(label_str, data['DE/DE3/1988.txt'].input_tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for key, preds in result.items():\n",
    "    labels = defaultdict(list)\n",
    "    #print(key)\n",
    "    text = data[key]\n",
    "    #print(len(text.input_tokens))\n",
    "    #print(preds)\n",
    "    for start, lbls in preds.items():\n",
    "        for i, label in enumerate(lbls):\n",
    "            labels[start+i].append(label)\n",
    "    #print('LABELS')\n",
    "    #print(labels)\n",
    "\n",
    "    label_str = []\n",
    "\n",
    "    for i, token in enumerate(text.input_tokens):\n",
    "        #print(i, token, labels[i])\n",
    "        if 2 in labels[i]:\n",
    "            label_str.append('2')\n",
    "        elif 1 in labels[i]:\n",
    "            label_str.append('1')\n",
    "        else:\n",
    "            label_str.append('0')\n",
    "    label_str = ''.join(label_str)\n",
    "\n",
    "    #print('LABEL STR')\n",
    "    #print(label_str)\n",
    "\n",
    "    output[key] = extract_icdar_output(label_str, text.input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
