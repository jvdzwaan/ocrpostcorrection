{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/mntDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = Path('/mntDrive/MyDrive/icdar-dataset-20220207')\n",
    "#in_dir = Path('icdar-dataset-20220207')\n",
    "\n",
    "train = pd.read_csv(in_dir/'task2_train.csv', index_col=0)\n",
    "val = pd.read_csv(in_dir/'task2_val.csv', index_col=0)\n",
    "\n",
    "train = train.fillna('')\n",
    "val = val.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:', train.shape[0], 'samples')\n",
    "print('val:', val.shape[0], 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lens(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data['len_ocr'] = data['ocr'].apply(lambda x: len(x))\n",
    "    data['len_gs'] = data['gs'].apply(lambda x: len(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "train = add_lens(train)\n",
    "val = add_lens(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Task2Dataset(Dataset):\n",
    "    def __init__(self, data, task1_data_dir, batch_size, max_len=11):\n",
    "        self.ds = data.query(f'len_ocr < {max_len}').query(f'len_gs < {max_len}').copy()\n",
    "        self.ds = self.ds.reset_index(drop=False)\n",
    "\n",
    "        self.task1_data_dir = task1_data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.vectors_loaded = {}\n",
    "        for in_file in tqdm(Path(self.task1_data_dir).iterdir()):\n",
    "          if in_file.is_file():\n",
    "            self.vectors_loaded[in_file] = torch.load(in_file)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds.loc[idx]\n",
    "        original_idx = sample['index']\n",
    "        #print('original idx', original_idx)\n",
    "\n",
    "        file_index = original_idx // self.batch_size\n",
    "        index_in_file = original_idx % self.batch_size\n",
    "        in_file = self.task1_data_dir/f'task2_task1_output_{file_index}.pt'\n",
    "        #print('original idx', original_idx, 'file index', file_index, 'index in file', index_in_file, in_file)\n",
    "        if in_file not in self.vectors_loaded.keys():\n",
    "            self.vectors_loaded[in_file] = torch.load(in_file)\n",
    "        # Copy the task1_ouput slice, so we have a new tensor\n",
    "        task1_output = self.vectors_loaded[in_file][index_in_file].clone().detach().requires_grad_(True)\n",
    "\n",
    "        return [char for char in sample.ocr], [char for char in sample.gs], task1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#out_dir = Path('icdar-dataset-20220207')\n",
    "out_dir = Path('/mntDrive/MyDrive/icdar-dataset-20220207')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab_transform = {}\n",
    "\n",
    "def yield_tokens(data, col):\n",
    "    for token in data[col].to_list():\n",
    "        for char in token:\n",
    "            yield char\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for name in ('ocr', 'gs'):\n",
    "    \n",
    "    vocab_transform[name] = build_vocab_from_iterator(yield_tokens(train, name),\n",
    "                                                      min_freq=1,\n",
    "                                                      specials=special_symbols,\n",
    "                                                      special_first=True)\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for name in ('ocr', 'gs'):\n",
    "  vocab_transform[name].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for name in ('ocr', 'gs'):\n",
    "    text_transform[name] = sequential_transforms(vocab_transform[name],  # Numericalization (char -> idx)\n",
    "                                                 tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch, input_hidden_batch = [], [], []\n",
    "    for src_sample, tgt_sample, input_hidden in batch:\n",
    "        src_batch.append(text_transform['ocr'](src_sample))\n",
    "        tgt_batch.append(text_transform['gs'](tgt_sample))\n",
    "        input_hidden_batch.append(input_hidden)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    input_hidden_batch = torch.stack(input_hidden_batch)\n",
    "    return src_batch.to(torch.int64), tgt_batch.to(torch.int64), torch.unsqueeze(input_hidden_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # print('Encoder')\n",
    "        # print('input size', input.size())\n",
    "        # print('hidden size', hidden.size())\n",
    "        embedded = self.embedding(input) \n",
    "        # print('embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "        # print('embedded size met view', embedded.view(1, 1, -1).size())\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # print('embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "        embedded = torch.permute(embedded, (1, 0, 2))\n",
    "        # print('permuted embedded size', embedded.size())\n",
    "        # print(embedded)\n",
    "\n",
    "        # print('hidden size', hidden.size())\n",
    "        # print(hidden)\n",
    "\n",
    "        # print('permuted embedded[0] size', embedded[0].size())\n",
    "        # print('hidden[0] size', hidden[0].size())\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "\n",
    "        # print('attn_weights', attn_weights.size())\n",
    "        # print('attn_weights unsqueeze(1)', attn_weights.unsqueeze(1).size())\n",
    "        # print('encoder outputs', encoder_outputs.size())\n",
    "\n",
    "\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs)\n",
    "\n",
    "        # print('attn_applied', attn_applied.size())\n",
    "        # print('attn_applied squeeze', attn_applied.squeeze().size())\n",
    "        output = torch.cat((embedded[0], attn_applied.squeeze()), 1)\n",
    "        # print('output', output.size())\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # print('output', output.size())\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        # print(f'output: {output.size()}; hidden: {hidden.size()}; attn_weigts: {attn_weights.size()}')\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICDARTask2Seq2seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout, max_length, teacher_forcing_ratio):\n",
    "        super(ICDARTask2Seq2seq, self).__init__()\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_size, \n",
    "                                      dropout_p=dropout, max_length=max_length)\n",
    "    \n",
    "    def forward(self, input, encoder_hidden, target, max_length):\n",
    "        # input is src seq len x batch size\n",
    "        # input voor de encoder (1 stap) moet zijn input seq len x batch size x 1\n",
    "        input_tensor = input.unsqueeze(2)\n",
    "        # print('input tensor size', input_tensor.size())\n",
    "\n",
    "        input_length = input.size(0)\n",
    "\n",
    "        batch_size = input.size(1)\n",
    "\n",
    "        # Encoder part\n",
    "        encoder_outputs = torch.zeros(batch_size, max_length, self.encoder.hidden_size, \n",
    "                                      device=device)\n",
    "        # print('encoder outputs size', encoder_outputs.size())\n",
    "    \n",
    "        for ei in range(input_length):\n",
    "            # print(f'Index {ei}; input size: {input_tensor[ei].size()}; encoder hidden size: {encoder_hidden.size()}')\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                input_tensor[ei], encoder_hidden)\n",
    "            # print('Index', ei)\n",
    "            # print('encoder output size', encoder_output.size())\n",
    "            # print('encoder outputs size', encoder_outputs.size())\n",
    "            # print('output selection size', encoder_output[:, 0].size())\n",
    "            # print('ouput to save', encoder_outputs[:,ei].size())\n",
    "            encoder_outputs[:, ei] = encoder_output[0, 0]\n",
    "        \n",
    "        # print('encoder outputs', encoder_outputs)\n",
    "        # print('encoder hidden', encoder_hidden)\n",
    "\n",
    "        # Decoder part\n",
    "        # Target = seq len x batch size\n",
    "        # Decoder input moet zijn: batch_size x 1 (van het eerste token = BOS)\n",
    "        target_length = target.size(0)\n",
    "\n",
    "        decoder_input = torch.tensor([[BOS_IDX] for _ in range(batch_size)], device=device)\n",
    "        # print('decoder input size', decoder_input.size())\n",
    "\n",
    "        decoder_outputs = torch.zeros(batch_size, max_length, self.decoder.output_size, \n",
    "                                      device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            if use_teacher_forcing:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target[di, :].unsqueeze(1)  # Teacher forcing\n",
    "                #print('decoder input size:', decoder_input.size())\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()  # detach from history as input\n",
    "                #print('decoder input size:', decoder_input.size())\n",
    "\n",
    "            # print(f'Index {di}; decoder output size: {decoder_output.size()}; decoder input size: {decoder_input.size()}')\n",
    "            decoder_outputs[:, di] = decoder_output\n",
    "\n",
    "        # Zero out probabilities for padded chars\n",
    "        target_masks = (target != PAD_IDX).float()\n",
    "\n",
    "        # Compute log probability of generating true target words\n",
    "        # print('P (decoder_outputs)', decoder_outputs.size())\n",
    "        # print(target.transpose(0, 1))\n",
    "        # print('Index', target.size(), target.transpose(0, 1).unsqueeze(-1))\n",
    "        target_gold_std_log_prob = torch.gather(decoder_outputs, index=target.transpose(0, 1).unsqueeze(-1), dim=-1).squeeze(-1) * target_masks.transpose(0, 1)\n",
    "        #print(target_gold_std_log_prob)\n",
    "        scores = target_gold_std_log_prob.sum(dim=1)\n",
    "\n",
    "        #print(scores)\n",
    "\n",
    "        return scores, encoder_outputs\n",
    "\n",
    "\n",
    "# batch_size = 2\n",
    "# hidden_size = 768\n",
    "# model = ICDARTask2Seq2seq(len(vocab_transform['ocr']), \n",
    "#                           hidden_size, \n",
    "#                           len(vocab_transform['gs']), \n",
    "#                           0.1, \n",
    "#                           11, \n",
    "#                           teacher_forcing_ratio=1)\n",
    "\n",
    "# input = torch.tensor([[  7,  50],\n",
    "#         [  4,   9],\n",
    "#         [  3, 171],\n",
    "#         [  1,  70],\n",
    "#         [  1,  41],\n",
    "#         [  1,   3]])\n",
    "# # Er moet een initiele hidden vector gemaakt worden voor elk sample in de batch\n",
    "# # De size is dus batch_size x encoder_hidden_size\n",
    "# encoder_hidden = model.encoder.initHidden(batch_size=batch_size)\n",
    "\n",
    "# target = torch.tensor([[47, 42],\n",
    "#         [ 4, 15],\n",
    "#         [ 3, 18],\n",
    "#         [ 1,  3]])\n",
    "\n",
    "# model(input, encoder_hidden, target, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import edlib\n",
    "\n",
    "def calculate_ed(src_strings, tgt_strings):\n",
    "    dist = []\n",
    "\n",
    "    for src, tgt in zip(src_strings, tgt_strings):\n",
    "        #print(repr(src), repr(tgt))\n",
    "        ed = edlib.align(src, tgt)\n",
    "        dist.append(ed['editDistance'])\n",
    "\n",
    "    return np.mean(dist), np.std(dist)\n",
    "\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 11\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "def indices2string(indices, itos):\n",
    "    output = []\n",
    "    for idxs in indices:\n",
    "        #print(idxs)\n",
    "        string = []\n",
    "        for idx in idxs:\n",
    "            if idx not in (UNK_IDX, PAD_IDX, BOS_IDX):\n",
    "                if idx == EOS_IDX:\n",
    "                    break\n",
    "                else:\n",
    "                    string.append(itos[idx])\n",
    "        word = ''.join(string)\n",
    "        output.append(word)\n",
    "    return output\n",
    "\n",
    "\n",
    "def validate_model(model, dataloader, MAX_LENGTH):\n",
    "    cum_loss = 0\n",
    "    cum_examples = 0\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # itos = tgt_vocab.get_itos()\n",
    "    # output_strings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, input_hidden in dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            encoder_hidden = input_hidden.to(DEVICE)\n",
    "            \n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            example_losses, decoder_ouputs = model(src, encoder_hidden, tgt, MAX_LENGTH)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            # Generate string outputs\n",
    "            # output_idxs = decoder_ouputs.argmax(-1)\n",
    "            # #print(output_idxs.size())\n",
    "            # #print(output_idxs)\n",
    "\n",
    "            # strings_batch = indices2string(output_idxs, itos)\n",
    "            # for s in strings_batch:\n",
    "            #     output_strings.append(s)\n",
    "\n",
    "            # m, std = calculate_ed(output_strings, tgt_strings)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return cum_loss/cum_examples\n",
    "            \n",
    "\n",
    "def train_model(model=None, optimizer=None, num_epochs=5, valid_niter=5000, \n",
    "                model_save_path='model.rar', max_num_patience=5, max_num_trial=5, \n",
    "                lr_decay=0.5):  \n",
    "    data_dir = out_dir/'task1_output'/'train'  \n",
    "    train_iter = Task2Dataset(train, data_dir, batch_size=128)\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, \n",
    "                                  collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    data_dir = out_dir/'task1_output'/'val'\n",
    "    val_dataloader = DataLoader(Task2Dataset(val, data_dir, batch_size=128), \n",
    "                                batch_size=4*BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    #_, tgt_strings = get_gold_tgt_words(val_dataloader, vocab_transform['ocr'], vocab_transform['gs'])\n",
    "\n",
    "\n",
    "    num_iter = 0\n",
    "    report_loss = 0\n",
    "    report_examples = 0\n",
    "    val_loss_hist = []\n",
    "    num_trial = 0\n",
    "    patience = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        cum_loss = 0\n",
    "        cum_examples = 0\n",
    "\n",
    "        for src, tgt, input_hidden in tqdm(train_dataloader):\n",
    "            num_iter += 1\n",
    "            # print('Source')\n",
    "            # print(src.size())\n",
    "            # print(src)\n",
    "            # print(src[0].size())\n",
    "            # print(src[0])\n",
    "            # a = src[0].unsqueeze(1)\n",
    "            # print(a.size())\n",
    "            # print(a)\n",
    "            # input_tensor = src.unsqueeze(2)\n",
    "            # print(input_tensor.size())\n",
    "            # print(input_tensor)\n",
    "            # print(input_tensor[0])\n",
    "            # print('Target')\n",
    "            # print(tgt.size())\n",
    "            # print(tgt)\n",
    "            # print(tgt.dtype)\n",
    "            if tgt.dtype == torch.float32:\n",
    "                print(tgt)\n",
    "\n",
    "            batch_size = src.size(1)\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            encoder_hidden = input_hidden.to(DEVICE)\n",
    "\n",
    "            # print(input_hidden.size())\n",
    "\n",
    "            example_losses, _ = model(src, encoder_hidden, tgt, MAX_LENGTH)\n",
    "            example_losses = -example_losses\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            bl = batch_loss.item()\n",
    "            report_loss += bl\n",
    "            report_examples += batch_size\n",
    "\n",
    "            cum_loss += bl\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if num_iter % valid_niter == 0:\n",
    "                val_loss = validate_model(model, val_dataloader, MAX_LENGTH)\n",
    "                print(f'Epoch {epoch}, iter {num_iter}, avg. train loss {report_loss/report_examples}, avg. val loss {val_loss}')\n",
    "\n",
    "                report_loss = 0\n",
    "                report_examples = 0\n",
    "\n",
    "                better_model = len(val_loss_hist) == 0 or val_loss < min(val_loss_hist)\n",
    "                if better_model:\n",
    "                    print(f'Saving model and optimizer to {model_save_path}')\n",
    "                    torch.save({\n",
    "                      'model_state_dict': model.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      }, model_save_path)\n",
    "                elif patience < max_num_patience:\n",
    "                    patience += 1\n",
    "                    print(f'hit patience {patience}')\n",
    "\n",
    "                    if patience == max_num_patience:\n",
    "                        num_trial += 1\n",
    "                        print(f'hit #{num_trial} trial')\n",
    "                        if num_trial == max_num_trial:\n",
    "                            print('early stop!')\n",
    "                            exit(0)\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0]['lr'] * lr_decay\n",
    "                        print(f'load previously best model and decay learning rate to {lr}')\n",
    "\n",
    "                        # load model\n",
    "                        checkpoint = torch.load(model_save_path)\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        \n",
    "                        model = model.to(device)\n",
    "                        \n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "                    \n",
    "\n",
    "                val_loss_hist.append(val_loss)\n",
    "\n",
    "\n",
    "def get_gold_tgt_words(dataloader, src_vocab, tgt_vocab):\n",
    "\n",
    "    src_itos = src_vocab.get_itos()\n",
    "    src_strings = []\n",
    "\n",
    "    tgt_itos = tgt_vocab.get_itos()\n",
    "    tgt_strings = []\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src_batch = indices2string(src.transpose(0, 1), src_itos)\n",
    "        for word in src_batch:\n",
    "            src_strings.append(word)\n",
    "        #print(tgt.size())\n",
    "        tgt_batch = indices2string(tgt.transpose(0, 1), tgt_itos)\n",
    "        for word in tgt_batch:\n",
    "            tgt_strings.append(word)\n",
    "    return src_strings, tgt_strings\n",
    "\n",
    "\n",
    "hidden_size = 768\n",
    "model = ICDARTask2Seq2seq(len(vocab_transform['ocr']), \n",
    "                          hidden_size, \n",
    "                          len(vocab_transform['gs']), \n",
    "                          0.1, \n",
    "                          MAX_LENGTH, \n",
    "                          teacher_forcing_ratio=0.0)\n",
    "model.to(device)    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "msp = '/mntDrive/MyDrive/model.rar'\n",
    "#msp = './model.rar'\n",
    "\n",
    "train_model(model=model, optimizer=optimizer, model_save_path=msp, \n",
    "            num_epochs=25, valid_niter=1000, max_num_patience=5, max_num_trial=5, lr_decay=0.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
