{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LOGURU_LEVEL'] = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "class InterceptHandler(logging.Handler):\n",
    "    def emit(self, record):\n",
    "        # Get corresponding Loguru level if it exists\n",
    "        try:\n",
    "            level = logger.level(record.levelname).name\n",
    "        except ValueError:\n",
    "            level = record.levelno\n",
    "\n",
    "        # Find caller from where originated the logged message\n",
    "        frame, depth = logging.currentframe(), 2\n",
    "        while frame.f_code.co_filename == logging.__file__:\n",
    "            frame = frame.f_back\n",
    "            depth += 1\n",
    "\n",
    "        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n",
    "\n",
    "logging.basicConfig(handlers=[InterceptHandler()], level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import generate_data\n",
    "\n",
    "in_dir = Path('../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_training_18M_without_Finnish')\n",
    "data, md = generate_data(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import generate_data\n",
    "\n",
    "in_dir = Path('../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_evaluation_4M_without_Finnish')\n",
    "data_test, X_test = generate_data(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import remove_label_and_nl\n",
    "\n",
    "def check_tokens(in_dir, key, data):\n",
    "\n",
    "    in_file = in_dir/key\n",
    "\n",
    "    with open(in_file) as f:\n",
    "        lines = f.readlines()\n",
    "        ocr_input = remove_label_and_nl(lines[0])\n",
    "        ocr_aligned = remove_label_and_nl(lines[1])\n",
    "        gs_aligned = remove_label_and_nl(lines[2])\n",
    "\n",
    "    text = data[key]\n",
    "\n",
    "    #assert ocr_input == ocr_aligned.replace('@', '')\n",
    "\n",
    "    #logger.info(f'Checking input tokens of {key}')\n",
    "    for token in text.input_tokens:\n",
    "        #logger.info(token)\n",
    "        inp = ocr_input[token.start:(token.start+token.len_ocr)]\n",
    "        try:\n",
    "            assert inp == token.ocr, f'\"{inp}\" != \"{token.ocr}\"'\n",
    "        except AssertionError:\n",
    "            logger.info(f'\"{inp}\" != \"{token.ocr}\" for token {token}')\n",
    "            raise\n",
    "        \n",
    "\n",
    "    #logger.info(f'Checking aligned tokens of {key}')\n",
    "\n",
    "    #print(ocr_input)\n",
    "\n",
    "check_tokens(in_dir, 'DE/DE6/60.txt', data_test)\n",
    "#check_tokens(in_dir, 'SL/SL1/40.txt', data_test)\n",
    "#check_tokens(in_dir, 'SL/SL1/17.txt', data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_errors = 0\n",
    "\n",
    "for key, _ in data_test.items():\n",
    "    try:\n",
    "        check_tokens(in_dir, key, data_test)\n",
    "    except AssertionError:\n",
    "        logger.info(f'Error in {key}')\n",
    "        num_errors += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_errors/len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('condensed_predictions_task1.json', 'r') as f:\n",
    "    result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_icdar_output(label_str, input_tokens):\n",
    "    text_output = {}\n",
    "\n",
    "    # Correct use of 2 (always following a 1)\n",
    "    regex = r'12*'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group())\n",
    "        idx = input_tokens[match.start()].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "\n",
    "    # Incorrect use of 2 (following a 0) -> interpret first 2 as 1\n",
    "    regex = r'02+'\n",
    "\n",
    "    for match in re.finditer(regex, label_str):\n",
    "        #print(match)\n",
    "        #print(match.group())\n",
    "        num_tokens = len(match.group()) - 1\n",
    "        idx = input_tokens[match.start()+1].start\n",
    "        text_output[f'{idx}:{num_tokens}'] = {}\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "#label_str = '12200010011120020222'\n",
    "#output = extract_icdar_output(label_str, data['DE/DE3/1988.txt'].input_tokens)\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output = {}\n",
    "\n",
    "for key, preds in result.items():\n",
    "    labels = defaultdict(list)\n",
    "    #print(key)\n",
    "    try:\n",
    "        text = data_test[key]\n",
    "        #print(len(text.input_tokens))\n",
    "        #print(preds)\n",
    "        for start, lbls in preds.items():\n",
    "            #print(start, type(start))\n",
    "            for i, label in enumerate(lbls):\n",
    "                labels[int(start)+i].append(label)\n",
    "        #print('LABELS')\n",
    "        #print(labels)\n",
    "\n",
    "        label_str = []\n",
    "\n",
    "        for i, token in enumerate(text.input_tokens):\n",
    "            #print(i, token, labels[i])\n",
    "            if 2 in labels[i]:\n",
    "                label_str.append('2')\n",
    "            elif 1 in labels[i]:\n",
    "                label_str.append('1')\n",
    "            else:\n",
    "                label_str.append('0')\n",
    "        label_str = ''.join(label_str)\n",
    "\n",
    "        #print('LABEL STR')\n",
    "        #print(label_str)\n",
    "\n",
    "        output[key] = extract_icdar_output(label_str, text.input_tokens)\n",
    "    except KeyError:\n",
    "        logger.warning(f'No data found for text {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('results_task1_new.json', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evalTool_ICDAR2017.py ../../data/ICDAR2019_POCR_competition_dataset/ICDAR2019_POCR_competition_evaluation_4M_without_Finnish results_task1_new.json results_task1.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
