# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02b_bert_vectors_correction_dataset.ipynb.

# %% auto 0
__all__ = ['BertVectorsCorrectionDataset', 'collate_fn_with_text_transform', 'collate_fn', 'validate_model']

# %% ../nbs/02b_bert_vectors_correction_dataset.ipynb 2
from functools import partial
from typing import List

import h5py
import numpy as np
import pandas as pd
import torch
from loguru import logger
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset

from ocrpostcorrection.error_correction import (
    PAD_IDX,
    generate_vocabs,
    get_text_transform,
)

# %% ../nbs/02b_bert_vectors_correction_dataset.ipynb 5
class BertVectorsCorrectionDataset(Dataset):
    def __init__(self, data: pd.DataFrame, bert_vectors_file: Path, split_name: str, max_len: int=11):
        ds = data.copy()
        ds.reset_index(drop=True, inplace=True)
        ds = ds.query(f'len_ocr < {max_len}').query(f'len_gs < {max_len}').copy()
        ds.reset_index(drop=False, inplace=True)
        self.ds = ds

        f = h5py.File(bert_vectors_file, "r")
        self.bert_vectors = f.get(split_name)

    def __len__(self):
        return self.ds.shape[0]

    def __getitem__(self, idx):
        sample = self.ds.loc[idx]
        original_idx = sample["index"]
        bert_vector = torch.as_tensor(np.array(self.bert_vectors[original_idx]))

        return [char for char in sample.ocr], [char for char in sample.gs], bert_vector

# %% ../nbs/02b_bert_vectors_correction_dataset.ipynb 8
def collate_fn_with_text_transform(text_transform, batch):
    """Function to collate data samples into batch tensors, to be used as partial with instatiated text_transform"""
    src_batch, tgt_batch, bert_vectors = [], [], []
    for src_sample, tgt_sample, bert_vector in batch:
        src_batch.append(text_transform["ocr"](src_sample))
        tgt_batch.append(text_transform["gs"](tgt_sample))
        bert_vectors.append(bert_vector)

    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)
    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)

    # Size of encoder_hidden should be 1 x batch_size x hidden_size
    encoder_hidden = torch.unsqueeze(torch.stack(bert_vectors, dim=0), dim=0)

    return src_batch.to(torch.int64), tgt_batch.to(torch.int64), encoder_hidden


def collate_fn(text_transform):
    """Function to collate data samples into batch tensors"""
    return partial(collate_fn_with_text_transform, text_transform)

# %% ../nbs/02b_bert_vectors_correction_dataset.ipynb 11
def validate_model(model, dataloader, device):
    cum_loss = 0
    cum_examples = 0

    was_training = model.training
    model.eval()

    with torch.no_grad():
        for src, tgt, encoder_hidden in dataloader:
            src = src.to(device)
            tgt = tgt.to(device)
            encoder_hidden.to(device)

            batch_size = src.size(1)

            example_losses, _decoder_ouputs = model(src, encoder_hidden, tgt)
            example_losses = -example_losses
            batch_loss = example_losses.sum()

            bl = batch_loss.item()
            cum_loss += bl
            cum_examples += batch_size

    if was_training:
        model.train()

    return cum_loss / cum_examples
