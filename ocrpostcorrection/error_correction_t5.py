# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02c_error_correction_t5.ipynb.

# %% auto 0
__all__ = ['filter_max_len', 'filter_len_ocr_mistake_in_context', 'preprocess_function']

# %% ../nbs/02c_error_correction_t5.ipynb 2
from typing import Dict

import pandas as pd

from loguru import logger

# %% ../nbs/02c_error_correction_t5.ipynb 6
def filter_max_len(example: Dict, max_len: int):
    if example["len_ocr"] <= max_len and example["len_gs"] <= max_len:
        return True
    return False

# %% ../nbs/02c_error_correction_t5.ipynb 7
def filter_len_ocr_mistake_in_context(data: pd.DataFrame, context_offset: int) -> pd.DataFrame:
    if context_offset:
        # Filter samples on length (to prevent using too much GPU memory)
        data = data.query(f"len_mistake_in_context <= {context_offset * 10}").copy()
        logger.info(f"Max length of input samples: {data.len_mistake_in_context.max()}")
    return data

# %% ../nbs/02c_error_correction_t5.ipynb 9
def preprocess_function(examples, tokenizer, add_task_prefix: bool=False, context_marker: str=""):
    if context_marker:
        input = [
            f"{before}<{context_marker}>{ocr_str}</{context_marker}>{after}"
            for before, ocr_str, after in zip(examples["context_before"], examples["ocr"], examples["context_after"])
        ]
    else:
        input = examples["ocr"]

    if add_task_prefix:
        input = [f"{language}: {ocr_str}" for ocr_str, language in zip(input, examples['language'])]

    model_inputs = tokenizer(input)

    labels = tokenizer(text_target=examples["gs"])

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
