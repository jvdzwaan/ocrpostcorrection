# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_error_correction.ipynb.

# %% auto 0
__all__ = ['UNK_IDX', 'PAD_IDX', 'BOS_IDX', 'EOS_IDX', 'special_symbols', 'get_tokens_with_OCR_mistakes', 'yield_tokens',
           'generate_vocabs', 'SimpleCorrectionDataset', 'sequential_transforms', 'tensor_transform',
           'get_text_transform', 'collate_fn_with_text_transform', 'collate_fn']

# %% ../nbs/02_error_correction.ipynb 2
import dataclasses
import random

from functools import partial
from itertools import chain
from typing import Iterable, List

import pandas as pd
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset
from torchtext.vocab import build_vocab_from_iterator

from .icdar_data import Text, normalized_ed

# %% ../nbs/02_error_correction.ipynb 5
def get_tokens_with_OCR_mistakes(data: dict[str, Text], 
                                data_test: dict[str, Text], 
                                val_files: list[str]) \
                                    -> pd.DataFrame:
    """Return pandas dataframe with all OCR mistakes from train, val, and test"""
    tokens = []
    # Train and val
    for key, d in data.items():
        for token in d.tokens:
            if token.ocr.strip() != token.gs.strip():
                r = dataclasses.asdict(token)
                r['language'] = key[:2]
                r['subset'] = key.split('/')[1]

                if key in val_files:
                    r['dataset'] = 'val'
                else:
                    r['dataset'] = 'train'

                tokens.append(r)
    # Test
    for key, d in data_test.items():
        for token in d.tokens:
            if token.ocr.strip() != token.gs.strip():
                r = dataclasses.asdict(token)
                r['language'] = key[:2]
                r['subset'] = key.split('/')[1]
                r['dataset'] = 'test'

                tokens.append(r)
    tdata = pd.DataFrame(tokens)
    tdata = _add_update_data_properties(tdata)

    return tdata

def _add_update_data_properties(tdata: pd.DataFrame) -> pd.DataFrame:
    """Add and update data properties for calculating statistics"""
    tdata['ocr'] = tdata['ocr'].apply(lambda x: x.strip())
    tdata['gs'] = tdata['gs'].apply(lambda x: x.strip())
    tdata['len_ocr'] = tdata.apply(lambda row: len(row.ocr), axis=1)
    tdata['len_gs'] = tdata.apply(lambda row: len(row.gs), axis=1)
    tdata['diff'] = tdata.len_ocr - tdata.len_gs
    return tdata


# %% ../nbs/02_error_correction.ipynb 10
UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3

special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']

# %% ../nbs/02_error_correction.ipynb 11
def yield_tokens(data, col):
    """Helper function to create vocabulary containing characters"""
    for token in data[col].to_list():
        for char in token:
            yield char

# %% ../nbs/02_error_correction.ipynb 12
def generate_vocabs(train):
    """Generate ocr and gs vocabularies from the train set"""
    vocab_transform = {}
    for name in ('ocr', 'gs'):
        vocab_transform[name] = build_vocab_from_iterator(yield_tokens(train, name),
                                                          min_freq=1,
                                                          specials=special_symbols,
                                                          special_first=True)
    # Set UNK_IDX as the default index. This index is returned when the token is not found.
    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.
    for name in ('ocr', 'gs'):
        vocab_transform[name].set_default_index(UNK_IDX)
    
    return vocab_transform

# %% ../nbs/02_error_correction.ipynb 16
class SimpleCorrectionDataset(Dataset):
    def __init__(self, data, max_len=11):
        self.ds = data.query(f'len_ocr < {max_len}').query(f'len_gs < {max_len}').copy()
        self.ds = self.ds.reset_index(drop=False)

    def __len__(self):
        return self.ds.shape[0]

    def __getitem__(self, idx):
        sample = self.ds.loc[idx]

        return [char for char in sample.ocr], [char for char in sample.gs]

# %% ../nbs/02_error_correction.ipynb 24
def sequential_transforms(*transforms):
    """Helper function to club together sequential operations"""
    def func(txt_input):
        for transform in transforms:
            txt_input = transform(txt_input)
        return txt_input
    return func

def tensor_transform(token_ids: List[int]):
    """Function to add BOS/EOS and create tensor for input sequence indices"""
    return torch.cat((torch.tensor(token_ids),
                      torch.tensor([EOS_IDX])))

def get_text_transform(vocab_transform):
    """Returns text transforms to convert raw strings into tensors indices"""
    text_transform = {}
    for name in ('ocr', 'gs'):
        text_transform[name] = sequential_transforms(vocab_transform[name],  # Numericalization (char -> idx)
                                                     tensor_transform) # Add BOS/EOS and create tensor
    return text_transform

# %% ../nbs/02_error_correction.ipynb 26
def collate_fn_with_text_transform(text_transform, batch):
    """Function to collate data samples into batch tensors, to be used as partial with instatiated text_transform"""
    src_batch, tgt_batch = [], []
    for src_sample, tgt_sample in batch:
        src_batch.append(text_transform['ocr'](src_sample))
        tgt_batch.append(text_transform['gs'](tgt_sample))

    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)
    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)

    return src_batch.to(torch.int64), tgt_batch.to(torch.int64)


def collate_fn(text_transform):
    """Function to collate data samples into batch tensors"""
    return partial(collate_fn_with_text_transform, text_transform)
